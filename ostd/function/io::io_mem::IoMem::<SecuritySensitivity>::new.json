{
  "name": "io::io_mem::IoMem::<SecuritySensitivity>::new",
  "safe": false,
  "callees": {
    "align_ext::AlignExt::align_down": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns to the greatest number that is smaller than or equal to\n `self` and is a multiple of the given power of two.\n\n The method panics if `power_of_two` is not a\n power of two or is smaller than 2 or the calculation overflows\n because `self` is too large. In release mode,\n\n # Examples\n\n ```\n use crate::align_ext::AlignExt;\n assert_eq!(12usize.align_down(2), 12);\n assert_eq!(12usize.align_down(4), 12);\n assert_eq!(12usize.align_down(8), 8);\n assert_eq!(12usize.align_down(16), 0);\n ```\n",
      "adt": {}
    },
    "align_ext::AlignExt::align_up": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns to the smallest number that is greater than or equal to\n `self` and is a multiple of the given power of two.\n\n The method panics if `power_of_two` is not a\n power of two or is smaller than 2 or the calculation overflows\n because `self` is too large.\n\n # Examples\n\n ```\n use crate::align_ext::AlignExt;\n assert_eq!(12usize.align_up(2), 12);\n assert_eq!(12usize.align_up(4), 12);\n assert_eq!(12usize.align_up(8), 16);\n assert_eq!(12usize.align_up(16), 16);\n ```\n",
      "adt": {}
    },
    "core::iter::ExactSizeIterator::len": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns the exact remaining length of the iterator.\n\n The implementation ensures that the iterator will return exactly `len()`\n more times a [`Some(T)`] value, before returning [`None`].\n This method has a default implementation, so you usually should not\n implement it directly. However, if you can provide a more efficient\n implementation, you can do so. See the [trait-level] docs for an\n example.\n\n This function has the same safety guarantees as the\n [`Iterator::size_hint`] function.\n\n [trait-level]: ExactSizeIterator\n [`Some(T)`]: Some\n\n # Examples\n\n Basic usage:\n\n ```\n // a finite range knows exactly how many times it will iterate\n let mut range = 0..5;\n\n assert_eq!(5, range.len());\n let _ = range.next();\n assert_eq!(4, range.len());\n ```\n",
      "adt": {}
    },
    "tdx_guest::tdx_is_enabled": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "",
      "adt": {}
    },
    "arch::tdx_guest::unprotect_gpa_tdvm_call": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Converts physical pages to Intel TDX shared pages.\n\n It invokes the [`map_gpa`] TDVMCALL to convert those pages into Intel TDX\n shared pages. Due to the conversion, any existing data on the pages will\n be lost.\n\n # Safety\n\n The caller must ensure that:\n  - The provided physical address is page aligned.\n  - The provided physical address range is in bounds, i.e., it should fall\n    within the maximum Guest Physical Address (GPA) limit.\n  - All of the physical pages are untyped memory. Therefore, converting and\n    erasing the data will not cause memory safety issues.\n",
      "adt": {
        "core::result::Result": "Constructor"
      }
    },
    "core::fmt::rt::Argument::<'_>::new_debug": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "",
      "adt": {}
    },
    "core::fmt::Arguments::<'a>::new": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "",
      "adt": {}
    },
    "core::panicking::panic_fmt": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " The entry point for panicking with a formatted message.\n\n This is designed to reduce the amount of code required at the call\n site as much as possible (so that `panic!()` has as low an impact\n on (e.g.) the inlining of other functions as possible), by moving\n the actual formatting into this shared place.\n",
      "adt": {}
    },
    "core::result::Result::<T, E>::unwrap": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns the contained [`Ok`] value, consuming the `self` value.\n\n Because this function may panic, its use is generally discouraged.\n Panics are meant for unrecoverable errors, and\n [may abort the entire program][panic-abort].\n\n Instead, prefer to use [the `?` (try) operator][try-operator], or pattern matching\n to handle the [`Err`] case explicitly, or call [`unwrap_or`],\n [`unwrap_or_else`], or [`unwrap_or_default`].\n\n [panic-abort]: https://doc.rust-lang.org/book/ch09-01-unrecoverable-errors-with-panic.html\n [try-operator]: https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html#a-shortcut-for-propagating-errors-the--operator\n [`unwrap_or`]: Result::unwrap_or\n [`unwrap_or_else`]: Result::unwrap_or_else\n [`unwrap_or_default`]: Result::unwrap_or_default\n\n # Panics\n\n Panics if the value is an [`Err`], with a panic message provided by the\n [`Err`]'s value.\n\n\n # Examples\n\n Basic usage:\n\n ```\n let x: Result<u32, &str> = Ok(2);\n assert_eq!(x.unwrap(), 2);\n ```\n\n ```should_panic\n let x: Result<u32, &str> = Err(\"emergency failure\");\n x.unwrap(); // panics with `emergency failure`\n ```\n",
      "adt": {}
    },
    "mm::page_prop::PrivilegedPageFlags::empty": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns an empty set of flags.\n",
      "adt": {
        "mm::page_prop::PrivilegedPageFlags": "Constructor"
      }
    },
    "mm::kspace::kvirt_area::KVirtArea::map_untracked_frames": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a kernel virtual area and maps untracked frames into it.\n\n The created virtual area will have a size of `area_size`, and the\n physical addresses will be mapped starting from `map_offset` in\n the area.\n\n You can provide a `0..0` physical range to create a virtual area without\n mapping any physical memory.\n\n # Panics\n\n This function panics if\n  - the area size is not a multiple of [`PAGE_SIZE`];\n  - the map offset is not aligned to [`PAGE_SIZE`];\n  - the provided physical range is not aligned to [`PAGE_SIZE`];\n  - the map offset plus the length of the physical range exceeds the\n    area size;\n  - the provided physical range contains tracked physical addresses.\n",
      "adt": {
        "core::ops::Range": "ImmutableAsArgument",
        "mm::kspace::kvirt_area::KVirtArea": "Constructor"
      }
    },
    "util::id_set::IdSet::<I>::new_full": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a new `IdSet` with all IDs in the system.\n",
      "adt": {
        "util::id_set::IdSet": "Constructor"
      }
    },
    "util::id_set::AtomicIdSet::<I>::new": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a new `AtomicIdSet` from an `IdSet`.\n",
      "adt": {
        "util::id_set::AtomicIdSet": "Constructor"
      }
    },
    "task::preempt::guard::disable_preempt": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Disables preemption.\n",
      "adt": {
        "task::preempt::guard::DisabledPreemptGuard": "Constructor"
      }
    },
    "mm::tlb::TlbFlusher::<'a, G>::new": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a new TLB flusher with the specified CPUs to be flushed.\n\n The target CPUs should be a reference to an [`AtomicCpuSet`] that will\n be loaded upon [`Self::dispatch_tlb_flush`].\n\n The flusher needs to stick to the current CPU. So please provide a\n guard that implements [`PinCurrentCpu`].\n",
      "adt": {
        "util::id_set::AtomicIdSet": "ImmutableAsArgument",
        "mm::tlb::TlbFlusher": "Constructor"
      }
    },
    "mm::kspace::kvirt_area::KVirtArea::range": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "",
      "adt": {
        "core::ops::Range": "Constructor",
        "mm::kspace::kvirt_area::KVirtArea": "ImmutableAsArgument"
      }
    },
    "mm::tlb::TlbFlushOp::for_range": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a new TLB flush operation that flushes the TLB entries for the\n specified virtual address range.\n\n If the range is too large, the resulting [`TlbFlushOp`] will flush all\n TLB entries instead.\n\n # Panics\n\n Panics if the range is not page-aligned or if the range is empty.\n",
      "adt": {
        "mm::tlb::TlbFlushOp": "Constructor"
      }
    },
    "mm::tlb::TlbFlusher::<'a, G>::issue_tlb_flush": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Issues a pending TLB flush request.\n\n This function does not guarantee to flush the TLB entries on either\n this CPU or remote CPUs. The flush requests are only performed when\n [`Self::dispatch_tlb_flush`] is called.\n",
      "adt": {
        "mm::tlb::TlbFlusher": "MutableAsArgument"
      }
    },
    "mm::tlb::TlbFlusher::<'a, G>::dispatch_tlb_flush": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Dispatches all the pending TLB flush requests.\n\n All previous pending requests issued by [`Self::issue_tlb_flush`] or\n [`Self::issue_tlb_flush_with`] starts to be processed after this\n function. But it may not be synchronous. Upon the return of this\n function, the TLB entries may not be coherent.\n",
      "adt": {
        "mm::tlb::TlbFlusher": "MutableAsArgument"
      }
    },
    "mm::tlb::TlbFlusher::<'a, G>::sync_tlb_flush": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Waits for all the previous TLB flush requests to be completed.\n\n After this function, all TLB entries corresponding to previous\n dispatched TLB flush requests are guaranteed to be coherent.\n\n The TLB flush requests are issued with [`Self::issue_tlb_flush`] and\n dispatched with [`Self::dispatch_tlb_flush`]. This method will not\n dispatch any issued requests so it will not guarantee TLB coherence\n of requests that are not dispatched.\n\n # Panics\n\n This method panics if the IRQs are disabled. Since the remote flush are\n processed in IRQs, two CPUs may deadlock if they are waiting for each\n other's TLB coherence.\n",
      "adt": {
        "mm::tlb::TlbFlusher": "MutableAsArgument"
      }
    },
    "alloc::sync::Arc::<T>::new": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Constructs a new `Arc<T>`.\n\n # Examples\n\n ```\n use std::sync::Arc;\n\n let five = Arc::new(5);\n ```\n",
      "adt": {}
    }
  },
  "adts": {
    "core::ops::Range": [
      "Unknown([Field(0, Ty { id: 11, kind: RigidTy(Uint(Usize)) })])",
      "Unknown([Field(1, Ty { id: 11, kind: RigidTy(Uint(Usize)) })])",
      "Plain",
      "Ref"
    ],
    "core::result::Result": [
      "Plain"
    ],
    "core::fmt::rt::Argument": [
      "Plain"
    ],
    "core::fmt::Arguments": [
      "Plain"
    ],
    "mm::page_prop::PrivilegedPageFlags": [
      "Plain"
    ],
    "mm::page_prop::PageProperty": [
      "Plain"
    ],
    "mm::page_prop::PageFlags": [
      "Plain"
    ],
    "mm::page_prop::CachePolicy": [
      "Plain"
    ],
    "mm::kspace::kvirt_area::KVirtArea": [
      "Plain",
      "Ref"
    ],
    "util::id_set::IdSet": [
      "Plain"
    ],
    "util::id_set::AtomicIdSet": [
      "Plain",
      "Ref"
    ],
    "task::preempt::guard::DisabledPreemptGuard": [
      "Plain"
    ],
    "mm::tlb::TlbFlusher": [
      "Plain",
      "MutRef"
    ],
    "mm::tlb::TlbFlushOp": [
      "Plain"
    ],
    "alloc::sync::Arc": [
      "Plain"
    ],
    "io::io_mem::IoMem": [
      "Plain"
    ]
  },
  "path": 1761,
  "span": "ostd/src/io/io_mem/mod.rs:83:5: 143:6",
  "src": "pub(crate) unsafe fn new(range: Range<Paddr>, flags: PageFlags, cache: CachePolicy) -> Self {\n        let first_page_start = range.start.align_down(PAGE_SIZE);\n        let last_page_end = range.end.align_up(PAGE_SIZE);\n\n        let frames_range = first_page_start..last_page_end;\n        let area_size = frames_range.len();\n\n        #[cfg(target_arch = \"x86_64\")]\n        let priv_flags = crate::arch::if_tdx_enabled!({\n            assert!(\n                first_page_start == range.start && last_page_end == range.end,\n                \"I/O memory is not page aligned, which cannot be unprotected in TDX: {:#x?}..{:#x?}\",\n                range.start,\n                range.end,\n            );\n\n            // SAFETY:\n            //  - The range `first_page_start..last_page_end` is always page aligned.\n            //  - FIXME: We currently do not limit the I/O memory allocator with the maximum GPA,\n            //    so the address range may not fall in the GPA limit.\n            //  - The caller guarantees that operations on the I/O memory do not have any side\n            //    effects that may cause soundness problems, so the pages can safely be viewed as\n            //    untyped memory.\n            unsafe { crate::arch::tdx_guest::unprotect_gpa_tdvm_call(first_page_start, area_size).unwrap() };\n\n            PrivilegedPageFlags::SHARED\n        } else {\n            PrivilegedPageFlags::empty()\n        });\n        #[cfg(not(target_arch = \"x86_64\"))]\n        let priv_flags = PrivilegedPageFlags::empty();\n\n        let prop = PageProperty {\n            flags,\n            cache,\n            priv_flags,\n        };\n\n        let kva = {\n            // SAFETY: The caller of `IoMem::new()` ensures that the given\n            // physical address range is I/O memory, so it is safe to map.\n            let kva = unsafe { KVirtArea::map_untracked_frames(area_size, 0, frames_range, prop) };\n\n            let target_cpus = AtomicCpuSet::new(CpuSet::new_full());\n            let mut flusher = TlbFlusher::new(&target_cpus, disable_preempt());\n            flusher.issue_tlb_flush(TlbFlushOp::for_range(kva.range()));\n            flusher.dispatch_tlb_flush();\n            flusher.sync_tlb_flush();\n\n            kva\n        };\n\n        Self {\n            kvirt_area: Arc::new(kva),\n            offset: range.start - first_page_start,\n            limit: range.len(),\n            pa: range.start,\n            cache_policy: cache,\n            phantom: PhantomData,\n        }\n    }",
  "mir": "fn io::io_mem::IoMem::<SecuritySensitivity>::new(_1: core::ops::Range<usize>, _2: mm::page_prop::PageFlags, _3: mm::page_prop::CachePolicy) -> io::io_mem::IoMem<SecuritySensitivity> {\n    let mut _0: io::io_mem::IoMem<SecuritySensitivity>;\n    let  _4: usize;\n    let mut _5: usize;\n    let  _6: usize;\n    let mut _7: usize;\n    let  _8: core::ops::Range<usize>;\n    let  _9: usize;\n    let mut _10: &core::ops::Range<usize>;\n    let  _11: mm::page_prop::PrivilegedPageFlags;\n    let mut _12: bool;\n    let mut _13: bool;\n    let mut _14: usize;\n    let mut _15: bool;\n    let mut _16: usize;\n    let  _17: !;\n    let mut _18: core::fmt::Arguments<'_>;\n    let  _19: (&usize, &usize);\n    let mut _20: &usize;\n    let mut _21: &usize;\n    let  _22: [core::fmt::rt::Argument<'_>; 2];\n    let mut _23: core::fmt::rt::Argument<'_>;\n    let mut _24: core::fmt::rt::Argument<'_>;\n    let mut _25: &[u8; 83];\n    let  _26: &[core::fmt::rt::Argument<'_>; 2];\n    let  _27: ();\n    let mut _28: core::result::Result<(), arch::tdx_guest::PageConvertError>;\n    let  _29: mm::page_prop::PageProperty;\n    let mut _30: mm::page_prop::PrivilegedPageFlags;\n    let  _31: mm::kspace::kvirt_area::KVirtArea;\n    let  _32: util::id_set::AtomicIdSet<cpu::id::CpuId>;\n    let mut _33: util::id_set::IdSet<cpu::id::CpuId>;\n    let mut _34: mm::tlb::TlbFlusher<'_, task::preempt::guard::DisabledPreemptGuard>;\n    let  _35: &util::id_set::AtomicIdSet<cpu::id::CpuId>;\n    let mut _36: task::preempt::guard::DisabledPreemptGuard;\n    let  _37: ();\n    let mut _38: &mut mm::tlb::TlbFlusher<'_, task::preempt::guard::DisabledPreemptGuard>;\n    let mut _39: mm::tlb::TlbFlushOp;\n    let mut _40: core::ops::Range<usize>;\n    let mut _41: &mm::kspace::kvirt_area::KVirtArea;\n    let  _42: ();\n    let mut _43: &mut mm::tlb::TlbFlusher<'_, task::preempt::guard::DisabledPreemptGuard>;\n    let  _44: ();\n    let mut _45: &mut mm::tlb::TlbFlusher<'_, task::preempt::guard::DisabledPreemptGuard>;\n    let mut _46: alloc::sync::Arc<mm::kspace::kvirt_area::KVirtArea>;\n    let mut _47: usize;\n    let mut _48: usize;\n    let mut _49: (usize, bool);\n    let mut _50: usize;\n    let mut _51: &core::ops::Range<usize>;\n    let mut _52: usize;\n    let mut _53: &usize;\n    let mut _54: &usize;\n    debug range => _1;\n    debug flags => _2;\n    debug cache => _3;\n    debug first_page_start => _4;\n    debug last_page_end => _6;\n    debug frames_range => _8;\n    debug area_size => _9;\n    debug priv_flags => _11;\n    debug args => _19;\n    debug args => _22;\n    debug prop => _29;\n    debug kva => _31;\n    debug kva => _31;\n    debug target_cpus => _32;\n    debug flusher => _34;\n    bb0: {\n        StorageLive(_5);\n        _5 = (_1.0: usize);\n        _4 = <usize as align_ext::AlignExt>::align_down(move _5, mm::PAGE_SIZE) -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        StorageDead(_5);\n        StorageLive(_7);\n        _7 = (_1.1: usize);\n        _6 = <usize as align_ext::AlignExt>::align_up(move _7, mm::PAGE_SIZE) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        StorageDead(_7);\n        _8 = Range(_4, _6);\n        StorageLive(_10);\n        _10 = &_8;\n        _9 = <core::ops::Range<usize> as core::iter::ExactSizeIterator>::len(move _10) -> [return: bb3, unwind unreachable];\n    }\n    bb3: {\n        StorageDead(_10);\n        StorageLive(_11);\n        StorageLive(_12);\n        _12 = tdx_guest::tdx_is_enabled() -> [return: bb4, unwind unreachable];\n    }\n    bb4: {\n        switchInt(move _12) -> [0: bb16, otherwise: bb5];\n    }\n    bb5: {\n        StorageLive(_13);\n        StorageLive(_14);\n        _14 = (_1.0: usize);\n        _13 = Eq(_4, move _14);\n        switchInt(move _13) -> [0: bb9, otherwise: bb6];\n    }\n    bb6: {\n        StorageDead(_14);\n        StorageLive(_15);\n        StorageLive(_16);\n        _16 = (_1.1: usize);\n        _15 = Eq(_6, move _16);\n        switchInt(move _15) -> [0: bb8, otherwise: bb7];\n    }\n    bb7: {\n        StorageDead(_16);\n        StorageDead(_15);\n        StorageDead(_13);\n        StorageLive(_28);\n        _28 = arch::tdx_guest::unprotect_gpa_tdvm_call(_4, _9) -> [return: bb14, unwind unreachable];\n    }\n    bb8: {\n        StorageDead(_16);\n        goto -> bb10;\n    }\n    bb9: {\n        StorageDead(_14);\n        goto -> bb10;\n    }\n    bb10: {\n        StorageLive(_18);\n        StorageLive(_19);\n        StorageLive(_20);\n        _20 = &(_1.0: usize);\n        StorageLive(_21);\n        _21 = &(_1.1: usize);\n        _19 = (move _20, move _21);\n        StorageDead(_21);\n        StorageDead(_20);\n        StorageLive(_22);\n        StorageLive(_23);\n        _53 = (_19.0: &usize);\n        _23 = core::fmt::rt::Argument::<'_>::new_debug::<usize>(_53) -> [return: bb11, unwind unreachable];\n    }\n    bb11: {\n        StorageLive(_24);\n        _54 = (_19.1: &usize);\n        _24 = core::fmt::rt::Argument::<'_>::new_debug::<usize>(_54) -> [return: bb12, unwind unreachable];\n    }\n    bb12: {\n        _22 = [move _23, move _24];\n        StorageDead(_24);\n        StorageDead(_23);\n        StorageLive(_25);\n        _25 = b\"DI/O memory is not page aligned, which cannot be unprotected in TDX: \\xc1 \\x00\\x80b\\x02..\\xc1 \\x00\\x80b\\x00\";\n        _26 = &_22;\n        _18 = core::fmt::Arguments::<'_>::new::<83, 2>(move _25, _26) -> [return: bb13, unwind unreachable];\n    }\n    bb13: {\n        StorageDead(_25);\n        _17 = core::panicking::panic_fmt(move _18) -> unwind unreachable;\n    }\n    bb14: {\n        _27 = core::result::Result::<(), arch::tdx_guest::PageConvertError>::unwrap(move _28) -> [return: bb15, unwind unreachable];\n    }\n    bb15: {\n        StorageDead(_28);\n        _11 = mm::page_prop::PrivilegedPageFlags::SHARED;\n        goto -> bb17;\n    }\n    bb16: {\n        _11 = mm::page_prop::PrivilegedPageFlags::empty() -> [return: bb17, unwind unreachable];\n    }\n    bb17: {\n        StorageDead(_12);\n        StorageLive(_30);\n        _30 = _11;\n        _29 = PageProperty(_2, _3, move _30);\n        StorageDead(_30);\n        _31 = mm::kspace::kvirt_area::KVirtArea::map_untracked_frames(_9, 0_usize, _8, _29) -> [return: bb18, unwind unreachable];\n    }\n    bb18: {\n        StorageLive(_32);\n        StorageLive(_33);\n        _33 = util::id_set::IdSet::<cpu::id::CpuId>::new_full() -> [return: bb19, unwind unreachable];\n    }\n    bb19: {\n        _32 = util::id_set::AtomicIdSet::<cpu::id::CpuId>::new(move _33) -> [return: bb20, unwind unreachable];\n    }\n    bb20: {\n        StorageDead(_33);\n        StorageLive(_34);\n        _35 = &_32;\n        _36 = task::preempt::guard::disable_preempt() -> [return: bb21, unwind unreachable];\n    }\n    bb21: {\n        _34 = mm::tlb::TlbFlusher::<'_, task::preempt::guard::DisabledPreemptGuard>::new(_35, task::preempt::guard::DisabledPreemptGuard {{ _private: () }}) -> [return: bb22, unwind unreachable];\n    }\n    bb22: {\n        StorageLive(_38);\n        _38 = &mut _34;\n        StorageLive(_39);\n        StorageLive(_40);\n        StorageLive(_41);\n        _41 = &_31;\n        _40 = mm::kspace::kvirt_area::KVirtArea::range(move _41) -> [return: bb23, unwind unreachable];\n    }\n    bb23: {\n        StorageDead(_41);\n        _39 = mm::tlb::TlbFlushOp::for_range(move _40) -> [return: bb24, unwind unreachable];\n    }\n    bb24: {\n        StorageDead(_40);\n        _37 = mm::tlb::TlbFlusher::<'_, task::preempt::guard::DisabledPreemptGuard>::issue_tlb_flush(move _38, move _39) -> [return: bb25, unwind unreachable];\n    }\n    bb25: {\n        StorageDead(_39);\n        StorageDead(_38);\n        StorageLive(_43);\n        _43 = &mut _34;\n        _42 = mm::tlb::TlbFlusher::<'_, task::preempt::guard::DisabledPreemptGuard>::dispatch_tlb_flush(move _43) -> [return: bb26, unwind unreachable];\n    }\n    bb26: {\n        StorageDead(_43);\n        StorageLive(_45);\n        _45 = &mut _34;\n        _44 = mm::tlb::TlbFlusher::<'_, task::preempt::guard::DisabledPreemptGuard>::sync_tlb_flush(move _45) -> [return: bb27, unwind unreachable];\n    }\n    bb27: {\n        StorageDead(_45);\n        drop(_34) -> [return: bb28, unwind unreachable];\n    }\n    bb28: {\n        StorageDead(_34);\n        drop(_32) -> [return: bb29, unwind unreachable];\n    }\n    bb29: {\n        StorageDead(_32);\n        StorageLive(_46);\n        _46 = alloc::sync::Arc::<mm::kspace::kvirt_area::KVirtArea>::new(_31) -> [return: bb30, unwind unreachable];\n    }\n    bb30: {\n        StorageLive(_47);\n        StorageLive(_48);\n        _48 = (_1.0: usize);\n        _49 = CheckedSub(_48, _4);\n        assert(!move (_49.1: bool), \"attempt to compute `{} - {}`, which would overflow\", move _48, _4) -> [success: bb31, unwind unreachable];\n    }\n    bb31: {\n        _47 = move (_49.0: usize);\n        StorageDead(_48);\n        StorageLive(_50);\n        StorageLive(_51);\n        _51 = &_1;\n        _50 = <core::ops::Range<usize> as core::iter::ExactSizeIterator>::len(move _51) -> [return: bb32, unwind unreachable];\n    }\n    bb32: {\n        StorageDead(_51);\n        StorageLive(_52);\n        _52 = (_1.0: usize);\n        _0 = IoMem(move _46, move _47, move _50, move _52, _3, ZeroSized: core::marker::PhantomData<SecuritySensitivity>);\n        StorageDead(_52);\n        StorageDead(_50);\n        StorageDead(_47);\n        StorageDead(_46);\n        StorageDead(_11);\n        return;\n    }\n}\n",
  "doc": " Creates a new `IoMem`.\n\n # Safety\n\n 1. This function must be called after the kernel page table is activated.\n 2. The given physical address range must be in the I/O memory region.\n 3. Reading from or writing to I/O memory regions may have side effects.\n    If `SecuritySensitivity` is `Insensitive`, those side effects must\n    not cause soundness problems (e.g., they must not corrupt the kernel\n    memory).\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}