{
  "name": "arch::iommu::registers::invalidation::InvalidationRegisters::new",
  "safe": false,
  "callees": {
    "core::ptr::NonNull::<T>::add": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Adds an offset to a pointer (convenience for `.offset(count as isize)`).\n\n `count` is in units of T; e.g., a `count` of 3 represents a pointer\n offset of `3 * size_of::<T>()` bytes.\n\n # Safety\n\n If any of the following conditions are violated, the result is Undefined Behavior:\n\n * The computed offset, `count * size_of::<T>()` bytes, must not overflow `isize`.\n\n * If the computed offset is non-zero, then `self` must be derived from a pointer to some\n   [allocation], and the entire memory range between `self` and the result must be in\n   bounds of that allocation. In particular, this range must not \"wrap around\" the edge\n   of the address space.\n\n Allocations can never be larger than `isize::MAX` bytes, so if the computed offset\n stays in bounds of the allocation, it is guaranteed to satisfy the first requirement.\n This implies, for instance, that `vec.as_ptr().add(vec.len())` (for `vec: Vec<T>`) is always\n safe.\n\n [allocation]: crate::ptr#allocation\n\n # Examples\n\n ```\n use std::ptr::NonNull;\n\n let s: &str = \"123\";\n let ptr: NonNull<u8> = NonNull::new(s.as_ptr().cast_mut()).unwrap();\n\n unsafe {\n     println!(\"{}\", ptr.add(1).read() as char);\n     println!(\"{}\", ptr.add(2).read() as char);\n }\n ```\n",
      "adt": {}
    },
    "core::ptr::NonNull::<T>::cast": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Casts to a pointer of another type.\n\n # Examples\n\n ```\n use std::ptr::NonNull;\n\n let mut x = 0u32;\n let ptr = NonNull::new(&mut x as *mut _).expect(\"null pointer\");\n\n let casted_ptr = ptr.cast::<i8>();\n let raw_ptr: *mut i8 = casted_ptr.as_ptr();\n ```\n",
      "adt": {}
    },
    "volatile::VolatileRef::<'a, T>::new_read_only": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Turns the given pointer into a read-only `VolatileRef`.\n\n ## Safety\n\n - The pointer must be properly aligned.\n - It must be “dereferenceable” in the sense defined in the [`core::ptr`] documentation.\n - The pointer must point to an initialized instance of T.\n - You must enforce Rust’s aliasing rules, since the returned lifetime 'a is arbitrarily\n   chosen and does not necessarily reflect the actual lifetime of the data. In particular,\n   while this `VolatileRef` exists, the memory the pointer points to _must not get mutated_.\n",
      "adt": {}
    },
    "volatile::VolatileRef::<'a, T, A>::as_ptr": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Borrows this `VolatileRef` as a read-only [`VolatilePtr`].\n\n Use this method to do (partial) volatile reads of the referenced data.\n",
      "adt": {}
    },
    "volatile::volatile_ptr::operations::<impl volatile::VolatilePtr<'a, T, A>>::read": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Performs a volatile read of the contained value.\n\n Returns a copy of the read value. Volatile reads are guaranteed not to be optimized\n away by the compiler, but by themselves do not have atomic ordering\n guarantees. To also get atomicity, consider looking at the `Atomic` wrapper types of\n the standard/`core` library.\n\n ## Examples\n\n ```rust\n use volatile::{VolatilePtr, access};\n use core::ptr::NonNull;\n\n let value = 42;\n let pointer = unsafe {\n     VolatilePtr::new_restricted(access::ReadOnly, NonNull::from(&value))\n };\n assert_eq!(pointer.read(), 42);\n ```\n",
      "adt": {}
    },
    "arch::iommu::registers::extended_cap::ExtendedCapability::new": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates ExtendedCapability from `value`\n",
      "adt": {
        "arch::iommu::registers::extended_cap::ExtendedCapability": "Constructor"
      }
    },
    "arch::iommu::registers::extended_cap::ExtendedCapability::iotlb_register_offset": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " IOTLB Register Offset. This field specifies the offset to the IOTLB registers relative\n to the register base address of this remapping hardware unit.\n\n If the register base address is X, and the value reported in this field is Y, the\n address for the IOTLB registers is calculated as X+(16*Y).\n",
      "adt": {
        "arch::iommu::registers::extended_cap::ExtendedCapability": "ImmutableAsArgument"
      }
    },
    "volatile::VolatileRef::<'a, T>::new": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Turns the given pointer into a `VolatileRef`.\n\n ## Safety\n\n - The pointer must be properly aligned.\n - It must be “dereferenceable” in the sense defined in the [`core::ptr`] documentation.\n - The pointer must point to an initialized instance of T.\n - You must enforce Rust’s aliasing rules, since the returned lifetime 'a is arbitrarily\n   chosen and does not necessarily reflect the actual lifetime of the data. In particular,\n   while this `VolatileRef` exists, the memory the pointer points to must not get accessed\n   (_read or written_) through any other pointer.\n",
      "adt": {}
    },
    "volatile::VolatileRef::<'a, T>::new_restricted": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Turns the given pointer into a `VolatileRef` instance with the given access.\n\n ## Safety\n\n - The pointer must be properly aligned.\n - It must be “dereferenceable” in the sense defined in the [`core::ptr`] documentation.\n - The pointer must point to an initialized instance of T.\n - You must enforce Rust’s aliasing rules, since the returned lifetime 'a is arbitrarily\n   chosen and does not necessarily reflect the actual lifetime of the data. In particular,\n   while this `VolatileRef` exists, the memory the pointer points to _must not get mutated_.\n   If the given `access` parameter allows write access, the pointer _must not get read\n   either_ while this `VolatileRef` exists.\n",
      "adt": {}
    }
  },
  "adts": {
    "core::ptr::NonNull": [
      "Plain"
    ],
    "volatile::VolatileRef": [
      "Plain",
      "Ref"
    ],
    "volatile::VolatilePtr": [
      "Plain"
    ],
    "arch::iommu::registers::extended_cap::ExtendedCapability": [
      "Plain",
      "Ref"
    ],
    "arch::iommu::registers::invalidation::InvalidationRegisters": [
      "Plain"
    ]
  },
  "path": 1040,
  "span": "ostd/src/arch/x86/iommu/registers/invalidation.rs:39:5: 71:6",
  "src": "pub(super) unsafe fn new(base: NonNull<u8>) -> Self {\n        let offset = {\n            // SAFETY: The safety is upheld by the caller.\n            let extended_capability =\n                unsafe { VolatileRef::new_read_only(base.add(0x10).cast::<u64>()) };\n            let extend_cap = ExtendedCapability::new(extended_capability.as_ptr().read());\n            extend_cap.iotlb_register_offset() as usize * 16\n        };\n\n        // FIXME: We now trust the hardware. We should instead find a way to check that `offset`\n        // are reasonable values before proceeding.\n\n        // SAFETY: The safety is upheld by the caller and the correctness of the capability value.\n        unsafe {\n            Self {\n                _queue_head: VolatileRef::new_read_only(base.add(0x80).cast::<u64>()),\n                queue_tail: VolatileRef::new(base.add(0x88).cast::<u64>()),\n                queue_addr: VolatileRef::new(base.add(0x90).cast::<u64>()),\n                completion_status: VolatileRef::new(base.add(0x9C).cast::<u32>()),\n                _completion_event_control: VolatileRef::new(base.add(0xA0).cast::<u32>()),\n                _completion_event_data: VolatileRef::new(base.add(0xA4).cast::<u32>()),\n                _completion_event_addr: VolatileRef::new(base.add(0xA8).cast::<u32>()),\n                _completion_event_upper_addr: VolatileRef::new(base.add(0xAC).cast::<u32>()),\n                _queue_error_record: VolatileRef::new_read_only(base.add(0xB0).cast::<u64>()),\n\n                _invalidate_address: VolatileRef::new_restricted(\n                    WriteOnly,\n                    base.add(offset).cast::<u64>(),\n                ),\n                iotlb_invalidate: VolatileRef::new(base.add(offset).add(0x08).cast::<u64>()),\n            }\n        }\n    }",
  "mir": "fn arch::iommu::registers::invalidation::InvalidationRegisters::new(_1: core::ptr::NonNull<u8>) -> arch::iommu::registers::invalidation::InvalidationRegisters {\n    let mut _0: arch::iommu::registers::invalidation::InvalidationRegisters;\n    let  _2: usize;\n    let  _3: volatile::VolatileRef<'_, u64, volatile::access::ReadOnly>;\n    let mut _4: core::ptr::NonNull<u64>;\n    let mut _5: core::ptr::NonNull<u8>;\n    let  _6: arch::iommu::registers::extended_cap::ExtendedCapability;\n    let mut _7: u64;\n    let mut _8: volatile::VolatilePtr<'_, u64, volatile::access::ReadOnly>;\n    let mut _9: &volatile::VolatileRef<'_, u64, volatile::access::ReadOnly>;\n    let mut _10: usize;\n    let mut _11: u64;\n    let mut _12: &arch::iommu::registers::extended_cap::ExtendedCapability;\n    let mut _13: (usize, bool);\n    let mut _14: volatile::VolatileRef<'_, u64, volatile::access::ReadOnly>;\n    let mut _15: core::ptr::NonNull<u64>;\n    let mut _16: core::ptr::NonNull<u8>;\n    let mut _17: volatile::VolatileRef<'_, u64>;\n    let mut _18: core::ptr::NonNull<u64>;\n    let mut _19: core::ptr::NonNull<u8>;\n    let mut _20: volatile::VolatileRef<'_, u64>;\n    let mut _21: core::ptr::NonNull<u64>;\n    let mut _22: core::ptr::NonNull<u8>;\n    let mut _23: volatile::VolatileRef<'_, u32>;\n    let mut _24: core::ptr::NonNull<u32>;\n    let mut _25: core::ptr::NonNull<u8>;\n    let mut _26: volatile::VolatileRef<'_, u32>;\n    let mut _27: core::ptr::NonNull<u32>;\n    let mut _28: core::ptr::NonNull<u8>;\n    let mut _29: volatile::VolatileRef<'_, u32>;\n    let mut _30: core::ptr::NonNull<u32>;\n    let mut _31: core::ptr::NonNull<u8>;\n    let mut _32: volatile::VolatileRef<'_, u32>;\n    let mut _33: core::ptr::NonNull<u32>;\n    let mut _34: core::ptr::NonNull<u8>;\n    let mut _35: volatile::VolatileRef<'_, u32>;\n    let mut _36: core::ptr::NonNull<u32>;\n    let mut _37: core::ptr::NonNull<u8>;\n    let mut _38: volatile::VolatileRef<'_, u64, volatile::access::ReadOnly>;\n    let mut _39: core::ptr::NonNull<u64>;\n    let mut _40: core::ptr::NonNull<u8>;\n    let mut _41: volatile::VolatileRef<'_, u64, volatile::access::WriteOnly>;\n    let mut _42: core::ptr::NonNull<u64>;\n    let mut _43: core::ptr::NonNull<u8>;\n    let mut _44: volatile::VolatileRef<'_, u64>;\n    let mut _45: core::ptr::NonNull<u64>;\n    let mut _46: core::ptr::NonNull<u8>;\n    let mut _47: core::ptr::NonNull<u8>;\n    debug base => _1;\n    debug offset => _2;\n    debug extended_capability => _3;\n    debug extend_cap => _6;\n    bb0: {\n        StorageLive(_3);\n        StorageLive(_4);\n        StorageLive(_5);\n        _5 = core::ptr::NonNull::<u8>::add(_1, 16_usize) -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        _4 = core::ptr::NonNull::<u8>::cast::<u64>(move _5) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        StorageDead(_5);\n        _3 = volatile::VolatileRef::<'_, u64>::new_read_only(move _4) -> [return: bb3, unwind unreachable];\n    }\n    bb3: {\n        StorageDead(_4);\n        StorageLive(_6);\n        StorageLive(_7);\n        StorageLive(_8);\n        StorageLive(_9);\n        _9 = &_3;\n        _8 = volatile::VolatileRef::<'_, u64, volatile::access::ReadOnly>::as_ptr(move _9) -> [return: bb4, unwind unreachable];\n    }\n    bb4: {\n        StorageDead(_9);\n        _7 = volatile::volatile_ptr::operations::<impl volatile::VolatilePtr<'_, u64, volatile::access::ReadOnly>>::read(move _8) -> [return: bb5, unwind unreachable];\n    }\n    bb5: {\n        StorageDead(_8);\n        _6 = arch::iommu::registers::extended_cap::ExtendedCapability::new(move _7) -> [return: bb6, unwind unreachable];\n    }\n    bb6: {\n        StorageDead(_7);\n        StorageLive(_10);\n        StorageLive(_11);\n        StorageLive(_12);\n        _12 = &_6;\n        _11 = arch::iommu::registers::extended_cap::ExtendedCapability::iotlb_register_offset(move _12) -> [return: bb7, unwind unreachable];\n    }\n    bb7: {\n        StorageDead(_12);\n        _10 = move _11 as usize;\n        StorageDead(_11);\n        _13 = CheckedMul(_10, 16_usize);\n        assert(!move (_13.1: bool), \"attempt to compute `{} * {}`, which would overflow\", move _10, 16_usize) -> [success: bb8, unwind unreachable];\n    }\n    bb8: {\n        _2 = move (_13.0: usize);\n        StorageDead(_10);\n        StorageDead(_6);\n        StorageDead(_3);\n        StorageLive(_14);\n        StorageLive(_15);\n        StorageLive(_16);\n        _16 = core::ptr::NonNull::<u8>::add(_1, 128_usize) -> [return: bb9, unwind unreachable];\n    }\n    bb9: {\n        _15 = core::ptr::NonNull::<u8>::cast::<u64>(move _16) -> [return: bb10, unwind unreachable];\n    }\n    bb10: {\n        StorageDead(_16);\n        _14 = volatile::VolatileRef::<'_, u64>::new_read_only(move _15) -> [return: bb11, unwind unreachable];\n    }\n    bb11: {\n        StorageDead(_15);\n        StorageLive(_17);\n        StorageLive(_18);\n        StorageLive(_19);\n        _19 = core::ptr::NonNull::<u8>::add(_1, 136_usize) -> [return: bb12, unwind unreachable];\n    }\n    bb12: {\n        _18 = core::ptr::NonNull::<u8>::cast::<u64>(move _19) -> [return: bb13, unwind unreachable];\n    }\n    bb13: {\n        StorageDead(_19);\n        _17 = volatile::VolatileRef::<'_, u64>::new(move _18) -> [return: bb14, unwind unreachable];\n    }\n    bb14: {\n        StorageDead(_18);\n        StorageLive(_20);\n        StorageLive(_21);\n        StorageLive(_22);\n        _22 = core::ptr::NonNull::<u8>::add(_1, 144_usize) -> [return: bb15, unwind unreachable];\n    }\n    bb15: {\n        _21 = core::ptr::NonNull::<u8>::cast::<u64>(move _22) -> [return: bb16, unwind unreachable];\n    }\n    bb16: {\n        StorageDead(_22);\n        _20 = volatile::VolatileRef::<'_, u64>::new(move _21) -> [return: bb17, unwind unreachable];\n    }\n    bb17: {\n        StorageDead(_21);\n        StorageLive(_23);\n        StorageLive(_24);\n        StorageLive(_25);\n        _25 = core::ptr::NonNull::<u8>::add(_1, 156_usize) -> [return: bb18, unwind unreachable];\n    }\n    bb18: {\n        _24 = core::ptr::NonNull::<u8>::cast::<u32>(move _25) -> [return: bb19, unwind unreachable];\n    }\n    bb19: {\n        StorageDead(_25);\n        _23 = volatile::VolatileRef::<'_, u32>::new(move _24) -> [return: bb20, unwind unreachable];\n    }\n    bb20: {\n        StorageDead(_24);\n        StorageLive(_26);\n        StorageLive(_27);\n        StorageLive(_28);\n        _28 = core::ptr::NonNull::<u8>::add(_1, 160_usize) -> [return: bb21, unwind unreachable];\n    }\n    bb21: {\n        _27 = core::ptr::NonNull::<u8>::cast::<u32>(move _28) -> [return: bb22, unwind unreachable];\n    }\n    bb22: {\n        StorageDead(_28);\n        _26 = volatile::VolatileRef::<'_, u32>::new(move _27) -> [return: bb23, unwind unreachable];\n    }\n    bb23: {\n        StorageDead(_27);\n        StorageLive(_29);\n        StorageLive(_30);\n        StorageLive(_31);\n        _31 = core::ptr::NonNull::<u8>::add(_1, 164_usize) -> [return: bb24, unwind unreachable];\n    }\n    bb24: {\n        _30 = core::ptr::NonNull::<u8>::cast::<u32>(move _31) -> [return: bb25, unwind unreachable];\n    }\n    bb25: {\n        StorageDead(_31);\n        _29 = volatile::VolatileRef::<'_, u32>::new(move _30) -> [return: bb26, unwind unreachable];\n    }\n    bb26: {\n        StorageDead(_30);\n        StorageLive(_32);\n        StorageLive(_33);\n        StorageLive(_34);\n        _34 = core::ptr::NonNull::<u8>::add(_1, 168_usize) -> [return: bb27, unwind unreachable];\n    }\n    bb27: {\n        _33 = core::ptr::NonNull::<u8>::cast::<u32>(move _34) -> [return: bb28, unwind unreachable];\n    }\n    bb28: {\n        StorageDead(_34);\n        _32 = volatile::VolatileRef::<'_, u32>::new(move _33) -> [return: bb29, unwind unreachable];\n    }\n    bb29: {\n        StorageDead(_33);\n        StorageLive(_35);\n        StorageLive(_36);\n        StorageLive(_37);\n        _37 = core::ptr::NonNull::<u8>::add(_1, 172_usize) -> [return: bb30, unwind unreachable];\n    }\n    bb30: {\n        _36 = core::ptr::NonNull::<u8>::cast::<u32>(move _37) -> [return: bb31, unwind unreachable];\n    }\n    bb31: {\n        StorageDead(_37);\n        _35 = volatile::VolatileRef::<'_, u32>::new(move _36) -> [return: bb32, unwind unreachable];\n    }\n    bb32: {\n        StorageDead(_36);\n        StorageLive(_38);\n        StorageLive(_39);\n        StorageLive(_40);\n        _40 = core::ptr::NonNull::<u8>::add(_1, 176_usize) -> [return: bb33, unwind unreachable];\n    }\n    bb33: {\n        _39 = core::ptr::NonNull::<u8>::cast::<u64>(move _40) -> [return: bb34, unwind unreachable];\n    }\n    bb34: {\n        StorageDead(_40);\n        _38 = volatile::VolatileRef::<'_, u64>::new_read_only(move _39) -> [return: bb35, unwind unreachable];\n    }\n    bb35: {\n        StorageDead(_39);\n        StorageLive(_41);\n        StorageLive(_42);\n        StorageLive(_43);\n        _43 = core::ptr::NonNull::<u8>::add(_1, _2) -> [return: bb36, unwind unreachable];\n    }\n    bb36: {\n        _42 = core::ptr::NonNull::<u8>::cast::<u64>(move _43) -> [return: bb37, unwind unreachable];\n    }\n    bb37: {\n        StorageDead(_43);\n        _41 = volatile::VolatileRef::<'_, u64>::new_restricted::<volatile::access::WriteOnly>(volatile::access::WriteOnly, move _42) -> [return: bb38, unwind unreachable];\n    }\n    bb38: {\n        StorageDead(_42);\n        StorageLive(_44);\n        StorageLive(_45);\n        StorageLive(_46);\n        StorageLive(_47);\n        _47 = core::ptr::NonNull::<u8>::add(_1, _2) -> [return: bb39, unwind unreachable];\n    }\n    bb39: {\n        _46 = core::ptr::NonNull::<u8>::add(move _47, 8_usize) -> [return: bb40, unwind unreachable];\n    }\n    bb40: {\n        StorageDead(_47);\n        _45 = core::ptr::NonNull::<u8>::cast::<u64>(move _46) -> [return: bb41, unwind unreachable];\n    }\n    bb41: {\n        StorageDead(_46);\n        _44 = volatile::VolatileRef::<'_, u64>::new(move _45) -> [return: bb42, unwind unreachable];\n    }\n    bb42: {\n        StorageDead(_45);\n        _0 = InvalidationRegisters(move _14, move _17, move _20, move _23, move _26, move _29, move _32, move _35, move _38, move _41, move _44);\n        StorageDead(_44);\n        StorageDead(_41);\n        StorageDead(_38);\n        StorageDead(_35);\n        StorageDead(_32);\n        StorageDead(_29);\n        StorageDead(_26);\n        StorageDead(_23);\n        StorageDead(_20);\n        StorageDead(_17);\n        StorageDead(_14);\n        return;\n    }\n}\n",
  "doc": " Creates an instance from the IOMMU base address.\n\n # Safety\n\n The caller must ensure that the base address is a valid IOMMU base address and that it has\n exclusive ownership of the IOMMU invalidation registers.\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}