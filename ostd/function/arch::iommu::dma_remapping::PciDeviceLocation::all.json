{
  "name": "arch::iommu::dma_remapping::PciDeviceLocation::all",
  "safe": true,
  "callees": {
    "core::ops::RangeInclusive::<Idx>::new": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a new inclusive range. Equivalent to writing `start..=end`.\n\n # Examples\n\n ```\n use std::ops::RangeInclusive;\n\n assert_eq!(3..=5, RangeInclusive::new(3, 5));\n ```\n",
      "adt": {}
    },
    "core::iter::Iterator::flat_map": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates an iterator that works like map, but flattens nested structure.\n\n The [`map`] adapter is very useful, but only when the closure\n argument produces values. If it produces an iterator instead, there's\n an extra layer of indirection. `flat_map()` will remove this extra layer\n on its own.\n\n You can think of `flat_map(f)` as the semantic equivalent\n of [`map`]ping, and then [`flatten`]ing as in `map(f).flatten()`.\n\n Another way of thinking about `flat_map()`: [`map`]'s closure returns\n one item for each element, and `flat_map()`'s closure returns an\n iterator for each element.\n\n [`map`]: Iterator::map\n [`flatten`]: Iterator::flatten\n\n # Examples\n\n ```\n let words = [\"alpha\", \"beta\", \"gamma\"];\n\n // chars() returns an iterator\n let merged: String = words.iter()\n                           .flat_map(|s| s.chars())\n                           .collect();\n assert_eq!(merged, \"alphabetagamma\");\n ```\n",
      "adt": {}
    },
    "core::iter::Iterator::map": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Takes a closure and creates an iterator which calls that closure on each\n element.\n\n `map()` transforms one iterator into another, by means of its argument:\n something that implements [`FnMut`]. It produces a new iterator which\n calls this closure on each element of the original iterator.\n\n If you are good at thinking in types, you can think of `map()` like this:\n If you have an iterator that gives you elements of some type `A`, and\n you want an iterator of some other type `B`, you can use `map()`,\n passing a closure that takes an `A` and returns a `B`.\n\n `map()` is conceptually similar to a [`for`] loop. However, as `map()` is\n lazy, it is best used when you're already working with other iterators.\n If you're doing some sort of looping for a side effect, it's considered\n more idiomatic to use [`for`] than `map()`.\n\n [`for`]: ../../book/ch03-05-control-flow.html#looping-through-a-collection-with-for\n\n # Examples\n\n Basic usage:\n\n ```\n let a = [1, 2, 3];\n\n let mut iter = a.iter().map(|x| 2 * x);\n\n assert_eq!(iter.next(), Some(2));\n assert_eq!(iter.next(), Some(4));\n assert_eq!(iter.next(), Some(6));\n assert_eq!(iter.next(), None);\n ```\n\n If you're doing some sort of side effect, prefer [`for`] to `map()`:\n\n ```\n # #![allow(unused_must_use)]\n // don't do this:\n (0..5).map(|x| println!(\"{x}\"));\n\n // it won't even execute, as it is lazy. Rust will warn you about this.\n\n // Instead, use a for-loop:\n for x in 0..5 {\n     println!(\"{x}\");\n }\n ```\n",
      "adt": {}
    }
  },
  "adts": {
    "core::ops::RangeInclusive": [
      "Plain"
    ],
    "core::iter::FlatMap": [
      "Plain"
    ],
    "core::iter::Map": [
      "Plain"
    ]
  },
  "path": 572,
  "span": "ostd/src/arch/x86/iommu/dma_remapping/mod.rs:48:5: 61:6",
  "src": "fn all() -> impl Iterator<Item = PciDeviceLocation> {\n        let all_bus = Self::MIN_BUS..=Self::MAX_BUS;\n        let all_dev = Self::MIN_DEVICE..=Self::MAX_DEVICE;\n        let all_func = Self::MIN_FUNCTION..=Self::MAX_FUNCTION;\n\n        all_bus\n            .flat_map(move |bus| all_dev.clone().map(move |dev| (bus, dev)))\n            .flat_map(move |(bus, dev)| all_func.clone().map(move |func| (bus, dev, func)))\n            .map(|(bus, dev, func)| PciDeviceLocation {\n                bus,\n                device: dev,\n                function: func,\n            })\n    }",
  "mir": "fn arch::iommu::dma_remapping::PciDeviceLocation::all() -> core::iter::Map<core::iter::FlatMap<core::iter::FlatMap<core::ops::RangeInclusive<u8>, core::iter::Map<core::ops::RangeInclusive<u8>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:54: 54:64}>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:23: 54:33}>, core::iter::Map<core::ops::RangeInclusive<u8>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:55:62: 55:73}>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:55:23: 55:40}>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:56:18: 56:36}> {\n    let mut _0: core::iter::Map<core::iter::FlatMap<core::iter::FlatMap<core::ops::RangeInclusive<u8>, core::iter::Map<core::ops::RangeInclusive<u8>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:54: 54:64}>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:23: 54:33}>, core::iter::Map<core::ops::RangeInclusive<u8>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:55:62: 55:73}>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:55:23: 55:40}>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:56:18: 56:36}>;\n    let  _1: core::ops::RangeInclusive<u8>;\n    let  _2: core::ops::RangeInclusive<u8>;\n    let  _3: core::ops::RangeInclusive<u8>;\n    let mut _4: core::iter::FlatMap<core::iter::FlatMap<core::ops::RangeInclusive<u8>, core::iter::Map<core::ops::RangeInclusive<u8>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:54: 54:64}>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:23: 54:33}>, core::iter::Map<core::ops::RangeInclusive<u8>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:55:62: 55:73}>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:55:23: 55:40}>;\n    let mut _5: core::iter::FlatMap<core::ops::RangeInclusive<u8>, core::iter::Map<core::ops::RangeInclusive<u8>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:54: 54:64}>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:23: 54:33}>;\n    let mut _6: {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:23: 54:33};\n    let mut _7: {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:55:23: 55:40};\n    debug all_bus => _1;\n    debug all_dev => _2;\n    debug all_func => _3;\n    bb0: {\n        _1 = core::ops::RangeInclusive::<u8>::new(arch::iommu::dma_remapping::PciDeviceLocation::MIN_BUS, arch::iommu::dma_remapping::PciDeviceLocation::MAX_BUS) -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        StorageLive(_2);\n        _2 = core::ops::RangeInclusive::<u8>::new(arch::iommu::dma_remapping::PciDeviceLocation::MIN_DEVICE, arch::iommu::dma_remapping::PciDeviceLocation::MAX_DEVICE) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        StorageLive(_3);\n        _3 = core::ops::RangeInclusive::<u8>::new(arch::iommu::dma_remapping::PciDeviceLocation::MIN_FUNCTION, arch::iommu::dma_remapping::PciDeviceLocation::MAX_FUNCTION) -> [return: bb3, unwind unreachable];\n    }\n    bb3: {\n        StorageLive(_4);\n        StorageLive(_5);\n        StorageLive(_6);\n        _6 = {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:23: 54:33}(move _2);\n        _5 = <core::ops::RangeInclusive<u8> as core::iter::Iterator>::flat_map::<core::iter::Map<core::ops::RangeInclusive<u8>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:54: 54:64}>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:23: 54:33}>(_1, move _6) -> [return: bb4, unwind unreachable];\n    }\n    bb4: {\n        StorageDead(_6);\n        StorageLive(_7);\n        _7 = {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:55:23: 55:40}(move _3);\n        _4 = <core::iter::FlatMap<core::ops::RangeInclusive<u8>, core::iter::Map<core::ops::RangeInclusive<u8>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:54: 54:64}>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:23: 54:33}> as core::iter::Iterator>::flat_map::<core::iter::Map<core::ops::RangeInclusive<u8>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:55:62: 55:73}>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:55:23: 55:40}>(move _5, move _7) -> [return: bb5, unwind unreachable];\n    }\n    bb5: {\n        StorageDead(_7);\n        StorageDead(_5);\n        _0 = <core::iter::FlatMap<core::iter::FlatMap<core::ops::RangeInclusive<u8>, core::iter::Map<core::ops::RangeInclusive<u8>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:54: 54:64}>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:54:23: 54:33}>, core::iter::Map<core::ops::RangeInclusive<u8>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:55:62: 55:73}>, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:55:23: 55:40}> as core::iter::Iterator>::map::<arch::iommu::dma_remapping::PciDeviceLocation, {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:56:18: 56:36}>(move _4, ZeroSized: {closure@ostd/src/arch/x86/iommu/dma_remapping/mod.rs:56:18: 56:36}) -> [return: bb6, unwind unreachable];\n    }\n    bb6: {\n        StorageDead(_4);\n        StorageDead(_3);\n        StorageDead(_2);\n        return;\n    }\n}\n",
  "doc": " Returns an iterator that enumerates all possible PCI device locations.\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}