{
  "name": "ostd::io::io_mem::IoMem::<SecuritySensitivity>::new",
  "span": "ostd/src/io/io_mem/mod.rs:83:5: 83:96",
  "src": "pub(crate) unsafe fn new(range: Range<Paddr>, flags: PageFlags, cache: CachePolicy) -> Self {\n        let first_page_start = range.start.align_down(PAGE_SIZE);\n        let last_page_end = range.end.align_up(PAGE_SIZE);\n\n        let frames_range = first_page_start..last_page_end;\n        let area_size = frames_range.len();\n\n        #[cfg(target_arch = \"x86_64\")]\n        let priv_flags = crate::arch::if_tdx_enabled!({\n            assert!(\n                first_page_start == range.start && last_page_end == range.end,\n                \"I/O memory is not page aligned, which cannot be unprotected in TDX: {:#x?}..{:#x?}\",\n                range.start,\n                range.end,\n            );\n\n            // SAFETY:\n            //  - The range `first_page_start..last_page_end` is always page aligned.\n            //  - FIXME: We currently do not limit the I/O memory allocator with the maximum GPA,\n            //    so the address range may not fall in the GPA limit.\n            //  - The caller guarantees that operations on the I/O memory do not have any side\n            //    effects that may cause soundness problems, so the pages can safely be viewed as\n            //    untyped memory.\n            unsafe { crate::arch::tdx_guest::unprotect_gpa_tdvm_call(first_page_start, area_size).unwrap() };\n\n            PrivilegedPageFlags::SHARED\n        } else {\n            PrivilegedPageFlags::empty()\n        });\n        #[cfg(not(target_arch = \"x86_64\"))]\n        let priv_flags = PrivilegedPageFlags::empty();\n\n        let prop = PageProperty {\n            flags,\n            cache,\n            priv_flags,\n        };\n\n        let kva = {\n            // SAFETY: The caller of `IoMem::new()` ensures that the given\n            // physical address range is I/O memory, so it is safe to map.\n            let kva = unsafe { KVirtArea::map_untracked_frames(area_size, 0, frames_range, prop) };\n\n            let target_cpus = AtomicCpuSet::new(CpuSet::new_full());\n            let mut flusher = TlbFlusher::new(&target_cpus, disable_preempt());\n            flusher.issue_tlb_flush(TlbFlushOp::for_range(kva.range()));\n            flusher.dispatch_tlb_flush();\n            flusher.sync_tlb_flush();\n\n            kva\n        };\n\n        Self {\n            kvirt_area: Arc::new(kva),\n            offset: range.start - first_page_start,\n            limit: range.len(),\n            pa: range.start,\n            cache_policy: cache,\n            phantom: PhantomData,\n        }\n    }"
}