{
  "name": "ostd::mm::kspace::init_kernel_page_table",
  "span": "ostd/src/mm/kspace/mod.rs:197:1: 197:65",
  "src": "pub fn init_kernel_page_table(meta_pages: Segment<MetaPageMeta>) {\n    info!(\"Initializing the kernel page table\");\n\n    // Start to initialize the kernel page table.\n    let kpt = PageTable::<KernelPtConfig>::new_kernel_page_table();\n    let preempt_guard = disable_preempt();\n\n    // In LoongArch64, we don't need to do linear mappings for the kernel because of DMW0.\n    #[cfg(not(target_arch = \"loongarch64\"))]\n    // Do linear mappings for the kernel.\n    {\n        let max_paddr = crate::mm::frame::max_paddr();\n        let from = LINEAR_MAPPING_BASE_VADDR..LINEAR_MAPPING_BASE_VADDR + max_paddr;\n        let prop = PageProperty {\n            flags: PageFlags::RW,\n            cache: CachePolicy::Writeback,\n            priv_flags: PrivilegedPageFlags::GLOBAL,\n        };\n        let mut cursor = kpt.cursor_mut(&preempt_guard, &from).unwrap();\n        for (pa, level) in largest_pages::<KernelPtConfig>(from.start, 0, max_paddr) {\n            // SAFETY: we are doing the linear mapping for the kernel.\n            unsafe { cursor.map(MappedItem::Untracked(pa, level, prop)) }\n                .expect(\"Kernel linear address space is mapped twice\");\n        }\n    }\n\n    // Map the metadata pages.\n    {\n        let start_va = mapping::frame_to_meta::<PagingConsts>(0);\n        let from = start_va..start_va + meta_pages.size();\n        let prop = PageProperty {\n            flags: PageFlags::RW,\n            cache: CachePolicy::Writeback,\n            priv_flags: PrivilegedPageFlags::GLOBAL,\n        };\n        let mut cursor = kpt.cursor_mut(&preempt_guard, &from).unwrap();\n        // We use untracked mapping so that we can benefit from huge pages.\n        // We won't unmap them anyway, so there's no leaking problem yet.\n        // TODO: support tracked huge page mapping.\n        let pa_range = meta_pages.into_raw();\n        for (pa, level) in\n            largest_pages::<KernelPtConfig>(from.start, pa_range.start, pa_range.len())\n        {\n            // SAFETY: We are doing the metadata mappings for the kernel.\n            unsafe { cursor.map(MappedItem::Untracked(pa, level, prop)) }\n                .expect(\"Frame metadata address space is mapped twice\");\n        }\n    }\n\n    // In LoongArch64, we don't need to do linear mappings for the kernel code because of DMW0.\n    #[cfg(not(target_arch = \"loongarch64\"))]\n    // Map for the kernel code itself.\n    // TODO: set separated permissions for each segments in the kernel.\n    {\n        let regions = &crate::boot::EARLY_INFO.get().unwrap().memory_regions;\n        let region = regions\n            .iter()\n            .find(|r| r.typ() == MemoryRegionType::Kernel)\n            .unwrap();\n        let offset = kernel_loaded_offset();\n        let from = region.base() + offset..region.end() + offset;\n        let prop = PageProperty {\n            flags: PageFlags::RWX,\n            cache: CachePolicy::Writeback,\n            priv_flags: PrivilegedPageFlags::GLOBAL,\n        };\n        let mut cursor = kpt.cursor_mut(&preempt_guard, &from).unwrap();\n        for (pa, level) in largest_pages::<KernelPtConfig>(from.start, region.base(), from.len()) {\n            // SAFETY: we are doing the kernel code mapping.\n            unsafe { cursor.map(MappedItem::Untracked(pa, level, prop)) }\n                .expect(\"Kernel code mapped twice\");\n        }\n    }\n\n    KERNEL_PAGE_TABLE.call_once(|| kpt);\n}"
}