{
  "name": "std::backtrace_rs::symbolize::gimli::Cache::avma_to_svma",
  "span": "$library/std/src/../../backtrace/src/symbolize/gimli.rs:383:5: 383:74",
  "src": "fn avma_to_svma(&self, addr: *const u8) -> Option<(usize, *const u8)> {\n        self.libraries\n            .iter()\n            .enumerate()\n            .filter_map(|(i, lib)| {\n                // First up, test if this `lib` has any segment containing the\n                // `addr` (handling relocation). If this check passes then we\n                // can continue below and actually translate the address.\n                //\n                // Note that we're using `wrapping_add` here to avoid overflow\n                // checks. It's been seen in the wild that the SVMA + bias\n                // computation overflows. It seems a bit odd that would happen\n                // but there's not a huge amount we can do about it other than\n                // probably just ignore those segments since they're likely\n                // pointing off into space. This originally came up in\n                // rust-lang/backtrace-rs#329.\n                if !lib.segments.iter().any(|s| {\n                    let svma = s.stated_virtual_memory_address;\n                    let start = svma.wrapping_add(lib.bias);\n                    let end = start.wrapping_add(s.len);\n                    let address = addr as usize;\n                    start <= address && address < end\n                }) {\n                    return None;\n                }\n\n                // Now that we know `lib` contains `addr`, we can offset with\n                // the bias to find the stated virtual memory address.\n                let svma = (addr as usize).wrapping_sub(lib.bias);\n                Some((i, svma as *const u8))\n            })\n            .next()\n    }"
}