{
  "name": "std::sync::mpmc::array::Channel::<T>::start_recv",
  "span": "$library/std/src/sync/mpmc/array.rs:215:5: 215:52",
  "mir": "fn std::sync::mpmc::array::Channel::<T>::start_recv(_1: &sync::mpmc::array::Channel<T>, _2: &mut sync::mpmc::select::Token) -> bool {\n    let mut _0: bool;\n    let  _3: sync::mpmc::utils::Backoff;\n    let mut _4: usize;\n    let  _5: &core::sync::atomic::AtomicUsize;\n    let mut _6: &sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>;\n    let mut _7: core::sync::atomic::Ordering;\n    let  _8: usize;\n    let mut _9: usize;\n    let mut _10: usize;\n    let mut _11: usize;\n    let mut _12: (usize, bool);\n    let  _13: usize;\n    let mut _14: usize;\n    let mut _15: usize;\n    let mut _16: usize;\n    let mut _17: usize;\n    let mut _18: (usize, bool);\n    let mut _19: bool;\n    let mut _20: usize;\n    let mut _21: &[sync::mpmc::array::Slot<T>];\n    let mut _22: !;\n    let  _23: &sync::mpmc::array::Slot<T>;\n    let mut _24: &[sync::mpmc::array::Slot<T>];\n    let  _25: usize;\n    let mut _26: &core::sync::atomic::AtomicUsize;\n    let mut _27: core::sync::atomic::Ordering;\n    let mut _28: bool;\n    let mut _29: usize;\n    let mut _30: usize;\n    let mut _31: (usize, bool);\n    let  _32: usize;\n    let mut _33: bool;\n    let mut _34: usize;\n    let mut _35: (usize, bool);\n    let mut _36: usize;\n    let mut _37: usize;\n    let mut _38: (usize, bool);\n    let mut _39: usize;\n    let mut _40: core::result::Result<usize, usize>;\n    let  _41: &core::sync::atomic::AtomicUsize;\n    let mut _42: &sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>;\n    let mut _43: usize;\n    let mut _44: usize;\n    let mut _45: core::sync::atomic::Ordering;\n    let mut _46: core::sync::atomic::Ordering;\n    let mut _47: isize;\n    let mut _48: *const sync::mpmc::array::Slot<T>;\n    let mut _49: usize;\n    let mut _50: usize;\n    let mut _51: usize;\n    let  _52: ();\n    let mut _53: &sync::mpmc::utils::Backoff;\n    let mut _54: usize;\n    let  _55: &core::sync::atomic::AtomicUsize;\n    let mut _56: &sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>;\n    let mut _57: core::sync::atomic::Ordering;\n    let mut _58: bool;\n    let mut _59: usize;\n    let  _60: ();\n    let mut _61: core::sync::atomic::Ordering;\n    let  _62: usize;\n    let  _63: &core::sync::atomic::AtomicUsize;\n    let mut _64: &sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>;\n    let mut _65: core::sync::atomic::Ordering;\n    let mut _66: bool;\n    let mut _67: usize;\n    let mut _68: usize;\n    let mut _69: usize;\n    let mut _70: usize;\n    let mut _71: usize;\n    let mut _72: usize;\n    let mut _73: *const u8;\n    let  _74: ();\n    let mut _75: &sync::mpmc::utils::Backoff;\n    let mut _76: usize;\n    let  _77: &core::sync::atomic::AtomicUsize;\n    let mut _78: &sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>;\n    let mut _79: core::sync::atomic::Ordering;\n    let  _80: ();\n    let mut _81: &sync::mpmc::utils::Backoff;\n    let mut _82: usize;\n    let  _83: &core::sync::atomic::AtomicUsize;\n    let mut _84: &sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>;\n    let mut _85: core::sync::atomic::Ordering;\n    let mut _86: alloc_crate::boxed::Box<[sync::mpmc::array::Slot<T>]>;\n    let mut _87: alloc_crate::boxed::Box<[sync::mpmc::array::Slot<T>]>;\n    let mut _88: *const [sync::mpmc::array::Slot<T>];\n    let mut _89: *const [sync::mpmc::array::Slot<T>];\n    debug self => _1;\n    debug token => _2;\n    debug backoff => _3;\n    debug head => _4;\n    debug index => _8;\n    debug lap => _13;\n    debug slot => _23;\n    debug stamp => _25;\n    debug new => _32;\n    debug tail => _62;\n    bb0: {\n        StorageLive(_3);\n        _3 = sync::mpmc::utils::Backoff::new() -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        StorageLive(_4);\n        StorageLive(_6);\n        _6 = &((*_1).0: sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>);\n        _5 = <sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize> as core::ops::Deref>::deref(move _6) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        StorageDead(_6);\n        StorageLive(_7);\n        _7 = core::sync::atomic::Ordering::Relaxed;\n        _4 = core::sync::atomic::AtomicUsize::load(_5, move _7) -> [return: bb3, unwind unreachable];\n    }\n    bb3: {\n        StorageDead(_7);\n        goto -> bb4;\n    }\n    bb4: {\n        StorageLive(_9);\n        _9 = _4;\n        StorageLive(_10);\n        StorageLive(_11);\n        _11 = ((*_1).5: usize);\n        _12 = CheckedSub(_11, 1_usize);\n        assert(!move (_12.1: bool), \"attempt to compute `{} - {}`, which would overflow\", move _11, 1_usize) -> [success: bb5, unwind unreachable];\n    }\n    bb5: {\n        _10 = move (_12.0: usize);\n        StorageDead(_11);\n        _8 = BitAnd(move _9, move _10);\n        StorageDead(_10);\n        StorageDead(_9);\n        StorageLive(_14);\n        _14 = _4;\n        StorageLive(_15);\n        StorageLive(_16);\n        StorageLive(_17);\n        _17 = ((*_1).4: usize);\n        _18 = CheckedSub(_17, 1_usize);\n        assert(!move (_18.1: bool), \"attempt to compute `{} - {}`, which would overflow\", move _17, 1_usize) -> [success: bb6, unwind unreachable];\n    }\n    bb6: {\n        _16 = move (_18.0: usize);\n        StorageDead(_17);\n        _15 = Not(move _16);\n        StorageDead(_16);\n        _13 = BitAnd(move _14, move _15);\n        StorageDead(_15);\n        StorageDead(_14);\n        StorageLive(_19);\n        StorageLive(_20);\n        StorageLive(_21);\n        _86 = ((*_1).2: alloc_crate::boxed::Box<[sync::mpmc::array::Slot<T>]>);\n        _88 = ((_86.0: core::ptr::Unique<[sync::mpmc::array::Slot<T>]>).0: core::ptr::NonNull<[sync::mpmc::array::Slot<T>]>) as *const [sync::mpmc::array::Slot<T>];\n        _21 = &(*_88);\n        _20 = PtrMetadata(move _21);\n        StorageDead(_21);\n        _19 = Lt(_8, move _20);\n        switchInt(move _19) -> [0: bb8, otherwise: bb7];\n    }\n    bb7: {\n        StorageDead(_20);\n        StorageDead(_19);\n        StorageLive(_23);\n        StorageLive(_24);\n        _87 = ((*_1).2: alloc_crate::boxed::Box<[sync::mpmc::array::Slot<T>]>);\n        _89 = ((_87.0: core::ptr::Unique<[sync::mpmc::array::Slot<T>]>).0: core::ptr::NonNull<[sync::mpmc::array::Slot<T>]>) as *const [sync::mpmc::array::Slot<T>];\n        _24 = &(*_89);\n        _23 = core::slice::<impl [sync::mpmc::array::Slot<T>]>::get_unchecked::<usize>(move _24, _8) -> [return: bb9, unwind unreachable];\n    }\n    bb8: {\n        StorageDead(_20);\n        _22 = core::panicking::panic(\"assertion failed: index < self.buffer.len()\") -> unwind unreachable;\n    }\n    bb9: {\n        StorageDead(_24);\n        StorageLive(_26);\n        _26 = &((*_23).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_27);\n        _27 = core::sync::atomic::Ordering::Acquire;\n        _25 = core::sync::atomic::AtomicUsize::load(move _26, move _27) -> [return: bb10, unwind unreachable];\n    }\n    bb10: {\n        StorageDead(_27);\n        StorageDead(_26);\n        StorageLive(_28);\n        StorageLive(_29);\n        StorageLive(_30);\n        _30 = _4;\n        _31 = CheckedAdd(_30, 1_usize);\n        assert(!move (_31.1: bool), \"attempt to compute `{} + {}`, which would overflow\", move _30, 1_usize) -> [success: bb11, unwind unreachable];\n    }\n    bb11: {\n        _29 = move (_31.0: usize);\n        StorageDead(_30);\n        _28 = Eq(move _29, _25);\n        switchInt(move _28) -> [0: bb28, otherwise: bb12];\n    }\n    bb12: {\n        StorageDead(_29);\n        StorageLive(_32);\n        StorageLive(_33);\n        StorageLive(_34);\n        _35 = CheckedAdd(_8, 1_usize);\n        assert(!move (_35.1: bool), \"attempt to compute `{} + {}`, which would overflow\", _8, 1_usize) -> [success: bb13, unwind unreachable];\n    }\n    bb13: {\n        _34 = move (_35.0: usize);\n        StorageLive(_36);\n        _36 = ((*_1).3: usize);\n        _33 = Lt(move _34, move _36);\n        switchInt(move _33) -> [0: bb16, otherwise: bb14];\n    }\n    bb14: {\n        StorageDead(_36);\n        StorageDead(_34);\n        StorageLive(_37);\n        _37 = _4;\n        _38 = CheckedAdd(_37, 1_usize);\n        assert(!move (_38.1: bool), \"attempt to compute `{} + {}`, which would overflow\", move _37, 1_usize) -> [success: bb15, unwind unreachable];\n    }\n    bb15: {\n        _32 = move (_38.0: usize);\n        StorageDead(_37);\n        goto -> bb18;\n    }\n    bb16: {\n        StorageDead(_36);\n        StorageDead(_34);\n        StorageLive(_39);\n        _39 = ((*_1).4: usize);\n        _32 = core::num::<impl usize>::wrapping_add(_13, move _39) -> [return: bb17, unwind unreachable];\n    }\n    bb17: {\n        StorageDead(_39);\n        goto -> bb18;\n    }\n    bb18: {\n        StorageDead(_33);\n        StorageLive(_40);\n        StorageLive(_42);\n        _42 = &((*_1).0: sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>);\n        _41 = <sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize> as core::ops::Deref>::deref(move _42) -> [return: bb19, unwind unreachable];\n    }\n    bb19: {\n        StorageDead(_42);\n        StorageLive(_43);\n        _43 = _4;\n        StorageLive(_44);\n        _44 = _32;\n        StorageLive(_45);\n        _45 = core::sync::atomic::Ordering::SeqCst;\n        StorageLive(_46);\n        _46 = core::sync::atomic::Ordering::Relaxed;\n        _40 = core::sync::atomic::AtomicUsize::compare_exchange_weak(_41, move _43, move _44, move _45, move _46) -> [return: bb20, unwind unreachable];\n    }\n    bb20: {\n        StorageDead(_46);\n        StorageDead(_45);\n        StorageDead(_44);\n        StorageDead(_43);\n        _47 = discriminant(_40);\n        switchInt(move _47) -> [0: bb23, 1: bb22, otherwise: bb21];\n    }\n    bb21: {\n        unreachable;\n    }\n    bb22: {\n        StorageLive(_53);\n        _53 = &_3;\n        _52 = sync::mpmc::utils::Backoff::spin_light(move _53) -> [return: bb25, unwind unreachable];\n    }\n    bb23: {\n        StorageLive(_48);\n        _48 = &raw const (*_23);\n        (((*_2).0: sync::mpmc::array::ArrayToken).0: *const u8) = move _48 as *const u8;\n        StorageDead(_48);\n        StorageLive(_49);\n        StorageLive(_50);\n        _50 = _4;\n        StorageLive(_51);\n        _51 = ((*_1).4: usize);\n        _49 = core::num::<impl usize>::wrapping_add(move _50, move _51) -> [return: bb24, unwind unreachable];\n    }\n    bb24: {\n        StorageDead(_51);\n        StorageDead(_50);\n        (((*_2).0: sync::mpmc::array::ArrayToken).1: usize) = move _49;\n        StorageDead(_49);\n        _0 = true;\n        StorageDead(_40);\n        StorageDead(_32);\n        goto -> bb48;\n    }\n    bb25: {\n        StorageDead(_53);\n        StorageLive(_54);\n        StorageLive(_56);\n        _56 = &((*_1).0: sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>);\n        _55 = <sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize> as core::ops::Deref>::deref(move _56) -> [return: bb26, unwind unreachable];\n    }\n    bb26: {\n        StorageDead(_56);\n        StorageLive(_57);\n        _57 = core::sync::atomic::Ordering::Relaxed;\n        _54 = core::sync::atomic::AtomicUsize::load(_55, move _57) -> [return: bb27, unwind unreachable];\n    }\n    bb27: {\n        StorageDead(_57);\n        _4 = move _54;\n        StorageDead(_54);\n        StorageDead(_40);\n        StorageDead(_32);\n        goto -> bb46;\n    }\n    bb28: {\n        StorageDead(_29);\n        StorageLive(_58);\n        StorageLive(_59);\n        _59 = _4;\n        _58 = Eq(_25, move _59);\n        switchInt(move _58) -> [0: bb41, otherwise: bb29];\n    }\n    bb29: {\n        StorageDead(_59);\n        StorageLive(_61);\n        _61 = core::sync::atomic::Ordering::SeqCst;\n        _60 = core::sync::atomic::fence(move _61) -> [return: bb30, unwind unreachable];\n    }\n    bb30: {\n        StorageDead(_61);\n        StorageLive(_64);\n        _64 = &((*_1).1: sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>);\n        _63 = <sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize> as core::ops::Deref>::deref(move _64) -> [return: bb31, unwind unreachable];\n    }\n    bb31: {\n        StorageDead(_64);\n        StorageLive(_65);\n        _65 = core::sync::atomic::Ordering::Relaxed;\n        _62 = core::sync::atomic::AtomicUsize::load(_63, move _65) -> [return: bb32, unwind unreachable];\n    }\n    bb32: {\n        StorageDead(_65);\n        StorageLive(_66);\n        StorageLive(_67);\n        StorageLive(_68);\n        StorageLive(_69);\n        _69 = ((*_1).5: usize);\n        _68 = Not(move _69);\n        StorageDead(_69);\n        _67 = BitAnd(_62, move _68);\n        StorageDead(_68);\n        StorageLive(_70);\n        _70 = _4;\n        _66 = Eq(move _67, move _70);\n        switchInt(move _66) -> [0: bb37, otherwise: bb33];\n    }\n    bb33: {\n        StorageDead(_70);\n        StorageDead(_67);\n        StorageLive(_71);\n        StorageLive(_72);\n        _72 = ((*_1).5: usize);\n        _71 = BitAnd(_62, move _72);\n        StorageDead(_72);\n        switchInt(move _71) -> [0: bb36, otherwise: bb34];\n    }\n    bb34: {\n        StorageDead(_71);\n        StorageLive(_73);\n        _73 = core::ptr::null::<u8>() -> [return: bb35, unwind unreachable];\n    }\n    bb35: {\n        (((*_2).0: sync::mpmc::array::ArrayToken).0: *const u8) = move _73;\n        StorageDead(_73);\n        (((*_2).0: sync::mpmc::array::ArrayToken).1: usize) = 0_usize;\n        _0 = true;\n        goto -> bb47;\n    }\n    bb36: {\n        StorageDead(_71);\n        _0 = false;\n        goto -> bb47;\n    }\n    bb37: {\n        StorageDead(_70);\n        StorageDead(_67);\n        StorageDead(_66);\n        StorageLive(_75);\n        _75 = &_3;\n        _74 = sync::mpmc::utils::Backoff::spin_light(move _75) -> [return: bb38, unwind unreachable];\n    }\n    bb38: {\n        StorageDead(_75);\n        StorageLive(_76);\n        StorageLive(_78);\n        _78 = &((*_1).0: sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>);\n        _77 = <sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize> as core::ops::Deref>::deref(move _78) -> [return: bb39, unwind unreachable];\n    }\n    bb39: {\n        StorageDead(_78);\n        StorageLive(_79);\n        _79 = core::sync::atomic::Ordering::Relaxed;\n        _76 = core::sync::atomic::AtomicUsize::load(_77, move _79) -> [return: bb40, unwind unreachable];\n    }\n    bb40: {\n        StorageDead(_79);\n        _4 = move _76;\n        StorageDead(_76);\n        goto -> bb45;\n    }\n    bb41: {\n        StorageDead(_59);\n        StorageLive(_81);\n        _81 = &_3;\n        _80 = sync::mpmc::utils::Backoff::spin_heavy(move _81) -> [return: bb42, unwind unreachable];\n    }\n    bb42: {\n        StorageDead(_81);\n        StorageLive(_82);\n        StorageLive(_84);\n        _84 = &((*_1).0: sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>);\n        _83 = <sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize> as core::ops::Deref>::deref(move _84) -> [return: bb43, unwind unreachable];\n    }\n    bb43: {\n        StorageDead(_84);\n        StorageLive(_85);\n        _85 = core::sync::atomic::Ordering::Relaxed;\n        _82 = core::sync::atomic::AtomicUsize::load(_83, move _85) -> [return: bb44, unwind unreachable];\n    }\n    bb44: {\n        StorageDead(_85);\n        _4 = move _82;\n        StorageDead(_82);\n        goto -> bb45;\n    }\n    bb45: {\n        StorageDead(_58);\n        goto -> bb46;\n    }\n    bb46: {\n        StorageDead(_28);\n        StorageDead(_23);\n        goto -> bb4;\n    }\n    bb47: {\n        StorageDead(_66);\n        StorageDead(_58);\n        goto -> bb48;\n    }\n    bb48: {\n        StorageDead(_28);\n        StorageDead(_23);\n        StorageDead(_4);\n        StorageDead(_3);\n        return;\n    }\n}\n"
}