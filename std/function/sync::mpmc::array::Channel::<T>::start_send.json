{
  "name": "sync::mpmc::array::Channel::<T>::start_send",
  "safe": true,
  "callees": {
    "sync::mpmc::utils::Backoff::new": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a new `Backoff`.\n",
      "adt": {
        "sync::mpmc::utils::Backoff": "Constructor"
      }
    },
    "core::ops::Deref::deref": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Dereferences the value.\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicUsize::load": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Loads a value from the atomic integer.\n\n `load` takes an [`Ordering`] argument which describes the memory ordering of this operation.\n Possible values are [`SeqCst`], [`Acquire`] and [`Relaxed`].\n\n # Panics\n\n Panics if `order` is [`Release`] or [`AcqRel`].\n\n # Examples\n\n ```\n\n\n assert_eq!(some_var.load(Ordering::Relaxed), 5);\n ```\n",
      "adt": {}
    },
    "core::ptr::null": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a null raw pointer.\n\n This function is equivalent to zero-initializing the pointer:\n `MaybeUninit::<*const T>::zeroed().assume_init()`.\n The resulting pointer has the address 0.\n\n # Examples\n\n ```\n use std::ptr;\n\n let p: *const i32 = ptr::null();\n assert!(p.is_null());\n assert_eq!(p as usize, 0); // this pointer has the address 0\n ```\n",
      "adt": {}
    },
    "core::slice::<impl [T]>::get_unchecked": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns a reference to an element or subslice, without doing bounds\n checking.\n\n For a safe alternative see [`get`].\n\n # Safety\n\n Calling this method with an out-of-bounds index is *[undefined behavior]*\n even if the resulting reference is not used.\n\n You can think of this like `.get(index).unwrap_unchecked()`.  It's UB\n to call `.get_unchecked(len)`, even if you immediately convert to a\n pointer.  And it's UB to call `.get_unchecked(..len + 1)`,\n `.get_unchecked(..=len)`, or similar.\n\n [`get`]: slice::get\n [undefined behavior]: https://doc.rust-lang.org/reference/behavior-considered-undefined.html\n\n # Examples\n\n ```\n let x = &[1, 2, 4];\n\n unsafe {\n     assert_eq!(x.get_unchecked(1), &2);\n }\n ```\n",
      "adt": {}
    },
    "core::panicking::panic": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " The underlying implementation of core's `panic!` macro when no formatting is used.\n",
      "adt": {}
    },
    "core::num::<impl usize>::wrapping_add": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Wrapping (modular) addition. Computes `self + rhs`,\n wrapping around at the boundary of the type.\n\n # Examples\n\n ```\n ```\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicUsize::compare_exchange_weak": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Stores a value into the atomic integer if the current value is the same as\n the `current` value.\n\n this function is allowed to spuriously fail even\n when the comparison succeeds, which can result in more efficient code on some\n platforms. The return value is a result indicating whether the new value was\n written and containing the previous value.\n\n `compare_exchange_weak` takes two [`Ordering`] arguments to describe the memory\n ordering of this operation. `success` describes the required ordering for the\n read-modify-write operation that takes place if the comparison with `current` succeeds.\n `failure` describes the required ordering for the load operation that takes place when\n the comparison fails. Using [`Acquire`] as success ordering makes the store part\n of this operation [`Relaxed`], and using [`Release`] makes the successful load\n [`Relaxed`]. The failure ordering can only be [`SeqCst`], [`Acquire`] or [`Relaxed`].\n\n **Note**: This method is only available on platforms that support atomic operations on\n\n # Examples\n\n ```\n\n\n let mut old = val.load(Ordering::Relaxed);\n loop {\n     let new = old * 2;\n     match val.compare_exchange_weak(old, new, Ordering::SeqCst, Ordering::Relaxed) {\n         Ok(_) => break,\n         Err(x) => old = x,\n     }\n }\n ```\n\n # Considerations\n\n `compare_exchange` is a [compare-and-swap operation] and thus exhibits the usual downsides\n of CAS operations. In particular, a load of the value followed by a successful\n `compare_exchange` with the previous load *does not ensure* that other threads have not\n changed the value in the interim. This is usually important when the *equality* check in\n the `compare_exchange` is being used to check the *identity* of a value, but equality\n does not necessarily imply identity. This is a particularly common case for pointers, as\n a pointer holding the same address does not imply that the same object exists at that\n address! In this case, `compare_exchange` can lead to the [ABA problem].\n\n [ABA Problem]: https://en.wikipedia.org/wiki/ABA_problem\n [compare-and-swap operation]: https://en.wikipedia.org/wiki/Compare-and-swap\n",
      "adt": {}
    },
    "sync::mpmc::utils::Backoff::spin_light": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Backs off using lightweight spinning.\n\n This method should be used for retrying an operation because another thread made\n progress. i.e. on CAS failure.\n",
      "adt": {
        "sync::mpmc::utils::Backoff": "ImmutableAsArgument"
      }
    },
    "core::sync::atomic::fence": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " An atomic fence.\n\n Fences create synchronization between themselves and atomic operations or fences in other\n threads. To achieve this, a fence prevents the compiler and CPU from reordering certain types of\n memory operations around it.\n\n There are 3 different ways to use an atomic fence:\n\n - atomic - fence synchronization: an atomic operation with (at least) [`Release`] ordering\n   semantics synchronizes with a fence with (at least) [`Acquire`] ordering semantics.\n - fence - atomic synchronization: a fence with (at least) [`Release`] ordering semantics\n   synchronizes with an atomic operation with (at least) [`Acquire`] ordering semantics.\n - fence - fence synchronization: a fence with (at least) [`Release`] ordering semantics\n   synchronizes with a fence with (at least) [`Acquire`] ordering semantics.\n\n These 3 ways complement the regular, fence-less, atomic - atomic synchronization.\n\n ## Atomic - Fence\n\n An atomic operation on one thread will synchronize with a fence on another thread when:\n\n -   on thread 1:\n     -   an atomic operation 'X' with (at least) [`Release`] ordering semantics on some atomic\n         object 'm',\n\n -   is paired on thread 2 with:\n     -   an atomic read 'Y' with any order on 'm',\n     -   followed by a fence 'B' with (at least) [`Acquire`] ordering semantics.\n\n This provides a happens-before dependence between X and B.\n\n ```text\n     Thread 1                                          Thread 2\n\n m.store(3, Release); X ---------\n                                |\n                                |\n                                -------------> Y  if m.load(Relaxed) == 3 {\n                                               B      fence(Acquire);\n                                                      ...\n                                                  }\n ```\n\n ## Fence - Atomic\n\n A fence on one thread will synchronize with an atomic operation on another thread when:\n\n -   on thread:\n     -   a fence 'A' with (at least) [`Release`] ordering semantics,\n     -   followed by an atomic write 'X' with any ordering on some atomic object 'm',\n\n -   is paired on thread 2 with:\n     -   an atomic operation 'Y' with (at least) [`Acquire`] ordering semantics.\n\n This provides a happens-before dependence between A and Y.\n\n ```text\n     Thread 1                                          Thread 2\n\n fence(Release);      A\n m.store(3, Relaxed); X ---------\n                                |\n                                |\n                                -------------> Y  if m.load(Acquire) == 3 {\n                                                      ...\n                                                  }\n ```\n\n ## Fence - Fence\n\n A fence on one thread will synchronize with a fence on another thread when:\n\n -   on thread 1:\n     -   a fence 'A' which has (at least) [`Release`] ordering semantics,\n     -   followed by an atomic write 'X' with any ordering on some atomic object 'm',\n\n -   is paired on thread 2 with:\n     -   an atomic read 'Y' with any ordering on 'm',\n     -   followed by a fence 'B' with (at least) [`Acquire`] ordering semantics.\n\n This provides a happens-before dependence between A and B.\n\n ```text\n     Thread 1                                          Thread 2\n\n fence(Release);      A --------------\n m.store(3, Relaxed); X ---------    |\n                                |    |\n                                |    |\n                                -------------> Y  if m.load(Relaxed) == 3 {\n                                     |-------> B      fence(Acquire);\n                                                      ...\n                                                  }\n ```\n\n ## Mandatory Atomic\n\n Note that in the examples above, it is crucial that the access to `m` are atomic. Fences cannot\n be used to establish synchronization between non-atomic accesses in different threads. However,\n thanks to the happens-before relationship, any non-atomic access that happen-before the atomic\n operation or fence with (at least) [`Release`] ordering semantics are now also properly\n synchronized with any non-atomic accesses that happen-after the atomic operation or fence with\n (at least) [`Acquire`] ordering semantics.\n\n ## Memory Ordering\n\n A fence which has [`SeqCst`] ordering, in addition to having both [`Acquire`] and [`Release`]\n semantics, participates in the global program order of the other [`SeqCst`] operations and/or\n fences.\n\n Accepts [`Acquire`], [`Release`], [`AcqRel`] and [`SeqCst`] orderings.\n\n # Panics\n\n Panics if `order` is [`Relaxed`].\n\n # Examples\n\n ```\n use std::sync::atomic::AtomicBool;\n use std::sync::atomic::fence;\n use std::sync::atomic::Ordering;\n\n // A mutual exclusion primitive based on spinlock.\n pub struct Mutex {\n     flag: AtomicBool,\n }\n\n impl Mutex {\n     pub fn new() -> Mutex {\n         Mutex {\n             flag: AtomicBool::new(false),\n         }\n     }\n\n     pub fn lock(&self) {\n         // Wait until the old value is `false`.\n         while self\n             .flag\n             .compare_exchange_weak(false, true, Ordering::Relaxed, Ordering::Relaxed)\n             .is_err()\n         {}\n         // This fence synchronizes-with store in `unlock`.\n         fence(Ordering::Acquire);\n     }\n\n     pub fn unlock(&self) {\n         self.flag.store(false, Ordering::Release);\n     }\n }\n ```\n",
      "adt": {}
    },
    "sync::mpmc::utils::Backoff::spin_heavy": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Backs off using heavyweight spinning.\n\n This method should be used in blocking loops where parking the thread is not an option.\n",
      "adt": {
        "sync::mpmc::utils::Backoff": "ImmutableAsArgument"
      }
    }
  },
  "adts": {
    "sync::mpmc::utils::Backoff": [
      "Plain",
      "Ref"
    ],
    "sync::mpmc::utils::CachePadded": [
      "Ref"
    ],
    "sync::mpmc::array::Channel": [
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(1)))",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(5)))",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(4)))",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(2)))",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(3)))",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))",
      "Ref"
    ],
    "core::sync::atomic::AtomicUsize": [
      "Ref"
    ],
    "core::sync::atomic::Ordering": [
      "Plain"
    ],
    "sync::mpmc::select::Token": [
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))",
      "MutRef"
    ],
    "alloc_crate::boxed::Box": [
      "Plain",
      "Unknown([Field(0, Ty { id: 10569, kind: RigidTy(Adt(AdtDef(DefId { id: 4689, name: \"core::ptr::Unique\" }), GenericArgs([Type(Ty { id: 10563, kind: RigidTy(Slice(Ty { id: 10561, kind: RigidTy(Adt(AdtDef(DefId { id: 9258, name: \"sync::mpmc::array::Slot\" }), GenericArgs([Type(Ty { id: 9, kind: Param(ParamTy { index: 0, name: \"T\" }) })]))) })) })]))) }), Field(0, Ty { id: 10570, kind: RigidTy(Adt(AdtDef(DefId { id: 4690, name: \"core::ptr::NonNull\" }), GenericArgs([Type(Ty { id: 10563, kind: RigidTy(Slice(Ty { id: 10561, kind: RigidTy(Adt(AdtDef(DefId { id: 9258, name: \"sync::mpmc::array::Slot\" }), GenericArgs([Type(Ty { id: 9, kind: Param(ParamTy { index: 0, name: \"T\" }) })]))) })) })]))) })])"
    ],
    "sync::mpmc::array::Slot": [
      "Ref",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))",
      "Deref"
    ],
    "core::result::Result": [
      "Plain"
    ]
  },
  "path": 2795,
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sync/mpmc/array.rs:124:5: 193:6",
  "src": "fn start_send(&self, token: &mut Token) -> bool {\n        let backoff = Backoff::new();\n        let mut tail = self.tail.load(Ordering::Relaxed);\n\n        loop {\n            // Check if the channel is disconnected.\n            if tail & self.mark_bit != 0 {\n                token.array.slot = ptr::null();\n                token.array.stamp = 0;\n                return true;\n            }\n\n            // Deconstruct the tail.\n            let index = tail & (self.mark_bit - 1);\n            let lap = tail & !(self.one_lap - 1);\n\n            // Inspect the corresponding slot.\n            debug_assert!(index < self.buffer.len());\n            let slot = unsafe { self.buffer.get_unchecked(index) };\n            let stamp = slot.stamp.load(Ordering::Acquire);\n\n            // If the tail and the stamp match, we may attempt to push.\n            if tail == stamp {\n                let new_tail = if index + 1 < self.cap {\n                    // Same lap, incremented index.\n                    // Set to `{ lap: lap, mark: 0, index: index + 1 }`.\n                    tail + 1\n                } else {\n                    // One lap forward, index wraps around to zero.\n                    // Set to `{ lap: lap.wrapping_add(1), mark: 0, index: 0 }`.\n                    lap.wrapping_add(self.one_lap)\n                };\n\n                // Try moving the tail.\n                match self.tail.compare_exchange_weak(\n                    tail,\n                    new_tail,\n                    Ordering::SeqCst,\n                    Ordering::Relaxed,\n                ) {\n                    Ok(_) => {\n                        // Prepare the token for the follow-up call to `write`.\n                        token.array.slot = slot as *const Slot<T> as *const u8;\n                        token.array.stamp = tail + 1;\n                        return true;\n                    }\n                    Err(_) => {\n                        backoff.spin_light();\n                        tail = self.tail.load(Ordering::Relaxed);\n                    }\n                }\n            } else if stamp.wrapping_add(self.one_lap) == tail + 1 {\n                atomic::fence(Ordering::SeqCst);\n                let head = self.head.load(Ordering::Relaxed);\n\n                // If the head lags one lap behind the tail as well...\n                if head.wrapping_add(self.one_lap) == tail {\n                    // ...then the channel is full.\n                    return false;\n                }\n\n                backoff.spin_light();\n                tail = self.tail.load(Ordering::Relaxed);\n            } else {\n                // Snooze because we need to wait for the stamp to get updated.\n                backoff.spin_heavy();\n                tail = self.tail.load(Ordering::Relaxed);\n            }\n        }\n    }",
  "mir": "fn sync::mpmc::array::Channel::<T>::start_send(_1: &sync::mpmc::array::Channel<T>, _2: &mut sync::mpmc::select::Token) -> bool {\n    let mut _0: bool;\n    let  _3: sync::mpmc::utils::Backoff;\n    let mut _4: usize;\n    let  _5: &core::sync::atomic::AtomicUsize;\n    let mut _6: &sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>;\n    let mut _7: core::sync::atomic::Ordering;\n    let mut _8: usize;\n    let mut _9: usize;\n    let mut _10: usize;\n    let mut _11: *const u8;\n    let  _12: usize;\n    let mut _13: usize;\n    let mut _14: usize;\n    let mut _15: usize;\n    let mut _16: (usize, bool);\n    let  _17: usize;\n    let mut _18: usize;\n    let mut _19: usize;\n    let mut _20: usize;\n    let mut _21: usize;\n    let mut _22: (usize, bool);\n    let mut _23: bool;\n    let mut _24: usize;\n    let mut _25: &[sync::mpmc::array::Slot<T>];\n    let mut _26: !;\n    let  _27: &sync::mpmc::array::Slot<T>;\n    let mut _28: &[sync::mpmc::array::Slot<T>];\n    let  _29: usize;\n    let mut _30: &core::sync::atomic::AtomicUsize;\n    let mut _31: core::sync::atomic::Ordering;\n    let mut _32: bool;\n    let mut _33: usize;\n    let  _34: usize;\n    let mut _35: bool;\n    let mut _36: usize;\n    let mut _37: (usize, bool);\n    let mut _38: usize;\n    let mut _39: usize;\n    let mut _40: (usize, bool);\n    let mut _41: usize;\n    let mut _42: core::result::Result<usize, usize>;\n    let  _43: &core::sync::atomic::AtomicUsize;\n    let mut _44: &sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>;\n    let mut _45: usize;\n    let mut _46: usize;\n    let mut _47: core::sync::atomic::Ordering;\n    let mut _48: core::sync::atomic::Ordering;\n    let mut _49: isize;\n    let mut _50: *const sync::mpmc::array::Slot<T>;\n    let mut _51: usize;\n    let mut _52: (usize, bool);\n    let  _53: ();\n    let mut _54: &sync::mpmc::utils::Backoff;\n    let mut _55: usize;\n    let  _56: &core::sync::atomic::AtomicUsize;\n    let mut _57: &sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>;\n    let mut _58: core::sync::atomic::Ordering;\n    let mut _59: bool;\n    let mut _60: usize;\n    let mut _61: usize;\n    let mut _62: usize;\n    let mut _63: usize;\n    let mut _64: (usize, bool);\n    let  _65: ();\n    let mut _66: core::sync::atomic::Ordering;\n    let  _67: usize;\n    let  _68: &core::sync::atomic::AtomicUsize;\n    let mut _69: &sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>;\n    let mut _70: core::sync::atomic::Ordering;\n    let mut _71: bool;\n    let mut _72: usize;\n    let mut _73: usize;\n    let mut _74: usize;\n    let  _75: ();\n    let mut _76: &sync::mpmc::utils::Backoff;\n    let mut _77: usize;\n    let  _78: &core::sync::atomic::AtomicUsize;\n    let mut _79: &sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>;\n    let mut _80: core::sync::atomic::Ordering;\n    let  _81: ();\n    let mut _82: &sync::mpmc::utils::Backoff;\n    let mut _83: usize;\n    let  _84: &core::sync::atomic::AtomicUsize;\n    let mut _85: &sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>;\n    let mut _86: core::sync::atomic::Ordering;\n    let mut _87: alloc_crate::boxed::Box<[sync::mpmc::array::Slot<T>]>;\n    let mut _88: alloc_crate::boxed::Box<[sync::mpmc::array::Slot<T>]>;\n    let mut _89: *const [sync::mpmc::array::Slot<T>];\n    let mut _90: *const [sync::mpmc::array::Slot<T>];\n    debug self => _1;\n    debug token => _2;\n    debug backoff => _3;\n    debug tail => _4;\n    debug index => _12;\n    debug lap => _17;\n    debug slot => _27;\n    debug stamp => _29;\n    debug new_tail => _34;\n    debug head => _67;\n    bb0: {\n        StorageLive(_3);\n        _3 = sync::mpmc::utils::Backoff::new() -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        StorageLive(_4);\n        StorageLive(_6);\n        _6 = &((*_1).1: sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>);\n        _5 = <sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize> as core::ops::Deref>::deref(move _6) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        StorageDead(_6);\n        StorageLive(_7);\n        _7 = core::sync::atomic::Ordering::Relaxed;\n        _4 = core::sync::atomic::AtomicUsize::load(_5, move _7) -> [return: bb3, unwind unreachable];\n    }\n    bb3: {\n        StorageDead(_7);\n        goto -> bb4;\n    }\n    bb4: {\n        StorageLive(_8);\n        StorageLive(_9);\n        _9 = _4;\n        StorageLive(_10);\n        _10 = ((*_1).5: usize);\n        _8 = BitAnd(move _9, move _10);\n        StorageDead(_10);\n        StorageDead(_9);\n        switchInt(move _8) -> [0: bb7, otherwise: bb5];\n    }\n    bb5: {\n        StorageDead(_8);\n        StorageLive(_11);\n        _11 = core::ptr::null::<u8>() -> [return: bb6, unwind unreachable];\n    }\n    bb6: {\n        (((*_2).0: sync::mpmc::array::ArrayToken).0: *const u8) = move _11;\n        StorageDead(_11);\n        (((*_2).0: sync::mpmc::array::ArrayToken).1: usize) = 0_usize;\n        _0 = true;\n        goto -> bb50;\n    }\n    bb7: {\n        StorageDead(_8);\n        StorageLive(_13);\n        _13 = _4;\n        StorageLive(_14);\n        StorageLive(_15);\n        _15 = ((*_1).5: usize);\n        _16 = CheckedSub(_15, 1_usize);\n        assert(!move (_16.1: bool), \"attempt to compute `{} - {}`, which would overflow\", move _15, 1_usize) -> [success: bb8, unwind unreachable];\n    }\n    bb8: {\n        _14 = move (_16.0: usize);\n        StorageDead(_15);\n        _12 = BitAnd(move _13, move _14);\n        StorageDead(_14);\n        StorageDead(_13);\n        StorageLive(_18);\n        _18 = _4;\n        StorageLive(_19);\n        StorageLive(_20);\n        StorageLive(_21);\n        _21 = ((*_1).4: usize);\n        _22 = CheckedSub(_21, 1_usize);\n        assert(!move (_22.1: bool), \"attempt to compute `{} - {}`, which would overflow\", move _21, 1_usize) -> [success: bb9, unwind unreachable];\n    }\n    bb9: {\n        _20 = move (_22.0: usize);\n        StorageDead(_21);\n        _19 = Not(move _20);\n        StorageDead(_20);\n        _17 = BitAnd(move _18, move _19);\n        StorageDead(_19);\n        StorageDead(_18);\n        StorageLive(_23);\n        StorageLive(_24);\n        StorageLive(_25);\n        _87 = ((*_1).2: alloc_crate::boxed::Box<[sync::mpmc::array::Slot<T>]>);\n        _89 = ((_87.0: core::ptr::Unique<[sync::mpmc::array::Slot<T>]>).0: core::ptr::NonNull<[sync::mpmc::array::Slot<T>]>) as *const [sync::mpmc::array::Slot<T>];\n        _25 = &(*_89);\n        _24 = PtrMetadata(move _25);\n        StorageDead(_25);\n        _23 = Lt(_12, move _24);\n        switchInt(move _23) -> [0: bb11, otherwise: bb10];\n    }\n    bb10: {\n        StorageDead(_24);\n        StorageDead(_23);\n        StorageLive(_27);\n        StorageLive(_28);\n        _88 = ((*_1).2: alloc_crate::boxed::Box<[sync::mpmc::array::Slot<T>]>);\n        _90 = ((_88.0: core::ptr::Unique<[sync::mpmc::array::Slot<T>]>).0: core::ptr::NonNull<[sync::mpmc::array::Slot<T>]>) as *const [sync::mpmc::array::Slot<T>];\n        _28 = &(*_90);\n        _27 = core::slice::<impl [sync::mpmc::array::Slot<T>]>::get_unchecked::<usize>(move _28, _12) -> [return: bb12, unwind unreachable];\n    }\n    bb11: {\n        StorageDead(_24);\n        _26 = core::panicking::panic(\"assertion failed: index < self.buffer.len()\") -> unwind unreachable;\n    }\n    bb12: {\n        StorageDead(_28);\n        StorageLive(_30);\n        _30 = &((*_27).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_31);\n        _31 = core::sync::atomic::Ordering::Acquire;\n        _29 = core::sync::atomic::AtomicUsize::load(move _30, move _31) -> [return: bb13, unwind unreachable];\n    }\n    bb13: {\n        StorageDead(_31);\n        StorageDead(_30);\n        StorageLive(_32);\n        StorageLive(_33);\n        _33 = _4;\n        _32 = Eq(move _33, _29);\n        switchInt(move _32) -> [0: bb30, otherwise: bb14];\n    }\n    bb14: {\n        StorageDead(_33);\n        StorageLive(_34);\n        StorageLive(_35);\n        StorageLive(_36);\n        _37 = CheckedAdd(_12, 1_usize);\n        assert(!move (_37.1: bool), \"attempt to compute `{} + {}`, which would overflow\", _12, 1_usize) -> [success: bb15, unwind unreachable];\n    }\n    bb15: {\n        _36 = move (_37.0: usize);\n        StorageLive(_38);\n        _38 = ((*_1).3: usize);\n        _35 = Lt(move _36, move _38);\n        switchInt(move _35) -> [0: bb18, otherwise: bb16];\n    }\n    bb16: {\n        StorageDead(_38);\n        StorageDead(_36);\n        StorageLive(_39);\n        _39 = _4;\n        _40 = CheckedAdd(_39, 1_usize);\n        assert(!move (_40.1: bool), \"attempt to compute `{} + {}`, which would overflow\", move _39, 1_usize) -> [success: bb17, unwind unreachable];\n    }\n    bb17: {\n        _34 = move (_40.0: usize);\n        StorageDead(_39);\n        goto -> bb20;\n    }\n    bb18: {\n        StorageDead(_38);\n        StorageDead(_36);\n        StorageLive(_41);\n        _41 = ((*_1).4: usize);\n        _34 = core::num::<impl usize>::wrapping_add(_17, move _41) -> [return: bb19, unwind unreachable];\n    }\n    bb19: {\n        StorageDead(_41);\n        goto -> bb20;\n    }\n    bb20: {\n        StorageDead(_35);\n        StorageLive(_42);\n        StorageLive(_44);\n        _44 = &((*_1).1: sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>);\n        _43 = <sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize> as core::ops::Deref>::deref(move _44) -> [return: bb21, unwind unreachable];\n    }\n    bb21: {\n        StorageDead(_44);\n        StorageLive(_45);\n        _45 = _4;\n        StorageLive(_46);\n        _46 = _34;\n        StorageLive(_47);\n        _47 = core::sync::atomic::Ordering::SeqCst;\n        StorageLive(_48);\n        _48 = core::sync::atomic::Ordering::Relaxed;\n        _42 = core::sync::atomic::AtomicUsize::compare_exchange_weak(_43, move _45, move _46, move _47, move _48) -> [return: bb22, unwind unreachable];\n    }\n    bb22: {\n        StorageDead(_48);\n        StorageDead(_47);\n        StorageDead(_46);\n        StorageDead(_45);\n        _49 = discriminant(_42);\n        switchInt(move _49) -> [0: bb25, 1: bb24, otherwise: bb23];\n    }\n    bb23: {\n        unreachable;\n    }\n    bb24: {\n        StorageLive(_54);\n        _54 = &_3;\n        _53 = sync::mpmc::utils::Backoff::spin_light(move _54) -> [return: bb27, unwind unreachable];\n    }\n    bb25: {\n        StorageLive(_50);\n        _50 = &raw const (*_27);\n        (((*_2).0: sync::mpmc::array::ArrayToken).0: *const u8) = move _50 as *const u8;\n        StorageDead(_50);\n        StorageLive(_51);\n        _51 = _4;\n        _52 = CheckedAdd(_51, 1_usize);\n        assert(!move (_52.1: bool), \"attempt to compute `{} + {}`, which would overflow\", move _51, 1_usize) -> [success: bb26, unwind unreachable];\n    }\n    bb26: {\n        (((*_2).0: sync::mpmc::array::ArrayToken).1: usize) = move (_52.0: usize);\n        StorageDead(_51);\n        _0 = true;\n        StorageDead(_42);\n        StorageDead(_34);\n        goto -> bb49;\n    }\n    bb27: {\n        StorageDead(_54);\n        StorageLive(_55);\n        StorageLive(_57);\n        _57 = &((*_1).1: sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>);\n        _56 = <sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize> as core::ops::Deref>::deref(move _57) -> [return: bb28, unwind unreachable];\n    }\n    bb28: {\n        StorageDead(_57);\n        StorageLive(_58);\n        _58 = core::sync::atomic::Ordering::Relaxed;\n        _55 = core::sync::atomic::AtomicUsize::load(_56, move _58) -> [return: bb29, unwind unreachable];\n    }\n    bb29: {\n        StorageDead(_58);\n        _4 = move _55;\n        StorageDead(_55);\n        StorageDead(_42);\n        StorageDead(_34);\n        goto -> bb48;\n    }\n    bb30: {\n        StorageDead(_33);\n        StorageLive(_59);\n        StorageLive(_60);\n        StorageLive(_61);\n        _61 = ((*_1).4: usize);\n        _60 = core::num::<impl usize>::wrapping_add(_29, move _61) -> [return: bb31, unwind unreachable];\n    }\n    bb31: {\n        StorageDead(_61);\n        StorageLive(_62);\n        StorageLive(_63);\n        _63 = _4;\n        _64 = CheckedAdd(_63, 1_usize);\n        assert(!move (_64.1: bool), \"attempt to compute `{} + {}`, which would overflow\", move _63, 1_usize) -> [success: bb32, unwind unreachable];\n    }\n    bb32: {\n        _62 = move (_64.0: usize);\n        StorageDead(_63);\n        _59 = Eq(move _60, move _62);\n        switchInt(move _59) -> [0: bb43, otherwise: bb33];\n    }\n    bb33: {\n        StorageDead(_62);\n        StorageDead(_60);\n        StorageLive(_66);\n        _66 = core::sync::atomic::Ordering::SeqCst;\n        _65 = core::sync::atomic::fence(move _66) -> [return: bb34, unwind unreachable];\n    }\n    bb34: {\n        StorageDead(_66);\n        StorageLive(_69);\n        _69 = &((*_1).0: sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>);\n        _68 = <sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize> as core::ops::Deref>::deref(move _69) -> [return: bb35, unwind unreachable];\n    }\n    bb35: {\n        StorageDead(_69);\n        StorageLive(_70);\n        _70 = core::sync::atomic::Ordering::Relaxed;\n        _67 = core::sync::atomic::AtomicUsize::load(_68, move _70) -> [return: bb36, unwind unreachable];\n    }\n    bb36: {\n        StorageDead(_70);\n        StorageLive(_71);\n        StorageLive(_72);\n        StorageLive(_73);\n        _73 = ((*_1).4: usize);\n        _72 = core::num::<impl usize>::wrapping_add(_67, move _73) -> [return: bb37, unwind unreachable];\n    }\n    bb37: {\n        StorageDead(_73);\n        StorageLive(_74);\n        _74 = _4;\n        _71 = Eq(move _72, move _74);\n        switchInt(move _71) -> [0: bb39, otherwise: bb38];\n    }\n    bb38: {\n        StorageDead(_74);\n        StorageDead(_72);\n        _0 = false;\n        StorageDead(_71);\n        StorageDead(_59);\n        goto -> bb49;\n    }\n    bb39: {\n        StorageDead(_74);\n        StorageDead(_72);\n        StorageDead(_71);\n        StorageLive(_76);\n        _76 = &_3;\n        _75 = sync::mpmc::utils::Backoff::spin_light(move _76) -> [return: bb40, unwind unreachable];\n    }\n    bb40: {\n        StorageDead(_76);\n        StorageLive(_77);\n        StorageLive(_79);\n        _79 = &((*_1).1: sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>);\n        _78 = <sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize> as core::ops::Deref>::deref(move _79) -> [return: bb41, unwind unreachable];\n    }\n    bb41: {\n        StorageDead(_79);\n        StorageLive(_80);\n        _80 = core::sync::atomic::Ordering::Relaxed;\n        _77 = core::sync::atomic::AtomicUsize::load(_78, move _80) -> [return: bb42, unwind unreachable];\n    }\n    bb42: {\n        StorageDead(_80);\n        _4 = move _77;\n        StorageDead(_77);\n        goto -> bb47;\n    }\n    bb43: {\n        StorageDead(_62);\n        StorageDead(_60);\n        StorageLive(_82);\n        _82 = &_3;\n        _81 = sync::mpmc::utils::Backoff::spin_heavy(move _82) -> [return: bb44, unwind unreachable];\n    }\n    bb44: {\n        StorageDead(_82);\n        StorageLive(_83);\n        StorageLive(_85);\n        _85 = &((*_1).1: sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize>);\n        _84 = <sync::mpmc::utils::CachePadded<core::sync::atomic::AtomicUsize> as core::ops::Deref>::deref(move _85) -> [return: bb45, unwind unreachable];\n    }\n    bb45: {\n        StorageDead(_85);\n        StorageLive(_86);\n        _86 = core::sync::atomic::Ordering::Relaxed;\n        _83 = core::sync::atomic::AtomicUsize::load(_84, move _86) -> [return: bb46, unwind unreachable];\n    }\n    bb46: {\n        StorageDead(_86);\n        _4 = move _83;\n        StorageDead(_83);\n        goto -> bb47;\n    }\n    bb47: {\n        StorageDead(_59);\n        goto -> bb48;\n    }\n    bb48: {\n        StorageDead(_32);\n        StorageDead(_27);\n        goto -> bb4;\n    }\n    bb49: {\n        StorageDead(_32);\n        StorageDead(_27);\n        goto -> bb50;\n    }\n    bb50: {\n        StorageDead(_4);\n        StorageDead(_3);\n        return;\n    }\n}\n",
  "doc": " Attempts to reserve a slot for sending a message.\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}