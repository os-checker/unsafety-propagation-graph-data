{
  "name": "sync::mpmc::list::Channel::<T>::start_recv",
  "safe": true,
  "callees": {
    "sync::mpmc::utils::Backoff::new": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a new `Backoff`.\n",
      "adt": {
        "sync::mpmc::utils::Backoff": "Constructor"
      }
    },
    "core::ops::Deref::deref": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Dereferences the value.\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicUsize::load": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Loads a value from the atomic integer.\n\n `load` takes an [`Ordering`] argument which describes the memory ordering of this operation.\n Possible values are [`SeqCst`], [`Acquire`] and [`Relaxed`].\n\n # Panics\n\n Panics if `order` is [`Release`] or [`AcqRel`].\n\n # Examples\n\n ```\n\n\n assert_eq!(some_var.load(Ordering::Relaxed), 5);\n ```\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicPtr::<T>::load": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Loads a value from the pointer.\n\n `load` takes an [`Ordering`] argument which describes the memory ordering\n of this operation. Possible values are [`SeqCst`], [`Acquire`] and [`Relaxed`].\n\n # Panics\n\n Panics if `order` is [`Release`] or [`AcqRel`].\n\n # Examples\n\n ```\n use std::sync::atomic::{AtomicPtr, Ordering};\n\n let ptr = &mut 5;\n let some_ptr = AtomicPtr::new(ptr);\n\n let value = some_ptr.load(Ordering::Relaxed);\n ```\n",
      "adt": {}
    },
    "sync::mpmc::utils::Backoff::spin_heavy": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Backs off using heavyweight spinning.\n\n This method should be used in blocking loops where parking the thread is not an option.\n",
      "adt": {
        "sync::mpmc::utils::Backoff": "ImmutableAsArgument"
      }
    },
    "core::sync::atomic::fence": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " An atomic fence.\n\n Fences create synchronization between themselves and atomic operations or fences in other\n threads. To achieve this, a fence prevents the compiler and CPU from reordering certain types of\n memory operations around it.\n\n There are 3 different ways to use an atomic fence:\n\n - atomic - fence synchronization: an atomic operation with (at least) [`Release`] ordering\n   semantics synchronizes with a fence with (at least) [`Acquire`] ordering semantics.\n - fence - atomic synchronization: a fence with (at least) [`Release`] ordering semantics\n   synchronizes with an atomic operation with (at least) [`Acquire`] ordering semantics.\n - fence - fence synchronization: a fence with (at least) [`Release`] ordering semantics\n   synchronizes with a fence with (at least) [`Acquire`] ordering semantics.\n\n These 3 ways complement the regular, fence-less, atomic - atomic synchronization.\n\n ## Atomic - Fence\n\n An atomic operation on one thread will synchronize with a fence on another thread when:\n\n -   on thread 1:\n     -   an atomic operation 'X' with (at least) [`Release`] ordering semantics on some atomic\n         object 'm',\n\n -   is paired on thread 2 with:\n     -   an atomic read 'Y' with any order on 'm',\n     -   followed by a fence 'B' with (at least) [`Acquire`] ordering semantics.\n\n This provides a happens-before dependence between X and B.\n\n ```text\n     Thread 1                                          Thread 2\n\n m.store(3, Release); X ---------\n                                |\n                                |\n                                -------------> Y  if m.load(Relaxed) == 3 {\n                                               B      fence(Acquire);\n                                                      ...\n                                                  }\n ```\n\n ## Fence - Atomic\n\n A fence on one thread will synchronize with an atomic operation on another thread when:\n\n -   on thread:\n     -   a fence 'A' with (at least) [`Release`] ordering semantics,\n     -   followed by an atomic write 'X' with any ordering on some atomic object 'm',\n\n -   is paired on thread 2 with:\n     -   an atomic operation 'Y' with (at least) [`Acquire`] ordering semantics.\n\n This provides a happens-before dependence between A and Y.\n\n ```text\n     Thread 1                                          Thread 2\n\n fence(Release);      A\n m.store(3, Relaxed); X ---------\n                                |\n                                |\n                                -------------> Y  if m.load(Acquire) == 3 {\n                                                      ...\n                                                  }\n ```\n\n ## Fence - Fence\n\n A fence on one thread will synchronize with a fence on another thread when:\n\n -   on thread 1:\n     -   a fence 'A' which has (at least) [`Release`] ordering semantics,\n     -   followed by an atomic write 'X' with any ordering on some atomic object 'm',\n\n -   is paired on thread 2 with:\n     -   an atomic read 'Y' with any ordering on 'm',\n     -   followed by a fence 'B' with (at least) [`Acquire`] ordering semantics.\n\n This provides a happens-before dependence between A and B.\n\n ```text\n     Thread 1                                          Thread 2\n\n fence(Release);      A --------------\n m.store(3, Relaxed); X ---------    |\n                                |    |\n                                |    |\n                                -------------> Y  if m.load(Relaxed) == 3 {\n                                     |-------> B      fence(Acquire);\n                                                      ...\n                                                  }\n ```\n\n ## Mandatory Atomic\n\n Note that in the examples above, it is crucial that the access to `m` are atomic. Fences cannot\n be used to establish synchronization between non-atomic accesses in different threads. However,\n thanks to the happens-before relationship, any non-atomic access that happen-before the atomic\n operation or fence with (at least) [`Release`] ordering semantics are now also properly\n synchronized with any non-atomic accesses that happen-after the atomic operation or fence with\n (at least) [`Acquire`] ordering semantics.\n\n ## Memory Ordering\n\n A fence which has [`SeqCst`] ordering, in addition to having both [`Acquire`] and [`Release`]\n semantics, participates in the global program order of the other [`SeqCst`] operations and/or\n fences.\n\n Accepts [`Acquire`], [`Release`], [`AcqRel`] and [`SeqCst`] orderings.\n\n # Panics\n\n Panics if `order` is [`Relaxed`].\n\n # Examples\n\n ```\n use std::sync::atomic::AtomicBool;\n use std::sync::atomic::fence;\n use std::sync::atomic::Ordering;\n\n // A mutual exclusion primitive based on spinlock.\n pub struct Mutex {\n     flag: AtomicBool,\n }\n\n impl Mutex {\n     pub fn new() -> Mutex {\n         Mutex {\n             flag: AtomicBool::new(false),\n         }\n     }\n\n     pub fn lock(&self) {\n         // Wait until the old value is `false`.\n         while self\n             .flag\n             .compare_exchange_weak(false, true, Ordering::Relaxed, Ordering::Relaxed)\n             .is_err()\n         {}\n         // This fence synchronizes-with store in `unlock`.\n         fence(Ordering::Acquire);\n     }\n\n     pub fn unlock(&self) {\n         self.flag.store(false, Ordering::Release);\n     }\n }\n ```\n",
      "adt": {}
    },
    "core::ptr::null": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a null raw pointer.\n\n This function is equivalent to zero-initializing the pointer:\n `MaybeUninit::<*const T>::zeroed().assume_init()`.\n The resulting pointer has the address 0.\n\n # Examples\n\n ```\n use std::ptr;\n\n let p: *const i32 = ptr::null();\n assert!(p.is_null());\n assert_eq!(p as usize, 0); // this pointer has the address 0\n ```\n",
      "adt": {}
    },
    "core::ptr::mut_ptr::<impl *mut T>::is_null": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "\n # Examples\n\n ```\n let mut s = [1, 2, 3];\n let ptr: *mut u32 = s.as_mut_ptr();\n assert!(!ptr.is_null());\n ```\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicUsize::compare_exchange_weak": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Stores a value into the atomic integer if the current value is the same as\n the `current` value.\n\n this function is allowed to spuriously fail even\n when the comparison succeeds, which can result in more efficient code on some\n platforms. The return value is a result indicating whether the new value was\n written and containing the previous value.\n\n `compare_exchange_weak` takes two [`Ordering`] arguments to describe the memory\n ordering of this operation. `success` describes the required ordering for the\n read-modify-write operation that takes place if the comparison with `current` succeeds.\n `failure` describes the required ordering for the load operation that takes place when\n the comparison fails. Using [`Acquire`] as success ordering makes the store part\n of this operation [`Relaxed`], and using [`Release`] makes the successful load\n [`Relaxed`]. The failure ordering can only be [`SeqCst`], [`Acquire`] or [`Relaxed`].\n\n **Note**: This method is only available on platforms that support atomic operations on\n\n # Examples\n\n ```\n\n\n let mut old = val.load(Ordering::Relaxed);\n loop {\n     let new = old * 2;\n     match val.compare_exchange_weak(old, new, Ordering::SeqCst, Ordering::Relaxed) {\n         Ok(_) => break,\n         Err(x) => old = x,\n     }\n }\n ```\n\n # Considerations\n\n `compare_exchange` is a [compare-and-swap operation] and thus exhibits the usual downsides\n of CAS operations. In particular, a load of the value followed by a successful\n `compare_exchange` with the previous load *does not ensure* that other threads have not\n changed the value in the interim. This is usually important when the *equality* check in\n the `compare_exchange` is being used to check the *identity* of a value, but equality\n does not necessarily imply identity. This is a particularly common case for pointers, as\n a pointer holding the same address does not imply that the same object exists at that\n address! In this case, `compare_exchange` can lead to the [ABA problem].\n\n [ABA Problem]: https://en.wikipedia.org/wiki/ABA_problem\n [compare-and-swap operation]: https://en.wikipedia.org/wiki/Compare-and-swap\n",
      "adt": {}
    },
    "sync::mpmc::utils::Backoff::spin_light": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Backs off using lightweight spinning.\n\n This method should be used for retrying an operation because another thread made\n progress. i.e. on CAS failure.\n",
      "adt": {
        "sync::mpmc::utils::Backoff": "ImmutableAsArgument"
      }
    },
    "core::num::<impl usize>::wrapping_add": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Wrapping (modular) addition. Computes `self + rhs`,\n wrapping around at the boundary of the type.\n\n # Examples\n\n ```\n ```\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicPtr::<T>::store": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Stores a value into the pointer.\n\n `store` takes an [`Ordering`] argument which describes the memory ordering\n of this operation. Possible values are [`SeqCst`], [`Release`] and [`Relaxed`].\n\n # Panics\n\n Panics if `order` is [`Acquire`] or [`AcqRel`].\n\n # Examples\n\n ```\n use std::sync::atomic::{AtomicPtr, Ordering};\n\n let ptr = &mut 5;\n let some_ptr = AtomicPtr::new(ptr);\n\n let other_ptr = &mut 10;\n\n some_ptr.store(other_ptr, Ordering::Relaxed);\n ```\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicUsize::store": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Stores a value into the atomic integer.\n\n `store` takes an [`Ordering`] argument which describes the memory ordering of this operation.\n  Possible values are [`SeqCst`], [`Release`] and [`Relaxed`].\n\n # Panics\n\n Panics if `order` is [`Acquire`] or [`AcqRel`].\n\n # Examples\n\n ```\n\n\n some_var.store(10, Ordering::Relaxed);\n assert_eq!(some_var.load(Ordering::Relaxed), 10);\n ```\n",
      "adt": {}
    },
    "sync::mpmc::list::Block::<T>::wait_next": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Waits until the next pointer is set.\n",
      "adt": {
        "sync::mpmc::list::Block": "ImmutableAsArgument"
      }
    }
  },
  "adts": {
    "sync::mpmc::utils::Backoff": [
      "Plain",
      "Ref"
    ],
    "sync::mpmc::utils::CachePadded": [
      "Ref"
    ],
    "sync::mpmc::list::Channel": [
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(1)))",
      "Ref"
    ],
    "sync::mpmc::list::Position": [
      "Ref",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(1)))"
    ],
    "core::sync::atomic::AtomicUsize": [
      "Ref"
    ],
    "core::sync::atomic::Ordering": [
      "Plain"
    ],
    "core::sync::atomic::AtomicPtr": [
      "Ref"
    ],
    "sync::mpmc::select::Token": [
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(1)))",
      "MutRef"
    ],
    "core::result::Result": [
      "Plain"
    ],
    "sync::mpmc::list::Block": [
      "Ref"
    ]
  },
  "path": 2858,
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sync/mpmc/list.rs:284:5: 366:6",
  "src": "fn start_recv(&self, token: &mut Token) -> bool {\n        let backoff = Backoff::new();\n        let mut head = self.head.index.load(Ordering::Acquire);\n        let mut block = self.head.block.load(Ordering::Acquire);\n\n        loop {\n            // Calculate the offset of the index into the block.\n            let offset = (head >> SHIFT) % LAP;\n\n            // If we reached the end of the block, wait until the next one is installed.\n            if offset == BLOCK_CAP {\n                backoff.spin_heavy();\n                head = self.head.index.load(Ordering::Acquire);\n                block = self.head.block.load(Ordering::Acquire);\n                continue;\n            }\n\n            let mut new_head = head + (1 << SHIFT);\n\n            if new_head & MARK_BIT == 0 {\n                atomic::fence(Ordering::SeqCst);\n                let tail = self.tail.index.load(Ordering::Relaxed);\n\n                // If the tail equals the head, that means the channel is empty.\n                if head >> SHIFT == tail >> SHIFT {\n                    // If the channel is disconnected...\n                    if tail & MARK_BIT != 0 {\n                        // ...then receive an error.\n                        token.list.block = ptr::null();\n                        return true;\n                    } else {\n                        // Otherwise, the receive operation is not ready.\n                        return false;\n                    }\n                }\n\n                // If head and tail are not in the same block, set `MARK_BIT` in head.\n                if (head >> SHIFT) / LAP != (tail >> SHIFT) / LAP {\n                    new_head |= MARK_BIT;\n                }\n            }\n\n            // The block can be null here only if the first message is being sent into the channel.\n            // In that case, just wait until it gets initialized.\n            if block.is_null() {\n                backoff.spin_heavy();\n                head = self.head.index.load(Ordering::Acquire);\n                block = self.head.block.load(Ordering::Acquire);\n                continue;\n            }\n\n            // Try moving the head index forward.\n            match self.head.index.compare_exchange_weak(\n                head,\n                new_head,\n                Ordering::SeqCst,\n                Ordering::Acquire,\n            ) {\n                Ok(_) => unsafe {\n                    // If we've reached the end of the block, move to the next one.\n                    if offset + 1 == BLOCK_CAP {\n                        let next = (*block).wait_next();\n                        let mut next_index = (new_head & !MARK_BIT).wrapping_add(1 << SHIFT);\n                        if !(*next).next.load(Ordering::Relaxed).is_null() {\n                            next_index |= MARK_BIT;\n                        }\n\n                        self.head.block.store(next, Ordering::Release);\n                        self.head.index.store(next_index, Ordering::Release);\n                    }\n\n                    token.list.block = block as *const u8;\n                    token.list.offset = offset;\n                    return true;\n                },\n                Err(_) => {\n                    backoff.spin_light();\n                    head = self.head.index.load(Ordering::Acquire);\n                    block = self.head.block.load(Ordering::Acquire);\n                }\n            }\n        }\n    }",
  "mir": "fn sync::mpmc::list::Channel::<T>::start_recv(_1: &sync::mpmc::list::Channel<T>, _2: &mut sync::mpmc::select::Token) -> bool {\n    let mut _0: bool;\n    let  _3: sync::mpmc::utils::Backoff;\n    let mut _4: usize;\n    let mut _5: &core::sync::atomic::AtomicUsize;\n    let  _6: &sync::mpmc::list::Position<T>;\n    let mut _7: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _8: core::sync::atomic::Ordering;\n    let mut _9: *mut sync::mpmc::list::Block<T>;\n    let mut _10: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let  _11: &sync::mpmc::list::Position<T>;\n    let mut _12: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _13: core::sync::atomic::Ordering;\n    let  _14: usize;\n    let mut _15: usize;\n    let mut _16: usize;\n    let mut _17: bool;\n    let mut _18: bool;\n    let mut _19: bool;\n    let  _20: ();\n    let mut _21: &sync::mpmc::utils::Backoff;\n    let mut _22: usize;\n    let mut _23: &core::sync::atomic::AtomicUsize;\n    let  _24: &sync::mpmc::list::Position<T>;\n    let mut _25: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _26: core::sync::atomic::Ordering;\n    let mut _27: *mut sync::mpmc::list::Block<T>;\n    let mut _28: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let  _29: &sync::mpmc::list::Position<T>;\n    let mut _30: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _31: core::sync::atomic::Ordering;\n    let mut _32: usize;\n    let mut _33: usize;\n    let mut _34: usize;\n    let mut _35: bool;\n    let mut _36: (usize, bool);\n    let mut _37: usize;\n    let mut _38: usize;\n    let  _39: ();\n    let mut _40: core::sync::atomic::Ordering;\n    let  _41: usize;\n    let mut _42: &core::sync::atomic::AtomicUsize;\n    let  _43: &sync::mpmc::list::Position<T>;\n    let mut _44: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _45: core::sync::atomic::Ordering;\n    let mut _46: bool;\n    let mut _47: usize;\n    let mut _48: usize;\n    let mut _49: bool;\n    let mut _50: usize;\n    let mut _51: bool;\n    let mut _52: usize;\n    let mut _53: *const u8;\n    let mut _54: bool;\n    let mut _55: usize;\n    let mut _56: usize;\n    let mut _57: usize;\n    let mut _58: bool;\n    let mut _59: bool;\n    let mut _60: usize;\n    let mut _61: usize;\n    let mut _62: bool;\n    let mut _63: bool;\n    let mut _64: bool;\n    let mut _65: *mut sync::mpmc::list::Block<T>;\n    let  _66: ();\n    let mut _67: &sync::mpmc::utils::Backoff;\n    let mut _68: usize;\n    let mut _69: &core::sync::atomic::AtomicUsize;\n    let  _70: &sync::mpmc::list::Position<T>;\n    let mut _71: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _72: core::sync::atomic::Ordering;\n    let mut _73: *mut sync::mpmc::list::Block<T>;\n    let mut _74: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let  _75: &sync::mpmc::list::Position<T>;\n    let mut _76: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _77: core::sync::atomic::Ordering;\n    let mut _78: core::result::Result<usize, usize>;\n    let mut _79: &core::sync::atomic::AtomicUsize;\n    let  _80: &sync::mpmc::list::Position<T>;\n    let mut _81: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _82: usize;\n    let mut _83: usize;\n    let mut _84: core::sync::atomic::Ordering;\n    let mut _85: core::sync::atomic::Ordering;\n    let mut _86: isize;\n    let mut _87: bool;\n    let mut _88: usize;\n    let mut _89: (usize, bool);\n    let  _90: *mut sync::mpmc::list::Block<T>;\n    let mut _91: &sync::mpmc::list::Block<T>;\n    let mut _92: usize;\n    let mut _93: usize;\n    let mut _94: usize;\n    let mut _95: usize;\n    let mut _96: usize;\n    let mut _97: bool;\n    let mut _98: bool;\n    let mut _99: *mut sync::mpmc::list::Block<T>;\n    let mut _100: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let mut _101: core::sync::atomic::Ordering;\n    let  _102: ();\n    let mut _103: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let  _104: &sync::mpmc::list::Position<T>;\n    let mut _105: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _106: core::sync::atomic::Ordering;\n    let  _107: ();\n    let mut _108: &core::sync::atomic::AtomicUsize;\n    let  _109: &sync::mpmc::list::Position<T>;\n    let mut _110: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _111: usize;\n    let mut _112: core::sync::atomic::Ordering;\n    let mut _113: *mut sync::mpmc::list::Block<T>;\n    let  _114: ();\n    let mut _115: &sync::mpmc::utils::Backoff;\n    let mut _116: usize;\n    let mut _117: &core::sync::atomic::AtomicUsize;\n    let  _118: &sync::mpmc::list::Position<T>;\n    let mut _119: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _120: core::sync::atomic::Ordering;\n    let mut _121: *mut sync::mpmc::list::Block<T>;\n    let mut _122: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let  _123: &sync::mpmc::list::Position<T>;\n    let mut _124: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _125: core::sync::atomic::Ordering;\n    let mut _126: *const ();\n    let mut _127: usize;\n    let mut _128: usize;\n    let mut _129: usize;\n    let mut _130: bool;\n    let mut _131: *const ();\n    let mut _132: usize;\n    let mut _133: usize;\n    let mut _134: usize;\n    let mut _135: bool;\n    let mut _136: *const ();\n    let mut _137: usize;\n    let mut _138: bool;\n    let mut _139: bool;\n    let mut _140: bool;\n    let mut _141: *const ();\n    let mut _142: usize;\n    let mut _143: bool;\n    let mut _144: bool;\n    let mut _145: bool;\n    debug self => _1;\n    debug token => _2;\n    debug backoff => _3;\n    debug head => _4;\n    debug block => _9;\n    debug offset => _14;\n    debug new_head => _32;\n    debug tail => _41;\n    debug next => _90;\n    debug next_index => _92;\n    bb0: {\n        StorageLive(_3);\n        _3 = sync::mpmc::utils::Backoff::new() -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        StorageLive(_4);\n        StorageLive(_5);\n        StorageLive(_6);\n        StorageLive(_7);\n        _7 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _6 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _7) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        StorageDead(_7);\n        _5 = &((*_6).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_8);\n        _8 = core::sync::atomic::Ordering::Acquire;\n        _4 = core::sync::atomic::AtomicUsize::load(move _5, move _8) -> [return: bb3, unwind unreachable];\n    }\n    bb3: {\n        StorageDead(_8);\n        StorageDead(_5);\n        StorageDead(_6);\n        StorageLive(_9);\n        StorageLive(_10);\n        StorageLive(_11);\n        StorageLive(_12);\n        _12 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _11 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _12) -> [return: bb4, unwind unreachable];\n    }\n    bb4: {\n        StorageDead(_12);\n        _10 = &((*_11).1: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_13);\n        _13 = core::sync::atomic::Ordering::Acquire;\n        _9 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::load(move _10, move _13) -> [return: bb5, unwind unreachable];\n    }\n    bb5: {\n        StorageDead(_13);\n        StorageDead(_10);\n        StorageDead(_11);\n        goto -> bb6;\n    }\n    bb6: {\n        StorageLive(_15);\n        StorageLive(_16);\n        _16 = _4;\n        _17 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _17, \"attempt to shift right by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb7, unwind unreachable];\n    }\n    bb7: {\n        _15 = Shr(move _16, sync::mpmc::list::SHIFT);\n        StorageDead(_16);\n        _18 = Eq(sync::mpmc::list::LAP, 0_usize);\n        assert(!move _18, \"attempt to calculate the remainder of `{}` with a divisor of zero\", _15) -> [success: bb8, unwind unreachable];\n    }\n    bb8: {\n        _14 = Rem(move _15, sync::mpmc::list::LAP);\n        StorageDead(_15);\n        StorageLive(_19);\n        _19 = Eq(_14, sync::mpmc::list::BLOCK_CAP);\n        switchInt(move _19) -> [0: bb15, otherwise: bb9];\n    }\n    bb9: {\n        StorageLive(_21);\n        _21 = &_3;\n        _20 = sync::mpmc::utils::Backoff::spin_heavy(move _21) -> [return: bb10, unwind unreachable];\n    }\n    bb10: {\n        StorageDead(_21);\n        StorageLive(_22);\n        StorageLive(_23);\n        StorageLive(_24);\n        StorageLive(_25);\n        _25 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _24 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _25) -> [return: bb11, unwind unreachable];\n    }\n    bb11: {\n        StorageDead(_25);\n        _23 = &((*_24).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_26);\n        _26 = core::sync::atomic::Ordering::Acquire;\n        _22 = core::sync::atomic::AtomicUsize::load(move _23, move _26) -> [return: bb12, unwind unreachable];\n    }\n    bb12: {\n        StorageDead(_26);\n        StorageDead(_23);\n        _4 = move _22;\n        StorageDead(_22);\n        StorageDead(_24);\n        StorageLive(_27);\n        StorageLive(_28);\n        StorageLive(_29);\n        StorageLive(_30);\n        _30 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _29 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _30) -> [return: bb13, unwind unreachable];\n    }\n    bb13: {\n        StorageDead(_30);\n        _28 = &((*_29).1: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_31);\n        _31 = core::sync::atomic::Ordering::Acquire;\n        _27 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::load(move _28, move _31) -> [return: bb14, unwind unreachable];\n    }\n    bb14: {\n        StorageDead(_31);\n        StorageDead(_28);\n        _9 = move _27;\n        StorageDead(_27);\n        StorageDead(_29);\n        StorageDead(_19);\n        goto -> bb72;\n    }\n    bb15: {\n        StorageDead(_19);\n        StorageLive(_32);\n        StorageLive(_33);\n        _33 = _4;\n        StorageLive(_34);\n        _35 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _35, \"attempt to shift left by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb16, unwind unreachable];\n    }\n    bb16: {\n        _34 = Shl(1_usize, sync::mpmc::list::SHIFT);\n        _36 = CheckedAdd(_33, _34);\n        assert(!move (_36.1: bool), \"attempt to compute `{} + {}`, which would overflow\", move _33, move _34) -> [success: bb17, unwind unreachable];\n    }\n    bb17: {\n        _32 = move (_36.0: usize);\n        StorageDead(_34);\n        StorageDead(_33);\n        StorageLive(_37);\n        StorageLive(_38);\n        _38 = _32;\n        _37 = BitAnd(move _38, sync::mpmc::list::MARK_BIT);\n        StorageDead(_38);\n        switchInt(move _37) -> [0: bb18, otherwise: bb36];\n    }\n    bb18: {\n        StorageDead(_37);\n        StorageLive(_40);\n        _40 = core::sync::atomic::Ordering::SeqCst;\n        _39 = core::sync::atomic::fence(move _40) -> [return: bb19, unwind unreachable];\n    }\n    bb19: {\n        StorageDead(_40);\n        StorageLive(_42);\n        StorageLive(_43);\n        StorageLive(_44);\n        _44 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _43 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _44) -> [return: bb20, unwind unreachable];\n    }\n    bb20: {\n        StorageDead(_44);\n        _42 = &((*_43).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_45);\n        _45 = core::sync::atomic::Ordering::Relaxed;\n        _41 = core::sync::atomic::AtomicUsize::load(move _42, move _45) -> [return: bb21, unwind unreachable];\n    }\n    bb21: {\n        StorageDead(_45);\n        StorageDead(_42);\n        StorageDead(_43);\n        StorageLive(_46);\n        StorageLive(_47);\n        StorageLive(_48);\n        _48 = _4;\n        _49 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _49, \"attempt to shift right by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb22, unwind unreachable];\n    }\n    bb22: {\n        _47 = Shr(move _48, sync::mpmc::list::SHIFT);\n        StorageDead(_48);\n        StorageLive(_50);\n        _51 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _51, \"attempt to shift right by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb23, unwind unreachable];\n    }\n    bb23: {\n        _50 = Shr(_41, sync::mpmc::list::SHIFT);\n        _46 = Eq(move _47, move _50);\n        switchInt(move _46) -> [0: bb28, otherwise: bb24];\n    }\n    bb24: {\n        StorageDead(_50);\n        StorageDead(_47);\n        StorageLive(_52);\n        _52 = BitAnd(_41, sync::mpmc::list::MARK_BIT);\n        switchInt(move _52) -> [0: bb27, otherwise: bb25];\n    }\n    bb25: {\n        StorageDead(_52);\n        StorageLive(_53);\n        _53 = core::ptr::null::<u8>() -> [return: bb26, unwind unreachable];\n    }\n    bb26: {\n        (((*_2).1: sync::mpmc::list::ListToken).0: *const u8) = move _53;\n        StorageDead(_53);\n        _0 = true;\n        goto -> bb73;\n    }\n    bb27: {\n        StorageDead(_52);\n        _0 = false;\n        goto -> bb73;\n    }\n    bb28: {\n        StorageDead(_50);\n        StorageDead(_47);\n        StorageDead(_46);\n        StorageLive(_54);\n        StorageLive(_55);\n        StorageLive(_56);\n        StorageLive(_57);\n        _57 = _4;\n        _58 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _58, \"attempt to shift right by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb29, unwind unreachable];\n    }\n    bb29: {\n        _56 = Shr(move _57, sync::mpmc::list::SHIFT);\n        StorageDead(_57);\n        _59 = Eq(sync::mpmc::list::LAP, 0_usize);\n        assert(!move _59, \"attempt to divide `{}` by zero\", _56) -> [success: bb30, unwind unreachable];\n    }\n    bb30: {\n        _55 = Div(move _56, sync::mpmc::list::LAP);\n        StorageDead(_56);\n        StorageLive(_60);\n        StorageLive(_61);\n        _62 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _62, \"attempt to shift right by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb31, unwind unreachable];\n    }\n    bb31: {\n        _61 = Shr(_41, sync::mpmc::list::SHIFT);\n        _63 = Eq(sync::mpmc::list::LAP, 0_usize);\n        assert(!move _63, \"attempt to divide `{}` by zero\", _61) -> [success: bb32, unwind unreachable];\n    }\n    bb32: {\n        _60 = Div(move _61, sync::mpmc::list::LAP);\n        StorageDead(_61);\n        _54 = Ne(move _55, move _60);\n        switchInt(move _54) -> [0: bb34, otherwise: bb33];\n    }\n    bb33: {\n        StorageDead(_60);\n        StorageDead(_55);\n        _32 = BitOr(_32, sync::mpmc::list::MARK_BIT);\n        goto -> bb35;\n    }\n    bb34: {\n        StorageDead(_60);\n        StorageDead(_55);\n        goto -> bb35;\n    }\n    bb35: {\n        StorageDead(_54);\n        goto -> bb37;\n    }\n    bb36: {\n        StorageDead(_37);\n        goto -> bb37;\n    }\n    bb37: {\n        StorageLive(_64);\n        StorageLive(_65);\n        _65 = _9;\n        _64 = core::ptr::mut_ptr::<impl *mut sync::mpmc::list::Block<T>>::is_null(move _65) -> [return: bb38, unwind unreachable];\n    }\n    bb38: {\n        switchInt(move _64) -> [0: bb45, otherwise: bb39];\n    }\n    bb39: {\n        StorageDead(_65);\n        StorageLive(_67);\n        _67 = &_3;\n        _66 = sync::mpmc::utils::Backoff::spin_heavy(move _67) -> [return: bb40, unwind unreachable];\n    }\n    bb40: {\n        StorageDead(_67);\n        StorageLive(_68);\n        StorageLive(_69);\n        StorageLive(_70);\n        StorageLive(_71);\n        _71 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _70 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _71) -> [return: bb41, unwind unreachable];\n    }\n    bb41: {\n        StorageDead(_71);\n        _69 = &((*_70).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_72);\n        _72 = core::sync::atomic::Ordering::Acquire;\n        _68 = core::sync::atomic::AtomicUsize::load(move _69, move _72) -> [return: bb42, unwind unreachable];\n    }\n    bb42: {\n        StorageDead(_72);\n        StorageDead(_69);\n        _4 = move _68;\n        StorageDead(_68);\n        StorageDead(_70);\n        StorageLive(_73);\n        StorageLive(_74);\n        StorageLive(_75);\n        StorageLive(_76);\n        _76 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _75 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _76) -> [return: bb43, unwind unreachable];\n    }\n    bb43: {\n        StorageDead(_76);\n        _74 = &((*_75).1: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_77);\n        _77 = core::sync::atomic::Ordering::Acquire;\n        _73 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::load(move _74, move _77) -> [return: bb44, unwind unreachable];\n    }\n    bb44: {\n        StorageDead(_77);\n        StorageDead(_74);\n        _9 = move _73;\n        StorageDead(_73);\n        StorageDead(_75);\n        StorageDead(_64);\n        StorageDead(_32);\n        goto -> bb72;\n    }\n    bb45: {\n        StorageDead(_65);\n        StorageDead(_64);\n        StorageLive(_78);\n        StorageLive(_79);\n        StorageLive(_80);\n        StorageLive(_81);\n        _81 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _80 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _81) -> [return: bb46, unwind unreachable];\n    }\n    bb46: {\n        StorageDead(_81);\n        _79 = &((*_80).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_82);\n        _82 = _4;\n        StorageLive(_83);\n        _83 = _32;\n        StorageLive(_84);\n        _84 = core::sync::atomic::Ordering::SeqCst;\n        StorageLive(_85);\n        _85 = core::sync::atomic::Ordering::Acquire;\n        _78 = core::sync::atomic::AtomicUsize::compare_exchange_weak(move _79, move _82, move _83, move _84, move _85) -> [return: bb47, unwind unreachable];\n    }\n    bb47: {\n        StorageDead(_85);\n        StorageDead(_84);\n        StorageDead(_83);\n        StorageDead(_82);\n        StorageDead(_79);\n        _86 = discriminant(_78);\n        switchInt(move _86) -> [0: bb50, 1: bb49, otherwise: bb48];\n    }\n    bb48: {\n        unreachable;\n    }\n    bb49: {\n        StorageLive(_115);\n        _115 = &_3;\n        _114 = sync::mpmc::utils::Backoff::spin_light(move _115) -> [return: bb67, unwind unreachable];\n    }\n    bb50: {\n        StorageLive(_87);\n        StorageLive(_88);\n        _89 = CheckedAdd(_14, 1_usize);\n        assert(!move (_89.1: bool), \"attempt to compute `{} + {}`, which would overflow\", _14, 1_usize) -> [success: bb51, unwind unreachable];\n    }\n    bb51: {\n        _88 = move (_89.0: usize);\n        _87 = Eq(move _88, sync::mpmc::list::BLOCK_CAP);\n        switchInt(move _87) -> [0: bb65, otherwise: bb52];\n    }\n    bb52: {\n        StorageDead(_88);\n        StorageLive(_91);\n        _131 = _9 as *const ();\n        _132 = _131 as usize;\n        _133 = Sub(<sync::mpmc::list::Block<T> as core::mem::SizedTypeProperties>::ALIGN, 1_usize);\n        _134 = BitAnd(_132, _133);\n        _135 = Eq(_134, 0_usize);\n        assert(_135, \"misaligned pointer dereference: address must be a multiple of {} but is {}\",<sync::mpmc::list::Block<T> as core::mem::SizedTypeProperties>::ALIGN, _132) -> [success: bb76, unwind unreachable];\n    }\n    bb53: {\n        StorageDead(_91);\n        StorageLive(_92);\n        StorageLive(_93);\n        StorageLive(_94);\n        _94 = _32;\n        StorageLive(_95);\n        _95 = Not(sync::mpmc::list::MARK_BIT);\n        _93 = BitAnd(move _94, move _95);\n        StorageDead(_95);\n        StorageDead(_94);\n        StorageLive(_96);\n        _97 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _97, \"attempt to shift left by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb54, unwind unreachable];\n    }\n    bb54: {\n        _96 = Shl(1_usize, sync::mpmc::list::SHIFT);\n        _92 = core::num::<impl usize>::wrapping_add(move _93, move _96) -> [return: bb55, unwind unreachable];\n    }\n    bb55: {\n        StorageDead(_96);\n        StorageDead(_93);\n        StorageLive(_98);\n        StorageLive(_99);\n        StorageLive(_100);\n        _126 = _90 as *const ();\n        _127 = _126 as usize;\n        _128 = Sub(<core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>> as core::mem::SizedTypeProperties>::ALIGN, 1_usize);\n        _129 = BitAnd(_127, _128);\n        _130 = Eq(_129, 0_usize);\n        assert(_130, \"misaligned pointer dereference: address must be a multiple of {} but is {}\",<core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>> as core::mem::SizedTypeProperties>::ALIGN, _127) -> [success: bb75, unwind unreachable];\n    }\n    bb56: {\n        StorageDead(_101);\n        StorageDead(_100);\n        _98 = core::ptr::mut_ptr::<impl *mut sync::mpmc::list::Block<T>>::is_null(move _99) -> [return: bb57, unwind unreachable];\n    }\n    bb57: {\n        switchInt(move _98) -> [0: bb59, otherwise: bb58];\n    }\n    bb58: {\n        StorageDead(_99);\n        goto -> bb60;\n    }\n    bb59: {\n        StorageDead(_99);\n        _92 = BitOr(_92, sync::mpmc::list::MARK_BIT);\n        goto -> bb60;\n    }\n    bb60: {\n        StorageDead(_98);\n        StorageLive(_103);\n        StorageLive(_104);\n        StorageLive(_105);\n        _105 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _104 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _105) -> [return: bb61, unwind unreachable];\n    }\n    bb61: {\n        StorageDead(_105);\n        _103 = &((*_104).1: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_106);\n        _106 = core::sync::atomic::Ordering::Release;\n        _102 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::store(move _103, _90, move _106) -> [return: bb62, unwind unreachable];\n    }\n    bb62: {\n        StorageDead(_106);\n        StorageDead(_103);\n        StorageDead(_104);\n        StorageLive(_108);\n        StorageLive(_109);\n        StorageLive(_110);\n        _110 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _109 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _110) -> [return: bb63, unwind unreachable];\n    }\n    bb63: {\n        StorageDead(_110);\n        _108 = &((*_109).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_111);\n        _111 = _92;\n        StorageLive(_112);\n        _112 = core::sync::atomic::Ordering::Release;\n        _107 = core::sync::atomic::AtomicUsize::store(move _108, move _111, move _112) -> [return: bb64, unwind unreachable];\n    }\n    bb64: {\n        StorageDead(_112);\n        StorageDead(_111);\n        StorageDead(_108);\n        StorageDead(_109);\n        StorageDead(_92);\n        goto -> bb66;\n    }\n    bb65: {\n        StorageDead(_88);\n        goto -> bb66;\n    }\n    bb66: {\n        StorageDead(_87);\n        StorageLive(_113);\n        _113 = _9;\n        (((*_2).1: sync::mpmc::list::ListToken).0: *const u8) = move _113 as *const u8;\n        StorageDead(_113);\n        (((*_2).1: sync::mpmc::list::ListToken).1: usize) = _14;\n        _0 = true;\n        StorageDead(_80);\n        StorageDead(_78);\n        goto -> bb74;\n    }\n    bb67: {\n        StorageDead(_115);\n        StorageLive(_116);\n        StorageLive(_117);\n        StorageLive(_118);\n        StorageLive(_119);\n        _119 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _118 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _119) -> [return: bb68, unwind unreachable];\n    }\n    bb68: {\n        StorageDead(_119);\n        _117 = &((*_118).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_120);\n        _120 = core::sync::atomic::Ordering::Acquire;\n        _116 = core::sync::atomic::AtomicUsize::load(move _117, move _120) -> [return: bb69, unwind unreachable];\n    }\n    bb69: {\n        StorageDead(_120);\n        StorageDead(_117);\n        _4 = move _116;\n        StorageDead(_116);\n        StorageDead(_118);\n        StorageLive(_121);\n        StorageLive(_122);\n        StorageLive(_123);\n        StorageLive(_124);\n        _124 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _123 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _124) -> [return: bb70, unwind unreachable];\n    }\n    bb70: {\n        StorageDead(_124);\n        _122 = &((*_123).1: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_125);\n        _125 = core::sync::atomic::Ordering::Acquire;\n        _121 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::load(move _122, move _125) -> [return: bb71, unwind unreachable];\n    }\n    bb71: {\n        StorageDead(_125);\n        StorageDead(_122);\n        _9 = move _121;\n        StorageDead(_121);\n        StorageDead(_123);\n        StorageDead(_80);\n        StorageDead(_78);\n        StorageDead(_32);\n        goto -> bb6;\n    }\n    bb72: {\n        goto -> bb6;\n    }\n    bb73: {\n        StorageDead(_46);\n        goto -> bb74;\n    }\n    bb74: {\n        StorageDead(_32);\n        StorageDead(_9);\n        StorageDead(_4);\n        StorageDead(_3);\n        return;\n    }\n    bb75: {\n        _141 = _90 as *const ();\n        _142 = _141 as usize;\n        _143 = Eq(_142, 0_usize);\n        _144 = BitAnd(_143, true);\n        _145 = Not(_144);\n        assert(_145, \"null pointer dereference occurred\") -> [success: bb78, unwind unreachable];\n    }\n    bb76: {\n        _136 = _9 as *const ();\n        _137 = _136 as usize;\n        _138 = Eq(_137, 0_usize);\n        _139 = BitAnd(_138, true);\n        _140 = Not(_139);\n        assert(_140, \"null pointer dereference occurred\") -> [success: bb77, unwind unreachable];\n    }\n    bb77: {\n        _91 = &(*_9);\n        _90 = sync::mpmc::list::Block::<T>::wait_next(move _91) -> [return: bb53, unwind unreachable];\n    }\n    bb78: {\n        _100 = &((*_90).0: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_101);\n        _101 = core::sync::atomic::Ordering::Relaxed;\n        _99 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::load(move _100, move _101) -> [return: bb56, unwind unreachable];\n    }\n}\n",
  "doc": " Attempts to reserve a slot for receiving a message.\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}