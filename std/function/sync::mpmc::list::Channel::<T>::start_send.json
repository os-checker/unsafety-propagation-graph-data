{
  "name": "sync::mpmc::list::Channel::<T>::start_send",
  "safe": true,
  "callees": {
    "sync::mpmc::utils::Backoff::new": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a new `Backoff`.\n",
      "adt": {
        "sync::mpmc::utils::Backoff": "Constructor"
      }
    },
    "core::ops::Deref::deref": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Dereferences the value.\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicUsize::load": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Loads a value from the atomic integer.\n\n `load` takes an [`Ordering`] argument which describes the memory ordering of this operation.\n Possible values are [`SeqCst`], [`Acquire`] and [`Relaxed`].\n\n # Panics\n\n Panics if `order` is [`Release`] or [`AcqRel`].\n\n # Examples\n\n ```\n\n\n assert_eq!(some_var.load(Ordering::Relaxed), 5);\n ```\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicPtr::<T>::load": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Loads a value from the pointer.\n\n `load` takes an [`Ordering`] argument which describes the memory ordering\n of this operation. Possible values are [`SeqCst`], [`Acquire`] and [`Relaxed`].\n\n # Panics\n\n Panics if `order` is [`Release`] or [`AcqRel`].\n\n # Examples\n\n ```\n use std::sync::atomic::{AtomicPtr, Ordering};\n\n let ptr = &mut 5;\n let some_ptr = AtomicPtr::new(ptr);\n\n let value = some_ptr.load(Ordering::Relaxed);\n ```\n",
      "adt": {}
    },
    "core::ptr::null": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a null raw pointer.\n\n This function is equivalent to zero-initializing the pointer:\n `MaybeUninit::<*const T>::zeroed().assume_init()`.\n The resulting pointer has the address 0.\n\n # Examples\n\n ```\n use std::ptr;\n\n let p: *const i32 = ptr::null();\n assert!(p.is_null());\n assert_eq!(p as usize, 0); // this pointer has the address 0\n ```\n",
      "adt": {}
    },
    "sync::mpmc::utils::Backoff::spin_heavy": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Backs off using heavyweight spinning.\n\n This method should be used in blocking loops where parking the thread is not an option.\n",
      "adt": {
        "sync::mpmc::utils::Backoff": "ImmutableAsArgument"
      }
    },
    "core::option::Option::<T>::is_none": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns `true` if the option is a [`None`] value.\n\n # Examples\n\n ```\n let x: Option<u32> = Some(2);\n assert_eq!(x.is_none(), false);\n\n let x: Option<u32> = None;\n assert_eq!(x.is_none(), true);\n ```\n",
      "adt": {}
    },
    "sync::mpmc::list::Block::<T>::new": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates an empty block.\n",
      "adt": {
        "alloc_crate::boxed::Box": "Constructor"
      }
    },
    "core::ptr::mut_ptr::<impl *mut T>::is_null": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "\n # Examples\n\n ```\n let mut s = [1, 2, 3];\n let ptr: *mut u32 = s.as_mut_ptr();\n assert!(!ptr.is_null());\n ```\n",
      "adt": {}
    },
    "alloc_crate::boxed::Box::<T>::into_raw": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Consumes the `Box`, returning a wrapped raw pointer.\n\n The pointer will be properly aligned and non-null.\n\n After calling this function, the caller is responsible for the\n memory previously managed by the `Box`. In particular, the\n caller should properly destroy `T` and release the memory, taking\n into account the [memory layout] used by `Box`. The easiest way to\n do this is to convert the raw pointer back into a `Box` with the\n [`Box::from_raw`] function, allowing the `Box` destructor to perform\n the cleanup.\n\n Note: this is an associated function, which means that you have\n to call it as `Box::into_raw(b)` instead of `b.into_raw()`. This\n is so that there is no conflict with a method on the inner type.\n\n # Examples\n Converting the raw pointer back into a `Box` with [`Box::from_raw`]\n for automatic cleanup:\n ```\n let x = Box::new(String::from(\"Hello\"));\n let ptr = Box::into_raw(x);\n let x = unsafe { Box::from_raw(ptr) };\n ```\n Manual cleanup by explicitly running the destructor and deallocating\n the memory:\n ```\n use std::alloc::{dealloc, Layout};\n use std::ptr;\n\n let x = Box::new(String::from(\"Hello\"));\n let ptr = Box::into_raw(x);\n unsafe {\n     ptr::drop_in_place(ptr);\n     dealloc(ptr as *mut u8, Layout::new::<String>());\n }\n ```\n Note: This is equivalent to the following:\n ```\n let x = Box::new(String::from(\"Hello\"));\n let ptr = Box::into_raw(x);\n unsafe {\n     drop(Box::from_raw(ptr));\n }\n ```\n\n [memory layout]: self#memory-layout\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicPtr::<T>::compare_exchange": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Stores a value into the pointer if the current value is the same as the `current` value.\n\n The return value is a result indicating whether the new value was written and containing\n the previous value. On success this value is guaranteed to be equal to `current`.\n\n `compare_exchange` takes two [`Ordering`] arguments to describe the memory\n ordering of this operation. `success` describes the required ordering for the\n read-modify-write operation that takes place if the comparison with `current` succeeds.\n `failure` describes the required ordering for the load operation that takes place when\n the comparison fails. Using [`Acquire`] as success ordering makes the store part\n of this operation [`Relaxed`], and using [`Release`] makes the successful load\n [`Relaxed`]. The failure ordering can only be [`SeqCst`], [`Acquire`] or [`Relaxed`].\n\n **Note:** This method is only available on platforms that support atomic\n operations on pointers.\n\n # Examples\n\n ```\n use std::sync::atomic::{AtomicPtr, Ordering};\n\n let ptr = &mut 5;\n let some_ptr = AtomicPtr::new(ptr);\n\n let other_ptr = &mut 10;\n\n let value = some_ptr.compare_exchange(ptr, other_ptr,\n                                       Ordering::SeqCst, Ordering::Relaxed);\n ```\n\n # Considerations\n\n `compare_exchange` is a [compare-and-swap operation] and thus exhibits the usual downsides\n of CAS operations. In particular, a load of the value followed by a successful\n `compare_exchange` with the previous load *does not ensure* that other threads have not\n changed the value in the interim. This is usually important when the *equality* check in\n the `compare_exchange` is being used to check the *identity* of a value, but equality\n does not necessarily imply identity. This is a particularly common case for pointers, as\n a pointer holding the same address does not imply that the same object exists at that\n address! In this case, `compare_exchange` can lead to the [ABA problem].\n\n [ABA Problem]: https://en.wikipedia.org/wiki/ABA_problem\n [compare-and-swap operation]: https://en.wikipedia.org/wiki/Compare-and-swap\n",
      "adt": {}
    },
    "core::result::Result::<T, E>::is_ok": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns `true` if the result is [`Ok`].\n\n # Examples\n\n ```\n let x: Result<i32, &str> = Ok(-3);\n assert_eq!(x.is_ok(), true);\n\n let x: Result<i32, &str> = Err(\"Some error message\");\n assert_eq!(x.is_ok(), false);\n ```\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicPtr::<T>::store": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Stores a value into the pointer.\n\n `store` takes an [`Ordering`] argument which describes the memory ordering\n of this operation. Possible values are [`SeqCst`], [`Release`] and [`Relaxed`].\n\n # Panics\n\n Panics if `order` is [`Acquire`] or [`AcqRel`].\n\n # Examples\n\n ```\n use std::sync::atomic::{AtomicPtr, Ordering};\n\n let ptr = &mut 5;\n let some_ptr = AtomicPtr::new(ptr);\n\n let other_ptr = &mut 10;\n\n some_ptr.store(other_ptr, Ordering::Relaxed);\n ```\n",
      "adt": {}
    },
    "alloc_crate::boxed::Box::<T>::from_raw": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Constructs a box from a raw pointer.\n\n After calling this function, the raw pointer is owned by the\n resulting `Box`. Specifically, the `Box` destructor will call\n the destructor of `T` and free the allocated memory. For this\n to be safe, the memory must have been allocated in accordance\n with the [memory layout] used by `Box` .\n\n # Safety\n\n This function is unsafe because improper use may lead to\n memory problems. For example, a double-free may occur if the\n function is called twice on the same raw pointer.\n\n The raw pointer must point to a block of memory allocated by the global allocator.\n\n The safety conditions are described in the [memory layout] section.\n\n # Examples\n\n Recreate a `Box` which was previously converted to a raw pointer\n using [`Box::into_raw`]:\n ```\n let x = Box::new(5);\n let ptr = Box::into_raw(x);\n let x = unsafe { Box::from_raw(ptr) };\n ```\n Manually create a `Box` from scratch by using the global allocator:\n ```\n use std::alloc::{alloc, Layout};\n\n unsafe {\n     let ptr = alloc(Layout::new::<i32>()) as *mut i32;\n     // In general .write is required to avoid attempting to destruct\n     // the (uninitialized) previous contents of `ptr`, though for this\n     // simple example `*ptr = 5` would have worked as well.\n     ptr.write(5);\n     let x = Box::from_raw(ptr);\n }\n ```\n\n [memory layout]: self#memory-layout\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicUsize::compare_exchange_weak": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Stores a value into the atomic integer if the current value is the same as\n the `current` value.\n\n this function is allowed to spuriously fail even\n when the comparison succeeds, which can result in more efficient code on some\n platforms. The return value is a result indicating whether the new value was\n written and containing the previous value.\n\n `compare_exchange_weak` takes two [`Ordering`] arguments to describe the memory\n ordering of this operation. `success` describes the required ordering for the\n read-modify-write operation that takes place if the comparison with `current` succeeds.\n `failure` describes the required ordering for the load operation that takes place when\n the comparison fails. Using [`Acquire`] as success ordering makes the store part\n of this operation [`Relaxed`], and using [`Release`] makes the successful load\n [`Relaxed`]. The failure ordering can only be [`SeqCst`], [`Acquire`] or [`Relaxed`].\n\n **Note**: This method is only available on platforms that support atomic operations on\n\n # Examples\n\n ```\n\n\n let mut old = val.load(Ordering::Relaxed);\n loop {\n     let new = old * 2;\n     match val.compare_exchange_weak(old, new, Ordering::SeqCst, Ordering::Relaxed) {\n         Ok(_) => break,\n         Err(x) => old = x,\n     }\n }\n ```\n\n # Considerations\n\n `compare_exchange` is a [compare-and-swap operation] and thus exhibits the usual downsides\n of CAS operations. In particular, a load of the value followed by a successful\n `compare_exchange` with the previous load *does not ensure* that other threads have not\n changed the value in the interim. This is usually important when the *equality* check in\n the `compare_exchange` is being used to check the *identity* of a value, but equality\n does not necessarily imply identity. This is a particularly common case for pointers, as\n a pointer holding the same address does not imply that the same object exists at that\n address! In this case, `compare_exchange` can lead to the [ABA problem].\n\n [ABA Problem]: https://en.wikipedia.org/wiki/ABA_problem\n [compare-and-swap operation]: https://en.wikipedia.org/wiki/Compare-and-swap\n",
      "adt": {}
    },
    "sync::mpmc::utils::Backoff::spin_light": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Backs off using lightweight spinning.\n\n This method should be used for retrying an operation because another thread made\n progress. i.e. on CAS failure.\n",
      "adt": {
        "sync::mpmc::utils::Backoff": "ImmutableAsArgument"
      }
    },
    "core::option::Option::<T>::unwrap": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns the contained [`Some`] value, consuming the `self` value.\n\n Because this function may panic, its use is generally discouraged.\n Panics are meant for unrecoverable errors, and\n [may abort the entire program][panic-abort].\n\n Instead, prefer to use pattern matching and handle the [`None`]\n case explicitly, or call [`unwrap_or`], [`unwrap_or_else`], or\n [`unwrap_or_default`]. In functions returning `Option`, you can use\n [the `?` (try) operator][try-option].\n\n [panic-abort]: https://doc.rust-lang.org/book/ch09-01-unrecoverable-errors-with-panic.html\n [try-option]: https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html#where-the--operator-can-be-used\n [`unwrap_or`]: Option::unwrap_or\n [`unwrap_or_else`]: Option::unwrap_or_else\n [`unwrap_or_default`]: Option::unwrap_or_default\n\n # Panics\n\n Panics if the self value equals [`None`].\n\n # Examples\n\n ```\n let x = Some(\"air\");\n assert_eq!(x.unwrap(), \"air\");\n ```\n\n ```should_panic\n let x: Option<&str> = None;\n assert_eq!(x.unwrap(), \"air\"); // fails\n ```\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicUsize::fetch_add": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Adds to the current value, returning the previous value.\n\n This operation wraps around on overflow.\n\n `fetch_add` takes an [`Ordering`] argument which describes the memory ordering\n of this operation. All ordering modes are possible. Note that using\n [`Acquire`] makes the store part of this operation [`Relaxed`], and\n using [`Release`] makes the load part [`Relaxed`].\n\n **Note**: This method is only available on platforms that support atomic operations on\n\n # Examples\n\n ```\n\n assert_eq!(foo.fetch_add(10, Ordering::SeqCst), 0);\n assert_eq!(foo.load(Ordering::SeqCst), 10);\n ```\n",
      "adt": {}
    }
  },
  "adts": {
    "sync::mpmc::utils::Backoff": [
      "Plain",
      "Ref"
    ],
    "sync::mpmc::utils::CachePadded": [
      "Ref"
    ],
    "sync::mpmc::list::Channel": [
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(1)))",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))",
      "Ref"
    ],
    "sync::mpmc::list::Position": [
      "Ref",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(1)))"
    ],
    "core::sync::atomic::AtomicUsize": [
      "Ref"
    ],
    "core::sync::atomic::Ordering": [
      "Plain"
    ],
    "core::sync::atomic::AtomicPtr": [
      "Ref"
    ],
    "core::option::Option": [
      "Plain",
      "Ref"
    ],
    "sync::mpmc::select::Token": [
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(1)))",
      "MutRef"
    ],
    "alloc_crate::boxed::Box": [
      "Plain"
    ],
    "core::result::Result": [
      "Plain",
      "Ref"
    ]
  },
  "path": 2859,
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sync/mpmc/list.rs:175:5: 260:6",
  "src": "fn start_send(&self, token: &mut Token) -> bool {\n        let backoff = Backoff::new();\n        let mut tail = self.tail.index.load(Ordering::Acquire);\n        let mut block = self.tail.block.load(Ordering::Acquire);\n        let mut next_block = None;\n\n        loop {\n            // Check if the channel is disconnected.\n            if tail & MARK_BIT != 0 {\n                token.list.block = ptr::null();\n                return true;\n            }\n\n            // Calculate the offset of the index into the block.\n            let offset = (tail >> SHIFT) % LAP;\n\n            // If we reached the end of the block, wait until the next one is installed.\n            if offset == BLOCK_CAP {\n                backoff.spin_heavy();\n                tail = self.tail.index.load(Ordering::Acquire);\n                block = self.tail.block.load(Ordering::Acquire);\n                continue;\n            }\n\n            // If we're going to have to install the next block, allocate it in advance in order to\n            // make the wait for other threads as short as possible.\n            if offset + 1 == BLOCK_CAP && next_block.is_none() {\n                next_block = Some(Block::<T>::new());\n            }\n\n            // If this is the first message to be sent into the channel, we need to allocate the\n            // first block and install it.\n            if block.is_null() {\n                let new = Box::into_raw(Block::<T>::new());\n\n                if self\n                    .tail\n                    .block\n                    .compare_exchange(block, new, Ordering::Release, Ordering::Relaxed)\n                    .is_ok()\n                {\n                    // This yield point leaves the channel in a half-initialized state where the\n                    // tail.block pointer is set but the head.block is not. This is used to\n                    // facilitate the test in src/tools/miri/tests/pass/issues/issue-139553.rs\n                    #[cfg(miri)]\n                    crate::thread::yield_now();\n                    self.head.block.store(new, Ordering::Release);\n                    block = new;\n                } else {\n                    next_block = unsafe { Some(Box::from_raw(new)) };\n                    tail = self.tail.index.load(Ordering::Acquire);\n                    block = self.tail.block.load(Ordering::Acquire);\n                    continue;\n                }\n            }\n\n            let new_tail = tail + (1 << SHIFT);\n\n            // Try advancing the tail forward.\n            match self.tail.index.compare_exchange_weak(\n                tail,\n                new_tail,\n                Ordering::SeqCst,\n                Ordering::Acquire,\n            ) {\n                Ok(_) => unsafe {\n                    // If we've reached the end of the block, install the next one.\n                    if offset + 1 == BLOCK_CAP {\n                        let next_block = Box::into_raw(next_block.unwrap());\n                        self.tail.block.store(next_block, Ordering::Release);\n                        self.tail.index.fetch_add(1 << SHIFT, Ordering::Release);\n                        (*block).next.store(next_block, Ordering::Release);\n                    }\n\n                    token.list.block = block as *const u8;\n                    token.list.offset = offset;\n                    return true;\n                },\n                Err(_) => {\n                    backoff.spin_light();\n                    tail = self.tail.index.load(Ordering::Acquire);\n                    block = self.tail.block.load(Ordering::Acquire);\n                }\n            }\n        }\n    }",
  "mir": "fn sync::mpmc::list::Channel::<T>::start_send(_1: &sync::mpmc::list::Channel<T>, _2: &mut sync::mpmc::select::Token) -> bool {\n    let mut _0: bool;\n    let  _3: sync::mpmc::utils::Backoff;\n    let mut _4: usize;\n    let mut _5: &core::sync::atomic::AtomicUsize;\n    let  _6: &sync::mpmc::list::Position<T>;\n    let mut _7: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _8: core::sync::atomic::Ordering;\n    let mut _9: *mut sync::mpmc::list::Block<T>;\n    let mut _10: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let  _11: &sync::mpmc::list::Position<T>;\n    let mut _12: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _13: core::sync::atomic::Ordering;\n    let mut _14: core::option::Option<alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>>;\n    let mut _15: usize;\n    let mut _16: usize;\n    let mut _17: *const u8;\n    let  _18: usize;\n    let mut _19: usize;\n    let mut _20: usize;\n    let mut _21: bool;\n    let mut _22: bool;\n    let mut _23: bool;\n    let  _24: ();\n    let mut _25: &sync::mpmc::utils::Backoff;\n    let mut _26: usize;\n    let mut _27: &core::sync::atomic::AtomicUsize;\n    let  _28: &sync::mpmc::list::Position<T>;\n    let mut _29: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _30: core::sync::atomic::Ordering;\n    let mut _31: *mut sync::mpmc::list::Block<T>;\n    let mut _32: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let  _33: &sync::mpmc::list::Position<T>;\n    let mut _34: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _35: core::sync::atomic::Ordering;\n    let mut _36: bool;\n    let mut _37: usize;\n    let mut _38: (usize, bool);\n    let mut _39: bool;\n    let mut _40: &core::option::Option<alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>>;\n    let mut _41: core::option::Option<alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>>;\n    let mut _42: alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>;\n    let mut _43: bool;\n    let mut _44: *mut sync::mpmc::list::Block<T>;\n    let  _45: *mut sync::mpmc::list::Block<T>;\n    let mut _46: alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>;\n    let mut _47: bool;\n    let mut _48: &core::result::Result<*mut sync::mpmc::list::Block<T>, *mut sync::mpmc::list::Block<T>>;\n    let  _49: core::result::Result<*mut sync::mpmc::list::Block<T>, *mut sync::mpmc::list::Block<T>>;\n    let mut _50: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let  _51: &sync::mpmc::list::Position<T>;\n    let mut _52: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _53: *mut sync::mpmc::list::Block<T>;\n    let mut _54: core::sync::atomic::Ordering;\n    let mut _55: core::sync::atomic::Ordering;\n    let  _56: ();\n    let mut _57: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let  _58: &sync::mpmc::list::Position<T>;\n    let mut _59: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _60: core::sync::atomic::Ordering;\n    let mut _61: core::option::Option<alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>>;\n    let mut _62: alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>;\n    let mut _63: usize;\n    let mut _64: &core::sync::atomic::AtomicUsize;\n    let  _65: &sync::mpmc::list::Position<T>;\n    let mut _66: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _67: core::sync::atomic::Ordering;\n    let mut _68: *mut sync::mpmc::list::Block<T>;\n    let mut _69: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let  _70: &sync::mpmc::list::Position<T>;\n    let mut _71: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _72: core::sync::atomic::Ordering;\n    let  _73: usize;\n    let mut _74: usize;\n    let mut _75: usize;\n    let mut _76: bool;\n    let mut _77: (usize, bool);\n    let mut _78: core::result::Result<usize, usize>;\n    let mut _79: &core::sync::atomic::AtomicUsize;\n    let  _80: &sync::mpmc::list::Position<T>;\n    let mut _81: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _82: usize;\n    let mut _83: core::sync::atomic::Ordering;\n    let mut _84: core::sync::atomic::Ordering;\n    let mut _85: isize;\n    let mut _86: bool;\n    let mut _87: usize;\n    let mut _88: (usize, bool);\n    let  _89: *mut sync::mpmc::list::Block<T>;\n    let mut _90: alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>;\n    let mut _91: core::option::Option<alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>>;\n    let  _92: ();\n    let mut _93: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let  _94: &sync::mpmc::list::Position<T>;\n    let mut _95: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _96: core::sync::atomic::Ordering;\n    let  _97: usize;\n    let mut _98: &core::sync::atomic::AtomicUsize;\n    let  _99: &sync::mpmc::list::Position<T>;\n    let mut _100: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _101: usize;\n    let mut _102: bool;\n    let mut _103: core::sync::atomic::Ordering;\n    let  _104: ();\n    let mut _105: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let mut _106: core::sync::atomic::Ordering;\n    let mut _107: *mut sync::mpmc::list::Block<T>;\n    let  _108: ();\n    let mut _109: &sync::mpmc::utils::Backoff;\n    let mut _110: usize;\n    let mut _111: &core::sync::atomic::AtomicUsize;\n    let  _112: &sync::mpmc::list::Position<T>;\n    let mut _113: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _114: core::sync::atomic::Ordering;\n    let mut _115: *mut sync::mpmc::list::Block<T>;\n    let mut _116: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let  _117: &sync::mpmc::list::Position<T>;\n    let mut _118: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _119: core::sync::atomic::Ordering;\n    let mut _120: bool;\n    let mut _121: *const ();\n    let mut _122: usize;\n    let mut _123: usize;\n    let mut _124: usize;\n    let mut _125: bool;\n    let mut _126: *const ();\n    let mut _127: usize;\n    let mut _128: bool;\n    let mut _129: bool;\n    let mut _130: bool;\n    debug self => _1;\n    debug token => _2;\n    debug backoff => _3;\n    debug tail => _4;\n    debug block => _9;\n    debug next_block => _14;\n    debug offset => _18;\n    debug new => _45;\n    debug new_tail => _73;\n    debug next_block => _89;\n    bb0: {\n        _120 = false;\n        StorageLive(_3);\n        _3 = sync::mpmc::utils::Backoff::new() -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        StorageLive(_4);\n        StorageLive(_5);\n        StorageLive(_6);\n        StorageLive(_7);\n        _7 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _6 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _7) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        StorageDead(_7);\n        _5 = &((*_6).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_8);\n        _8 = core::sync::atomic::Ordering::Acquire;\n        _4 = core::sync::atomic::AtomicUsize::load(move _5, move _8) -> [return: bb3, unwind unreachable];\n    }\n    bb3: {\n        StorageDead(_8);\n        StorageDead(_5);\n        StorageDead(_6);\n        StorageLive(_9);\n        StorageLive(_10);\n        StorageLive(_11);\n        StorageLive(_12);\n        _12 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _11 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _12) -> [return: bb4, unwind unreachable];\n    }\n    bb4: {\n        StorageDead(_12);\n        _10 = &((*_11).1: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_13);\n        _13 = core::sync::atomic::Ordering::Acquire;\n        _9 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::load(move _10, move _13) -> [return: bb5, unwind unreachable];\n    }\n    bb5: {\n        StorageDead(_13);\n        StorageDead(_10);\n        StorageDead(_11);\n        StorageLive(_14);\n        _120 = true;\n        _14 = core::option::Option::None;\n        goto -> bb6;\n    }\n    bb6: {\n        StorageLive(_15);\n        StorageLive(_16);\n        _16 = _4;\n        _15 = BitAnd(move _16, sync::mpmc::list::MARK_BIT);\n        StorageDead(_16);\n        switchInt(move _15) -> [0: bb9, otherwise: bb7];\n    }\n    bb7: {\n        StorageDead(_15);\n        StorageLive(_17);\n        _17 = core::ptr::null::<u8>() -> [return: bb8, unwind unreachable];\n    }\n    bb8: {\n        (((*_2).1: sync::mpmc::list::ListToken).0: *const u8) = move _17;\n        StorageDead(_17);\n        _0 = true;\n        goto -> bb72;\n    }\n    bb9: {\n        StorageDead(_15);\n        StorageLive(_19);\n        StorageLive(_20);\n        _20 = _4;\n        _21 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _21, \"attempt to shift right by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb10, unwind unreachable];\n    }\n    bb10: {\n        _19 = Shr(move _20, sync::mpmc::list::SHIFT);\n        StorageDead(_20);\n        _22 = Eq(sync::mpmc::list::LAP, 0_usize);\n        assert(!move _22, \"attempt to calculate the remainder of `{}` with a divisor of zero\", _19) -> [success: bb11, unwind unreachable];\n    }\n    bb11: {\n        _18 = Rem(move _19, sync::mpmc::list::LAP);\n        StorageDead(_19);\n        StorageLive(_23);\n        _23 = Eq(_18, sync::mpmc::list::BLOCK_CAP);\n        switchInt(move _23) -> [0: bb18, otherwise: bb12];\n    }\n    bb12: {\n        StorageLive(_25);\n        _25 = &_3;\n        _24 = sync::mpmc::utils::Backoff::spin_heavy(move _25) -> [return: bb13, unwind unreachable];\n    }\n    bb13: {\n        StorageDead(_25);\n        StorageLive(_26);\n        StorageLive(_27);\n        StorageLive(_28);\n        StorageLive(_29);\n        _29 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _28 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _29) -> [return: bb14, unwind unreachable];\n    }\n    bb14: {\n        StorageDead(_29);\n        _27 = &((*_28).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_30);\n        _30 = core::sync::atomic::Ordering::Acquire;\n        _26 = core::sync::atomic::AtomicUsize::load(move _27, move _30) -> [return: bb15, unwind unreachable];\n    }\n    bb15: {\n        StorageDead(_30);\n        StorageDead(_27);\n        _4 = move _26;\n        StorageDead(_26);\n        StorageDead(_28);\n        StorageLive(_31);\n        StorageLive(_32);\n        StorageLive(_33);\n        StorageLive(_34);\n        _34 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _33 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _34) -> [return: bb16, unwind unreachable];\n    }\n    bb16: {\n        StorageDead(_34);\n        _32 = &((*_33).1: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_35);\n        _35 = core::sync::atomic::Ordering::Acquire;\n        _31 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::load(move _32, move _35) -> [return: bb17, unwind unreachable];\n    }\n    bb17: {\n        StorageDead(_35);\n        StorageDead(_32);\n        _9 = move _31;\n        StorageDead(_31);\n        StorageDead(_33);\n        StorageDead(_23);\n        goto -> bb71;\n    }\n    bb18: {\n        StorageDead(_23);\n        StorageLive(_36);\n        StorageLive(_37);\n        _38 = CheckedAdd(_18, 1_usize);\n        assert(!move (_38.1: bool), \"attempt to compute `{} + {}`, which would overflow\", _18, 1_usize) -> [success: bb19, unwind unreachable];\n    }\n    bb19: {\n        _37 = move (_38.0: usize);\n        _36 = Eq(move _37, sync::mpmc::list::BLOCK_CAP);\n        switchInt(move _36) -> [0: bb26, otherwise: bb20];\n    }\n    bb20: {\n        StorageDead(_37);\n        StorageLive(_39);\n        StorageLive(_40);\n        _40 = &_14;\n        _39 = core::option::Option::<alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>>::is_none(move _40) -> [return: bb21, unwind unreachable];\n    }\n    bb21: {\n        switchInt(move _39) -> [0: bb25, otherwise: bb22];\n    }\n    bb22: {\n        StorageDead(_40);\n        StorageLive(_41);\n        StorageLive(_42);\n        _42 = sync::mpmc::list::Block::<T>::new() -> [return: bb23, unwind unreachable];\n    }\n    bb23: {\n        _41 = core::option::Option::Some(move _42);\n        StorageDead(_42);\n        drop(_14) -> [return: bb24, unwind unreachable];\n    }\n    bb24: {\n        _120 = true;\n        _14 = move _41;\n        StorageDead(_41);\n        goto -> bb27;\n    }\n    bb25: {\n        StorageDead(_40);\n        goto -> bb27;\n    }\n    bb26: {\n        StorageDead(_37);\n        goto -> bb27;\n    }\n    bb27: {\n        StorageDead(_39);\n        StorageDead(_36);\n        StorageLive(_43);\n        StorageLive(_44);\n        _44 = _9;\n        _43 = core::ptr::mut_ptr::<impl *mut sync::mpmc::list::Block<T>>::is_null(move _44) -> [return: bb28, unwind unreachable];\n    }\n    bb28: {\n        switchInt(move _43) -> [0: bb45, otherwise: bb29];\n    }\n    bb29: {\n        StorageDead(_44);\n        StorageLive(_46);\n        _46 = sync::mpmc::list::Block::<T>::new() -> [return: bb30, unwind unreachable];\n    }\n    bb30: {\n        _45 = alloc_crate::boxed::Box::<sync::mpmc::list::Block<T>>::into_raw(move _46) -> [return: bb31, unwind unreachable];\n    }\n    bb31: {\n        StorageDead(_46);\n        StorageLive(_47);\n        StorageLive(_48);\n        StorageLive(_49);\n        StorageLive(_50);\n        StorageLive(_51);\n        StorageLive(_52);\n        _52 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _51 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _52) -> [return: bb32, unwind unreachable];\n    }\n    bb32: {\n        StorageDead(_52);\n        _50 = &((*_51).1: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_53);\n        _53 = _9;\n        StorageLive(_54);\n        _54 = core::sync::atomic::Ordering::Release;\n        StorageLive(_55);\n        _55 = core::sync::atomic::Ordering::Relaxed;\n        _49 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::compare_exchange(move _50, move _53, _45, move _54, move _55) -> [return: bb33, unwind unreachable];\n    }\n    bb33: {\n        _48 = &_49;\n        StorageDead(_55);\n        StorageDead(_54);\n        StorageDead(_53);\n        StorageDead(_50);\n        _47 = core::result::Result::<*mut sync::mpmc::list::Block<T>, *mut sync::mpmc::list::Block<T>>::is_ok(move _48) -> [return: bb34, unwind unreachable];\n    }\n    bb34: {\n        switchInt(move _47) -> [0: bb38, otherwise: bb35];\n    }\n    bb35: {\n        StorageDead(_51);\n        StorageDead(_49);\n        StorageDead(_48);\n        StorageLive(_57);\n        StorageLive(_58);\n        StorageLive(_59);\n        _59 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _58 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _59) -> [return: bb36, unwind unreachable];\n    }\n    bb36: {\n        StorageDead(_59);\n        _57 = &((*_58).1: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_60);\n        _60 = core::sync::atomic::Ordering::Release;\n        _56 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::store(move _57, _45, move _60) -> [return: bb37, unwind unreachable];\n    }\n    bb37: {\n        StorageDead(_60);\n        StorageDead(_57);\n        StorageDead(_58);\n        _9 = _45;\n        StorageDead(_47);\n        goto -> bb46;\n    }\n    bb38: {\n        StorageDead(_51);\n        StorageDead(_49);\n        StorageDead(_48);\n        StorageLive(_61);\n        StorageLive(_62);\n        _62 = alloc_crate::boxed::Box::<sync::mpmc::list::Block<T>>::from_raw(_45) -> [return: bb39, unwind unreachable];\n    }\n    bb39: {\n        _61 = core::option::Option::Some(move _62);\n        StorageDead(_62);\n        drop(_14) -> [return: bb40, unwind unreachable];\n    }\n    bb40: {\n        _120 = true;\n        _14 = move _61;\n        StorageDead(_61);\n        StorageLive(_63);\n        StorageLive(_64);\n        StorageLive(_65);\n        StorageLive(_66);\n        _66 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _65 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _66) -> [return: bb41, unwind unreachable];\n    }\n    bb41: {\n        StorageDead(_66);\n        _64 = &((*_65).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_67);\n        _67 = core::sync::atomic::Ordering::Acquire;\n        _63 = core::sync::atomic::AtomicUsize::load(move _64, move _67) -> [return: bb42, unwind unreachable];\n    }\n    bb42: {\n        StorageDead(_67);\n        StorageDead(_64);\n        _4 = move _63;\n        StorageDead(_63);\n        StorageDead(_65);\n        StorageLive(_68);\n        StorageLive(_69);\n        StorageLive(_70);\n        StorageLive(_71);\n        _71 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _70 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _71) -> [return: bb43, unwind unreachable];\n    }\n    bb43: {\n        StorageDead(_71);\n        _69 = &((*_70).1: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_72);\n        _72 = core::sync::atomic::Ordering::Acquire;\n        _68 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::load(move _69, move _72) -> [return: bb44, unwind unreachable];\n    }\n    bb44: {\n        StorageDead(_72);\n        StorageDead(_69);\n        _9 = move _68;\n        StorageDead(_68);\n        StorageDead(_70);\n        StorageDead(_47);\n        StorageDead(_43);\n        goto -> bb71;\n    }\n    bb45: {\n        StorageDead(_44);\n        goto -> bb46;\n    }\n    bb46: {\n        StorageDead(_43);\n        StorageLive(_74);\n        _74 = _4;\n        StorageLive(_75);\n        _76 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _76, \"attempt to shift left by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb47, unwind unreachable];\n    }\n    bb47: {\n        _75 = Shl(1_usize, sync::mpmc::list::SHIFT);\n        _77 = CheckedAdd(_74, _75);\n        assert(!move (_77.1: bool), \"attempt to compute `{} + {}`, which would overflow\", move _74, move _75) -> [success: bb48, unwind unreachable];\n    }\n    bb48: {\n        _73 = move (_77.0: usize);\n        StorageDead(_75);\n        StorageDead(_74);\n        StorageLive(_78);\n        StorageLive(_79);\n        StorageLive(_80);\n        StorageLive(_81);\n        _81 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _80 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _81) -> [return: bb49, unwind unreachable];\n    }\n    bb49: {\n        StorageDead(_81);\n        _79 = &((*_80).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_82);\n        _82 = _4;\n        StorageLive(_83);\n        _83 = core::sync::atomic::Ordering::SeqCst;\n        StorageLive(_84);\n        _84 = core::sync::atomic::Ordering::Acquire;\n        _78 = core::sync::atomic::AtomicUsize::compare_exchange_weak(move _79, move _82, _73, move _83, move _84) -> [return: bb50, unwind unreachable];\n    }\n    bb50: {\n        StorageDead(_84);\n        StorageDead(_83);\n        StorageDead(_82);\n        StorageDead(_79);\n        _85 = discriminant(_78);\n        switchInt(move _85) -> [0: bb53, 1: bb52, otherwise: bb51];\n    }\n    bb51: {\n        unreachable;\n    }\n    bb52: {\n        StorageLive(_109);\n        _109 = &_3;\n        _108 = sync::mpmc::utils::Backoff::spin_light(move _109) -> [return: bb66, unwind unreachable];\n    }\n    bb53: {\n        StorageLive(_86);\n        StorageLive(_87);\n        _88 = CheckedAdd(_18, 1_usize);\n        assert(!move (_88.1: bool), \"attempt to compute `{} + {}`, which would overflow\", _18, 1_usize) -> [success: bb54, unwind unreachable];\n    }\n    bb54: {\n        _87 = move (_88.0: usize);\n        _86 = Eq(move _87, sync::mpmc::list::BLOCK_CAP);\n        switchInt(move _86) -> [0: bb64, otherwise: bb55];\n    }\n    bb55: {\n        StorageDead(_87);\n        StorageLive(_90);\n        StorageLive(_91);\n        _120 = false;\n        _91 = move _14;\n        _90 = core::option::Option::<alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>>::unwrap(move _91) -> [return: bb56, unwind unreachable];\n    }\n    bb56: {\n        StorageDead(_91);\n        _89 = alloc_crate::boxed::Box::<sync::mpmc::list::Block<T>>::into_raw(move _90) -> [return: bb57, unwind unreachable];\n    }\n    bb57: {\n        StorageDead(_90);\n        StorageLive(_93);\n        StorageLive(_94);\n        StorageLive(_95);\n        _95 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _94 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _95) -> [return: bb58, unwind unreachable];\n    }\n    bb58: {\n        StorageDead(_95);\n        _93 = &((*_94).1: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_96);\n        _96 = core::sync::atomic::Ordering::Release;\n        _92 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::store(move _93, _89, move _96) -> [return: bb59, unwind unreachable];\n    }\n    bb59: {\n        StorageDead(_96);\n        StorageDead(_93);\n        StorageDead(_94);\n        StorageLive(_97);\n        StorageLive(_98);\n        StorageLive(_99);\n        StorageLive(_100);\n        _100 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _99 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _100) -> [return: bb60, unwind unreachable];\n    }\n    bb60: {\n        StorageDead(_100);\n        _98 = &((*_99).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_101);\n        _102 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _102, \"attempt to shift left by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb61, unwind unreachable];\n    }\n    bb61: {\n        _101 = Shl(1_usize, sync::mpmc::list::SHIFT);\n        StorageLive(_103);\n        _103 = core::sync::atomic::Ordering::Release;\n        _97 = core::sync::atomic::AtomicUsize::fetch_add(move _98, move _101, move _103) -> [return: bb62, unwind unreachable];\n    }\n    bb62: {\n        StorageDead(_103);\n        StorageDead(_101);\n        StorageDead(_98);\n        StorageDead(_99);\n        StorageDead(_97);\n        StorageLive(_105);\n        _121 = _9 as *const ();\n        _122 = _121 as usize;\n        _123 = Sub(<core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>> as core::mem::SizedTypeProperties>::ALIGN, 1_usize);\n        _124 = BitAnd(_122, _123);\n        _125 = Eq(_124, 0_usize);\n        assert(_125, \"misaligned pointer dereference: address must be a multiple of {} but is {}\",<core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>> as core::mem::SizedTypeProperties>::ALIGN, _122) -> [success: bb75, unwind unreachable];\n    }\n    bb63: {\n        StorageDead(_106);\n        StorageDead(_105);\n        goto -> bb65;\n    }\n    bb64: {\n        StorageDead(_87);\n        goto -> bb65;\n    }\n    bb65: {\n        StorageDead(_86);\n        StorageLive(_107);\n        _107 = _9;\n        (((*_2).1: sync::mpmc::list::ListToken).0: *const u8) = move _107 as *const u8;\n        StorageDead(_107);\n        (((*_2).1: sync::mpmc::list::ListToken).1: usize) = _18;\n        _0 = true;\n        StorageDead(_80);\n        StorageDead(_78);\n        goto -> bb72;\n    }\n    bb66: {\n        StorageDead(_109);\n        StorageLive(_110);\n        StorageLive(_111);\n        StorageLive(_112);\n        StorageLive(_113);\n        _113 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _112 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _113) -> [return: bb67, unwind unreachable];\n    }\n    bb67: {\n        StorageDead(_113);\n        _111 = &((*_112).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_114);\n        _114 = core::sync::atomic::Ordering::Acquire;\n        _110 = core::sync::atomic::AtomicUsize::load(move _111, move _114) -> [return: bb68, unwind unreachable];\n    }\n    bb68: {\n        StorageDead(_114);\n        StorageDead(_111);\n        _4 = move _110;\n        StorageDead(_110);\n        StorageDead(_112);\n        StorageLive(_115);\n        StorageLive(_116);\n        StorageLive(_117);\n        StorageLive(_118);\n        _118 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _117 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _118) -> [return: bb69, unwind unreachable];\n    }\n    bb69: {\n        StorageDead(_118);\n        _116 = &((*_117).1: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_119);\n        _119 = core::sync::atomic::Ordering::Acquire;\n        _115 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::load(move _116, move _119) -> [return: bb70, unwind unreachable];\n    }\n    bb70: {\n        StorageDead(_119);\n        StorageDead(_116);\n        _9 = move _115;\n        StorageDead(_115);\n        StorageDead(_117);\n        StorageDead(_80);\n        StorageDead(_78);\n        goto -> bb6;\n    }\n    bb71: {\n        goto -> bb6;\n    }\n    bb72: {\n        switchInt(_120) -> [0: bb73, otherwise: bb74];\n    }\n    bb73: {\n        _120 = false;\n        StorageDead(_14);\n        StorageDead(_9);\n        StorageDead(_4);\n        StorageDead(_3);\n        return;\n    }\n    bb74: {\n        drop(_14) -> [return: bb73, unwind unreachable];\n    }\n    bb75: {\n        _126 = _9 as *const ();\n        _127 = _126 as usize;\n        _128 = Eq(_127, 0_usize);\n        _129 = BitAnd(_128, true);\n        _130 = Not(_129);\n        assert(_130, \"null pointer dereference occurred\") -> [success: bb76, unwind unreachable];\n    }\n    bb76: {\n        _105 = &((*_9).0: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_106);\n        _106 = core::sync::atomic::Ordering::Release;\n        _104 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::store(move _105, _89, move _106) -> [return: bb63, unwind unreachable];\n    }\n}\n",
  "doc": " Attempts to reserve a slot for sending a message.\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}