{
  "name": "sync::mpmc::list::Channel::<T>::discard_all_messages",
  "safe": true,
  "callees": {
    "sync::mpmc::utils::Backoff::new": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a new `Backoff`.\n",
      "adt": {
        "sync::mpmc::utils::Backoff": "Constructor"
      }
    },
    "core::ops::Deref::deref": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Dereferences the value.\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicUsize::load": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Loads a value from the atomic integer.\n\n `load` takes an [`Ordering`] argument which describes the memory ordering of this operation.\n Possible values are [`SeqCst`], [`Acquire`] and [`Relaxed`].\n\n # Panics\n\n Panics if `order` is [`Release`] or [`AcqRel`].\n\n # Examples\n\n ```\n\n\n assert_eq!(some_var.load(Ordering::Relaxed), 5);\n ```\n",
      "adt": {}
    },
    "sync::mpmc::utils::Backoff::spin_heavy": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Backs off using heavyweight spinning.\n\n This method should be used in blocking loops where parking the thread is not an option.\n",
      "adt": {
        "sync::mpmc::utils::Backoff": "ImmutableAsArgument"
      }
    },
    "core::ptr::null_mut": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a null mutable raw pointer.\n\n This function is equivalent to zero-initializing the pointer:\n `MaybeUninit::<*mut T>::zeroed().assume_init()`.\n The resulting pointer has the address 0.\n\n # Examples\n\n ```\n use std::ptr;\n\n let p: *mut i32 = ptr::null_mut();\n assert!(p.is_null());\n assert_eq!(p as usize, 0); // this pointer has the address 0\n ```\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicPtr::<T>::swap": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Stores a value into the pointer, returning the previous value.\n\n `swap` takes an [`Ordering`] argument which describes the memory ordering\n of this operation. All ordering modes are possible. Note that using\n [`Acquire`] makes the store part of this operation [`Relaxed`], and\n using [`Release`] makes the load part [`Relaxed`].\n\n **Note:** This method is only available on platforms that support atomic\n operations on pointers.\n\n # Examples\n\n ```\n use std::sync::atomic::{AtomicPtr, Ordering};\n\n let ptr = &mut 5;\n let some_ptr = AtomicPtr::new(ptr);\n\n let other_ptr = &mut 10;\n\n let value = some_ptr.swap(other_ptr, Ordering::Relaxed);\n ```\n",
      "adt": {}
    },
    "core::ptr::mut_ptr::<impl *mut T>::is_null": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "\n # Examples\n\n ```\n let mut s = [1, 2, 3];\n let ptr: *mut u32 = s.as_mut_ptr();\n assert!(!ptr.is_null());\n ```\n",
      "adt": {}
    },
    "sync::mpmc::list::Slot::<T>::wait_write": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Waits until a message is written into the slot.\n",
      "adt": {
        "sync::mpmc::list::Slot": "ImmutableAsArgument"
      }
    },
    "core::cell::UnsafeCell::<T>::get": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Gets a mutable pointer to the wrapped value.\n\n This can be cast to a pointer of any kind. When creating references, you must uphold the\n aliasing rules; see [the type-level docs][UnsafeCell#aliasing-rules] for more discussion and\n caveats.\n\n # Examples\n\n ```\n use std::cell::UnsafeCell;\n\n let uc = UnsafeCell::new(5);\n\n let five = uc.get();\n ```\n",
      "adt": {}
    },
    "core::ptr::mut_ptr::<impl *mut T>::drop_in_place": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Executes the destructor (if any) of the pointed-to value.\n\n See [`ptr::drop_in_place`] for safety concerns and examples.\n\n [`ptr::drop_in_place`]: crate::ptr::drop_in_place()\n",
      "adt": {}
    },
    "alloc_crate::boxed::Box::<T>::from_raw": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Constructs a box from a raw pointer.\n\n After calling this function, the raw pointer is owned by the\n resulting `Box`. Specifically, the `Box` destructor will call\n the destructor of `T` and free the allocated memory. For this\n to be safe, the memory must have been allocated in accordance\n with the [memory layout] used by `Box` .\n\n # Safety\n\n This function is unsafe because improper use may lead to\n memory problems. For example, a double-free may occur if the\n function is called twice on the same raw pointer.\n\n The raw pointer must point to a block of memory allocated by the global allocator.\n\n The safety conditions are described in the [memory layout] section.\n\n # Examples\n\n Recreate a `Box` which was previously converted to a raw pointer\n using [`Box::into_raw`]:\n ```\n let x = Box::new(5);\n let ptr = Box::into_raw(x);\n let x = unsafe { Box::from_raw(ptr) };\n ```\n Manually create a `Box` from scratch by using the global allocator:\n ```\n use std::alloc::{alloc, Layout};\n\n unsafe {\n     let ptr = alloc(Layout::new::<i32>()) as *mut i32;\n     // In general .write is required to avoid attempting to destruct\n     // the (uninitialized) previous contents of `ptr`, though for this\n     // simple example `*ptr = 5` would have worked as well.\n     ptr.write(5);\n     let x = Box::from_raw(ptr);\n }\n ```\n\n [memory layout]: self#memory-layout\n",
      "adt": {}
    },
    "core::mem::drop": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Disposes of a value.\n\n This effectively does nothing for types which implement `Copy`, e.g.\n integers. Such values are copied and _then_ moved into the function, so the\n value persists after this function call.\n\n This function is not magic; it is literally defined as\n\n ```\n pub fn drop<T>(_x: T) {}\n ```\n\n Because `_x` is moved into the function, it is automatically [dropped][drop] before\n the function returns.\n\n [drop]: Drop\n\n # Examples\n\n Basic usage:\n\n ```\n let v = vec![1, 2, 3];\n\n drop(v); // explicitly drop the vector\n ```\n\n Since [`RefCell`] enforces the borrow rules at runtime, `drop` can\n release a [`RefCell`] borrow:\n\n ```\n use std::cell::RefCell;\n\n let x = RefCell::new(1);\n\n let mut mutable_borrow = x.borrow_mut();\n *mutable_borrow = 1;\n\n drop(mutable_borrow); // relinquish the mutable borrow on this slot\n\n let borrow = x.borrow();\n println!(\"{}\", *borrow);\n ```\n\n Integers and other types implementing [`Copy`] are unaffected by `drop`.\n\n ```\n # #![allow(dropping_copy_types)]\n #[derive(Copy, Clone)]\n struct Foo(u8);\n\n let x = 1;\n let y = Foo(2);\n drop(x); // a copy of `x` is moved and dropped\n drop(y); // a copy of `y` is moved and dropped\n\n println!(\"x: {}, y: {}\", x, y.0); // still available\n ```\n\n [`RefCell`]: crate::cell::RefCell\n",
      "adt": {}
    },
    "core::num::<impl usize>::wrapping_add": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Wrapping (modular) addition. Computes `self + rhs`,\n wrapping around at the boundary of the type.\n\n # Examples\n\n ```\n ```\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicUsize::store": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Stores a value into the atomic integer.\n\n `store` takes an [`Ordering`] argument which describes the memory ordering of this operation.\n  Possible values are [`SeqCst`], [`Release`] and [`Relaxed`].\n\n # Panics\n\n Panics if `order` is [`Acquire`] or [`AcqRel`].\n\n # Examples\n\n ```\n\n\n some_var.store(10, Ordering::Relaxed);\n assert_eq!(some_var.load(Ordering::Relaxed), 10);\n ```\n",
      "adt": {}
    },
    "core::slice::<impl [T]>::get_unchecked": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns a reference to an element or subslice, without doing bounds\n checking.\n\n For a safe alternative see [`get`].\n\n # Safety\n\n Calling this method with an out-of-bounds index is *[undefined behavior]*\n even if the resulting reference is not used.\n\n You can think of this like `.get(index).unwrap_unchecked()`.  It's UB\n to call `.get_unchecked(len)`, even if you immediately convert to a\n pointer.  And it's UB to call `.get_unchecked(..len + 1)`,\n `.get_unchecked(..=len)`, or similar.\n\n [`get`]: slice::get\n [undefined behavior]: https://doc.rust-lang.org/reference/behavior-considered-undefined.html\n\n # Examples\n\n ```\n let x = &[1, 2, 4];\n\n unsafe {\n     assert_eq!(x.get_unchecked(1), &2);\n }\n ```\n",
      "adt": {}
    },
    "core::mem::MaybeUninit::<T>::as_mut_ptr": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Gets a mutable pointer to the contained value. Reading from this pointer or turning it\n into a reference is undefined behavior unless the `MaybeUninit<T>` is initialized.\n\n # Examples\n\n Correct usage of this method:\n\n ```rust\n use std::mem::MaybeUninit;\n\n let mut x = MaybeUninit::<Vec<u32>>::uninit();\n x.write(vec![0, 1, 2]);\n // Create a reference into the `MaybeUninit<Vec<u32>>`.\n // This is okay because we initialized it.\n let x_vec = unsafe { &mut *x.as_mut_ptr() };\n x_vec.push(3);\n assert_eq!(x_vec.len(), 4);\n # // Prevent leaks for Miri\n # unsafe { MaybeUninit::assume_init_drop(&mut x); }\n ```\n\n *Incorrect* usage of this method:\n\n ```rust,no_run\n use std::mem::MaybeUninit;\n\n let mut x = MaybeUninit::<Vec<u32>>::uninit();\n let x_vec = unsafe { &mut *x.as_mut_ptr() };\n // We have created a reference to an uninitialized vector! This is undefined behavior. ⚠️\n ```\n\n (Notice that the rules around references to uninitialized data are not finalized yet, but\n until they are, it is advisable to avoid them.)\n",
      "adt": {}
    },
    "sync::mpmc::list::Block::<T>::wait_next": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Waits until the next pointer is set.\n",
      "adt": {
        "sync::mpmc::list::Block": "ImmutableAsArgument"
      }
    },
    "core::sync::atomic::AtomicPtr::<T>::load": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Loads a value from the pointer.\n\n `load` takes an [`Ordering`] argument which describes the memory ordering\n of this operation. Possible values are [`SeqCst`], [`Acquire`] and [`Relaxed`].\n\n # Panics\n\n Panics if `order` is [`Release`] or [`AcqRel`].\n\n # Examples\n\n ```\n use std::sync::atomic::{AtomicPtr, Ordering};\n\n let ptr = &mut 5;\n let some_ptr = AtomicPtr::new(ptr);\n\n let value = some_ptr.load(Ordering::Relaxed);\n ```\n",
      "adt": {}
    }
  },
  "adts": {
    "sync::mpmc::utils::Backoff": [
      "Plain",
      "Ref"
    ],
    "sync::mpmc::utils::CachePadded": [
      "Ref"
    ],
    "sync::mpmc::list::Channel": [
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(1)))",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))",
      "Ref"
    ],
    "sync::mpmc::list::Position": [
      "Ref",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(1)))"
    ],
    "core::sync::atomic::AtomicUsize": [
      "Ref"
    ],
    "core::sync::atomic::Ordering": [
      "Plain"
    ],
    "core::sync::atomic::AtomicPtr": [
      "Ref"
    ],
    "sync::mpmc::list::Slot": [
      "Ref",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))"
    ],
    "core::cell::UnsafeCell": [
      "Ref"
    ],
    "alloc_crate::boxed::Box": [
      "Plain"
    ],
    "core::mem::MaybeUninit": [
      "MutRef"
    ],
    "sync::mpmc::list::Block": [
      "Ref"
    ]
  },
  "path": {
    "type": "Local",
    "path": "std::sync::mpmc::list::Channel::<T>::discard_all_messages"
  },
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sync/mpmc/list.rs:542:5: 612:6",
  "src": "fn discard_all_messages(&self) {\n        let backoff = Backoff::new();\n        let mut tail = self.tail.index.load(Ordering::Acquire);\n        loop {\n            let offset = (tail >> SHIFT) % LAP;\n            if offset != BLOCK_CAP {\n                break;\n            }\n\n            // New updates to tail will be rejected by MARK_BIT and aborted unless it's\n            // at boundary. We need to wait for the updates take affect otherwise there\n            // can be memory leaks.\n            backoff.spin_heavy();\n            tail = self.tail.index.load(Ordering::Acquire);\n        }\n\n        let mut head = self.head.index.load(Ordering::Acquire);\n        // The channel may be uninitialized, so we have to swap to avoid overwriting any sender's attempts\n        // to initialize the first block before noticing that the receivers disconnected. Late allocations\n        // will be deallocated by the sender in Drop.\n        let mut block = self.head.block.swap(ptr::null_mut(), Ordering::AcqRel);\n\n        // If we're going to be dropping messages we need to synchronize with initialization\n        if head >> SHIFT != tail >> SHIFT {\n            // The block can be null here only if a sender is in the process of initializing the\n            // channel while another sender managed to send a message by inserting it into the\n            // semi-initialized channel and advanced the tail.\n            // In that case, just wait until it gets initialized.\n            while block.is_null() {\n                backoff.spin_heavy();\n                block = self.head.block.swap(ptr::null_mut(), Ordering::AcqRel);\n            }\n        }\n        // After this point `head.block` is not modified again and it will be deallocated if it's\n        // non-null. The `Drop` code of the channel, which runs after this function, also attempts\n        // to deallocate `head.block` if it's non-null. Therefore this function must maintain the\n        // invariant that if a deallocation of head.block is attempted then it must also be set to\n        // NULL. Failing to do so will lead to the Drop code attempting a double free. For this\n        // reason both reads above do an atomic swap instead of a simple atomic load.\n\n        unsafe {\n            // Drop all messages between head and tail and deallocate the heap-allocated blocks.\n            while head >> SHIFT != tail >> SHIFT {\n                let offset = (head >> SHIFT) % LAP;\n\n                if offset < BLOCK_CAP {\n                    // Drop the message in the slot.\n                    let slot = (*block).slots.get_unchecked(offset);\n                    slot.wait_write();\n                    let p = &mut *slot.msg.get();\n                    p.as_mut_ptr().drop_in_place();\n                } else {\n                    (*block).wait_next();\n                    // Deallocate the block and move to the next one.\n                    let next = (*block).next.load(Ordering::Acquire);\n                    drop(Box::from_raw(block));\n                    block = next;\n                }\n\n                head = head.wrapping_add(1 << SHIFT);\n            }\n\n            // Deallocate the last remaining block.\n            if !block.is_null() {\n                drop(Box::from_raw(block));\n            }\n        }\n\n        head &= !MARK_BIT;\n        self.head.index.store(head, Ordering::Release);\n    }",
  "mir": "fn sync::mpmc::list::Channel::<T>::discard_all_messages(_1: &sync::mpmc::list::Channel<T>) -> () {\n    let mut _0: ();\n    let  _2: sync::mpmc::utils::Backoff;\n    let mut _3: usize;\n    let mut _4: &core::sync::atomic::AtomicUsize;\n    let  _5: &sync::mpmc::list::Position<T>;\n    let mut _6: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _7: core::sync::atomic::Ordering;\n    let  _8: usize;\n    let mut _9: usize;\n    let mut _10: usize;\n    let mut _11: bool;\n    let mut _12: bool;\n    let mut _13: bool;\n    let  _14: ();\n    let mut _15: &sync::mpmc::utils::Backoff;\n    let mut _16: usize;\n    let mut _17: &core::sync::atomic::AtomicUsize;\n    let  _18: &sync::mpmc::list::Position<T>;\n    let mut _19: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _20: core::sync::atomic::Ordering;\n    let mut _21: usize;\n    let mut _22: &core::sync::atomic::AtomicUsize;\n    let  _23: &sync::mpmc::list::Position<T>;\n    let mut _24: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _25: core::sync::atomic::Ordering;\n    let mut _26: *mut sync::mpmc::list::Block<T>;\n    let mut _27: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let  _28: &sync::mpmc::list::Position<T>;\n    let mut _29: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _30: *mut sync::mpmc::list::Block<T>;\n    let mut _31: core::sync::atomic::Ordering;\n    let mut _32: bool;\n    let mut _33: usize;\n    let mut _34: usize;\n    let mut _35: bool;\n    let mut _36: usize;\n    let mut _37: usize;\n    let mut _38: bool;\n    let mut _39: bool;\n    let mut _40: *mut sync::mpmc::list::Block<T>;\n    let  _41: ();\n    let mut _42: &sync::mpmc::utils::Backoff;\n    let mut _43: *mut sync::mpmc::list::Block<T>;\n    let mut _44: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let  _45: &sync::mpmc::list::Position<T>;\n    let mut _46: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _47: *mut sync::mpmc::list::Block<T>;\n    let mut _48: core::sync::atomic::Ordering;\n    let mut _49: bool;\n    let mut _50: usize;\n    let mut _51: usize;\n    let mut _52: bool;\n    let mut _53: usize;\n    let mut _54: usize;\n    let mut _55: bool;\n    let  _56: usize;\n    let mut _57: usize;\n    let mut _58: usize;\n    let mut _59: bool;\n    let mut _60: bool;\n    let mut _61: bool;\n    let  _62: &sync::mpmc::list::Slot<T>;\n    let mut _63: &[sync::mpmc::list::Slot<T>];\n    let mut _64: &[sync::mpmc::list::Slot<T>; 31];\n    let  _65: ();\n    let  _66: &mut core::mem::MaybeUninit<T>;\n    let mut _67: *mut core::mem::MaybeUninit<T>;\n    let mut _68: &core::cell::UnsafeCell<core::mem::MaybeUninit<T>>;\n    let  _69: ();\n    let mut _70: *mut T;\n    let  _71: *mut sync::mpmc::list::Block<T>;\n    let mut _72: &sync::mpmc::list::Block<T>;\n    let  _73: *mut sync::mpmc::list::Block<T>;\n    let mut _74: &core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>;\n    let mut _75: core::sync::atomic::Ordering;\n    let  _76: ();\n    let mut _77: alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>;\n    let mut _78: *mut sync::mpmc::list::Block<T>;\n    let mut _79: usize;\n    let mut _80: usize;\n    let mut _81: usize;\n    let mut _82: bool;\n    let mut _83: bool;\n    let mut _84: *mut sync::mpmc::list::Block<T>;\n    let  _85: ();\n    let mut _86: alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>;\n    let mut _87: *mut sync::mpmc::list::Block<T>;\n    let mut _88: usize;\n    let  _89: ();\n    let mut _90: &core::sync::atomic::AtomicUsize;\n    let  _91: &sync::mpmc::list::Position<T>;\n    let mut _92: &sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>;\n    let mut _93: usize;\n    let mut _94: core::sync::atomic::Ordering;\n    let mut _95: *const ();\n    let mut _96: usize;\n    let mut _97: usize;\n    let mut _98: usize;\n    let mut _99: bool;\n    let mut _100: *const ();\n    let mut _101: usize;\n    let mut _102: usize;\n    let mut _103: usize;\n    let mut _104: bool;\n    let mut _105: *const ();\n    let mut _106: usize;\n    let mut _107: usize;\n    let mut _108: usize;\n    let mut _109: bool;\n    let mut _110: *const ();\n    let mut _111: usize;\n    let mut _112: usize;\n    let mut _113: usize;\n    let mut _114: bool;\n    let mut _115: *const ();\n    let mut _116: usize;\n    let mut _117: bool;\n    let mut _118: bool;\n    let mut _119: bool;\n    let mut _120: *const ();\n    let mut _121: usize;\n    let mut _122: bool;\n    let mut _123: bool;\n    let mut _124: bool;\n    let mut _125: *const ();\n    let mut _126: usize;\n    let mut _127: bool;\n    let mut _128: bool;\n    let mut _129: bool;\n    let mut _130: *const ();\n    let mut _131: usize;\n    let mut _132: bool;\n    let mut _133: bool;\n    let mut _134: bool;\n    debug self => _1;\n    debug backoff => _2;\n    debug tail => _3;\n    debug offset => _8;\n    debug head => _21;\n    debug block => _26;\n    debug offset => _56;\n    debug slot => _62;\n    debug p => _66;\n    debug next => _73;\n    bb0: {\n        StorageLive(_2);\n        _2 = sync::mpmc::utils::Backoff::new() -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        StorageLive(_3);\n        StorageLive(_4);\n        StorageLive(_5);\n        StorageLive(_6);\n        _6 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _5 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _6) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        StorageDead(_6);\n        _4 = &((*_5).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_7);\n        _7 = core::sync::atomic::Ordering::Acquire;\n        _3 = core::sync::atomic::AtomicUsize::load(move _4, move _7) -> [return: bb3, unwind unreachable];\n    }\n    bb3: {\n        StorageDead(_7);\n        StorageDead(_4);\n        StorageDead(_5);\n        goto -> bb4;\n    }\n    bb4: {\n        StorageLive(_9);\n        StorageLive(_10);\n        _10 = _3;\n        _11 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _11, \"attempt to shift right by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb5, unwind unreachable];\n    }\n    bb5: {\n        _9 = Shr(move _10, sync::mpmc::list::SHIFT);\n        StorageDead(_10);\n        _12 = Eq(sync::mpmc::list::LAP, 0_usize);\n        assert(!move _12, \"attempt to calculate the remainder of `{}` with a divisor of zero\", _9) -> [success: bb6, unwind unreachable];\n    }\n    bb6: {\n        _8 = Rem(move _9, sync::mpmc::list::LAP);\n        StorageDead(_9);\n        StorageLive(_13);\n        _13 = Ne(_8, sync::mpmc::list::BLOCK_CAP);\n        switchInt(move _13) -> [0: bb8, otherwise: bb7];\n    }\n    bb7: {\n        StorageDead(_13);\n        StorageLive(_21);\n        StorageLive(_22);\n        StorageLive(_23);\n        StorageLive(_24);\n        _24 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _23 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _24) -> [return: bb12, unwind unreachable];\n    }\n    bb8: {\n        StorageDead(_13);\n        StorageLive(_15);\n        _15 = &_2;\n        _14 = sync::mpmc::utils::Backoff::spin_heavy(move _15) -> [return: bb9, unwind unreachable];\n    }\n    bb9: {\n        StorageDead(_15);\n        StorageLive(_16);\n        StorageLive(_17);\n        StorageLive(_18);\n        StorageLive(_19);\n        _19 = &((*_1).1: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _18 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _19) -> [return: bb10, unwind unreachable];\n    }\n    bb10: {\n        StorageDead(_19);\n        _17 = &((*_18).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_20);\n        _20 = core::sync::atomic::Ordering::Acquire;\n        _16 = core::sync::atomic::AtomicUsize::load(move _17, move _20) -> [return: bb11, unwind unreachable];\n    }\n    bb11: {\n        StorageDead(_20);\n        StorageDead(_17);\n        _3 = move _16;\n        StorageDead(_16);\n        StorageDead(_18);\n        goto -> bb4;\n    }\n    bb12: {\n        StorageDead(_24);\n        _22 = &((*_23).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_25);\n        _25 = core::sync::atomic::Ordering::Acquire;\n        _21 = core::sync::atomic::AtomicUsize::load(move _22, move _25) -> [return: bb13, unwind unreachable];\n    }\n    bb13: {\n        StorageDead(_25);\n        StorageDead(_22);\n        StorageDead(_23);\n        StorageLive(_26);\n        StorageLive(_27);\n        StorageLive(_28);\n        StorageLive(_29);\n        _29 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _28 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _29) -> [return: bb14, unwind unreachable];\n    }\n    bb14: {\n        StorageDead(_29);\n        _27 = &((*_28).1: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_30);\n        _30 = core::ptr::null_mut::<sync::mpmc::list::Block<T>>() -> [return: bb15, unwind unreachable];\n    }\n    bb15: {\n        StorageLive(_31);\n        _31 = core::sync::atomic::Ordering::AcqRel;\n        _26 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::swap(move _27, move _30, move _31) -> [return: bb16, unwind unreachable];\n    }\n    bb16: {\n        StorageDead(_31);\n        StorageDead(_30);\n        StorageDead(_27);\n        StorageDead(_28);\n        StorageLive(_32);\n        StorageLive(_33);\n        StorageLive(_34);\n        _34 = _21;\n        _35 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _35, \"attempt to shift right by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb17, unwind unreachable];\n    }\n    bb17: {\n        _33 = Shr(move _34, sync::mpmc::list::SHIFT);\n        StorageDead(_34);\n        StorageLive(_36);\n        StorageLive(_37);\n        _37 = _3;\n        _38 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _38, \"attempt to shift right by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb18, unwind unreachable];\n    }\n    bb18: {\n        _36 = Shr(move _37, sync::mpmc::list::SHIFT);\n        StorageDead(_37);\n        _32 = Ne(move _33, move _36);\n        switchInt(move _32) -> [0: bb28, otherwise: bb19];\n    }\n    bb19: {\n        StorageDead(_36);\n        StorageDead(_33);\n        goto -> bb20;\n    }\n    bb20: {\n        StorageLive(_39);\n        StorageLive(_40);\n        _40 = _26;\n        _39 = core::ptr::mut_ptr::<impl *mut sync::mpmc::list::Block<T>>::is_null(move _40) -> [return: bb21, unwind unreachable];\n    }\n    bb21: {\n        switchInt(move _39) -> [0: bb27, otherwise: bb22];\n    }\n    bb22: {\n        StorageDead(_40);\n        StorageLive(_42);\n        _42 = &_2;\n        _41 = sync::mpmc::utils::Backoff::spin_heavy(move _42) -> [return: bb23, unwind unreachable];\n    }\n    bb23: {\n        StorageDead(_42);\n        StorageLive(_43);\n        StorageLive(_44);\n        StorageLive(_45);\n        StorageLive(_46);\n        _46 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _45 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _46) -> [return: bb24, unwind unreachable];\n    }\n    bb24: {\n        StorageDead(_46);\n        _44 = &((*_45).1: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_47);\n        _47 = core::ptr::null_mut::<sync::mpmc::list::Block<T>>() -> [return: bb25, unwind unreachable];\n    }\n    bb25: {\n        StorageLive(_48);\n        _48 = core::sync::atomic::Ordering::AcqRel;\n        _43 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::swap(move _44, move _47, move _48) -> [return: bb26, unwind unreachable];\n    }\n    bb26: {\n        StorageDead(_48);\n        StorageDead(_47);\n        StorageDead(_44);\n        _26 = move _43;\n        StorageDead(_43);\n        StorageDead(_45);\n        StorageDead(_39);\n        goto -> bb20;\n    }\n    bb27: {\n        StorageDead(_40);\n        StorageDead(_39);\n        goto -> bb29;\n    }\n    bb28: {\n        StorageDead(_36);\n        StorageDead(_33);\n        goto -> bb29;\n    }\n    bb29: {\n        StorageDead(_32);\n        goto -> bb30;\n    }\n    bb30: {\n        StorageLive(_49);\n        StorageLive(_50);\n        StorageLive(_51);\n        _51 = _21;\n        _52 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _52, \"attempt to shift right by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb31, unwind unreachable];\n    }\n    bb31: {\n        _50 = Shr(move _51, sync::mpmc::list::SHIFT);\n        StorageDead(_51);\n        StorageLive(_53);\n        StorageLive(_54);\n        _54 = _3;\n        _55 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _55, \"attempt to shift right by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb32, unwind unreachable];\n    }\n    bb32: {\n        _53 = Shr(move _54, sync::mpmc::list::SHIFT);\n        StorageDead(_54);\n        _49 = Ne(move _50, move _53);\n        switchInt(move _49) -> [0: bb50, otherwise: bb33];\n    }\n    bb33: {\n        StorageDead(_53);\n        StorageDead(_50);\n        StorageLive(_57);\n        StorageLive(_58);\n        _58 = _21;\n        _59 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _59, \"attempt to shift right by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb34, unwind unreachable];\n    }\n    bb34: {\n        _57 = Shr(move _58, sync::mpmc::list::SHIFT);\n        StorageDead(_58);\n        _60 = Eq(sync::mpmc::list::LAP, 0_usize);\n        assert(!move _60, \"attempt to calculate the remainder of `{}` with a divisor of zero\", _57) -> [success: bb35, unwind unreachable];\n    }\n    bb35: {\n        _56 = Rem(move _57, sync::mpmc::list::LAP);\n        StorageDead(_57);\n        StorageLive(_61);\n        _61 = Lt(_56, sync::mpmc::list::BLOCK_CAP);\n        switchInt(move _61) -> [0: bb42, otherwise: bb36];\n    }\n    bb36: {\n        StorageLive(_63);\n        StorageLive(_64);\n        _110 = _26 as *const ();\n        _111 = _110 as usize;\n        _112 = Sub(<[sync::mpmc::list::Slot<T>; 31] as core::mem::SizedTypeProperties>::ALIGN, 1_usize);\n        _113 = BitAnd(_111, _112);\n        _114 = Eq(_113, 0_usize);\n        assert(_114, \"misaligned pointer dereference: address must be a multiple of {} but is {}\",<[sync::mpmc::list::Slot<T>; 31] as core::mem::SizedTypeProperties>::ALIGN, _111) -> [success: bb62, unwind unreachable];\n    }\n    bb37: {\n        StorageDead(_63);\n        _65 = sync::mpmc::list::Slot::<T>::wait_write(_62) -> [return: bb38, unwind unreachable];\n    }\n    bb38: {\n        StorageLive(_67);\n        StorageLive(_68);\n        _68 = &((*_62).0: core::cell::UnsafeCell<core::mem::MaybeUninit<T>>);\n        _67 = core::cell::UnsafeCell::<core::mem::MaybeUninit<T>>::get(move _68) -> [return: bb39, unwind unreachable];\n    }\n    bb39: {\n        StorageDead(_68);\n        _105 = _67 as *const ();\n        _106 = _105 as usize;\n        _107 = Sub(<core::mem::MaybeUninit<T> as core::mem::SizedTypeProperties>::ALIGN, 1_usize);\n        _108 = BitAnd(_106, _107);\n        _109 = Eq(_108, 0_usize);\n        assert(_109, \"misaligned pointer dereference: address must be a multiple of {} but is {}\",<core::mem::MaybeUninit<T> as core::mem::SizedTypeProperties>::ALIGN, _106) -> [success: bb61, unwind unreachable];\n    }\n    bb40: {\n        _69 = core::ptr::mut_ptr::<impl *mut T>::drop_in_place(move _70) -> [return: bb41, unwind unreachable];\n    }\n    bb41: {\n        StorageDead(_70);\n        StorageDead(_67);\n        goto -> bb47;\n    }\n    bb42: {\n        StorageLive(_71);\n        StorageLive(_72);\n        _100 = _26 as *const ();\n        _101 = _100 as usize;\n        _102 = Sub(<sync::mpmc::list::Block<T> as core::mem::SizedTypeProperties>::ALIGN, 1_usize);\n        _103 = BitAnd(_101, _102);\n        _104 = Eq(_103, 0_usize);\n        assert(_104, \"misaligned pointer dereference: address must be a multiple of {} but is {}\",<sync::mpmc::list::Block<T> as core::mem::SizedTypeProperties>::ALIGN, _101) -> [success: bb60, unwind unreachable];\n    }\n    bb43: {\n        StorageDead(_72);\n        StorageDead(_71);\n        StorageLive(_74);\n        _95 = _26 as *const ();\n        _96 = _95 as usize;\n        _97 = Sub(<core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>> as core::mem::SizedTypeProperties>::ALIGN, 1_usize);\n        _98 = BitAnd(_96, _97);\n        _99 = Eq(_98, 0_usize);\n        assert(_99, \"misaligned pointer dereference: address must be a multiple of {} but is {}\",<core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>> as core::mem::SizedTypeProperties>::ALIGN, _96) -> [success: bb59, unwind unreachable];\n    }\n    bb44: {\n        StorageDead(_75);\n        StorageDead(_74);\n        StorageLive(_77);\n        StorageLive(_78);\n        _78 = _26;\n        _77 = alloc_crate::boxed::Box::<sync::mpmc::list::Block<T>>::from_raw(move _78) -> [return: bb45, unwind unreachable];\n    }\n    bb45: {\n        StorageDead(_78);\n        _76 = core::mem::drop::<alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>>(move _77) -> [return: bb46, unwind unreachable];\n    }\n    bb46: {\n        StorageDead(_77);\n        _26 = _73;\n        goto -> bb47;\n    }\n    bb47: {\n        StorageDead(_61);\n        StorageLive(_79);\n        StorageLive(_80);\n        _80 = _21;\n        StorageLive(_81);\n        _82 = Lt(sync::mpmc::list::SHIFT, 64_usize);\n        assert(move _82, \"attempt to shift left by `{}`, which would overflow\", sync::mpmc::list::SHIFT) -> [success: bb48, unwind unreachable];\n    }\n    bb48: {\n        _81 = Shl(1_usize, sync::mpmc::list::SHIFT);\n        _79 = core::num::<impl usize>::wrapping_add(move _80, move _81) -> [return: bb49, unwind unreachable];\n    }\n    bb49: {\n        StorageDead(_81);\n        StorageDead(_80);\n        _21 = move _79;\n        StorageDead(_79);\n        StorageDead(_49);\n        goto -> bb30;\n    }\n    bb50: {\n        StorageDead(_53);\n        StorageDead(_50);\n        StorageDead(_49);\n        StorageLive(_83);\n        StorageLive(_84);\n        _84 = _26;\n        _83 = core::ptr::mut_ptr::<impl *mut sync::mpmc::list::Block<T>>::is_null(move _84) -> [return: bb51, unwind unreachable];\n    }\n    bb51: {\n        switchInt(move _83) -> [0: bb53, otherwise: bb52];\n    }\n    bb52: {\n        StorageDead(_84);\n        goto -> bb56;\n    }\n    bb53: {\n        StorageDead(_84);\n        StorageLive(_86);\n        StorageLive(_87);\n        _87 = _26;\n        _86 = alloc_crate::boxed::Box::<sync::mpmc::list::Block<T>>::from_raw(move _87) -> [return: bb54, unwind unreachable];\n    }\n    bb54: {\n        StorageDead(_87);\n        _85 = core::mem::drop::<alloc_crate::boxed::Box<sync::mpmc::list::Block<T>>>(move _86) -> [return: bb55, unwind unreachable];\n    }\n    bb55: {\n        StorageDead(_86);\n        goto -> bb56;\n    }\n    bb56: {\n        StorageDead(_83);\n        StorageLive(_88);\n        _88 = Not(sync::mpmc::list::MARK_BIT);\n        _21 = BitAnd(_21, move _88);\n        StorageDead(_88);\n        StorageLive(_90);\n        StorageLive(_91);\n        StorageLive(_92);\n        _92 = &((*_1).0: sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>>);\n        _91 = <sync::mpmc::utils::CachePadded<sync::mpmc::list::Position<T>> as core::ops::Deref>::deref(move _92) -> [return: bb57, unwind unreachable];\n    }\n    bb57: {\n        StorageDead(_92);\n        _90 = &((*_91).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_93);\n        _93 = _21;\n        StorageLive(_94);\n        _94 = core::sync::atomic::Ordering::Release;\n        _89 = core::sync::atomic::AtomicUsize::store(move _90, move _93, move _94) -> [return: bb58, unwind unreachable];\n    }\n    bb58: {\n        StorageDead(_94);\n        StorageDead(_93);\n        StorageDead(_90);\n        StorageDead(_91);\n        StorageDead(_26);\n        StorageDead(_21);\n        StorageDead(_3);\n        StorageDead(_2);\n        return;\n    }\n    bb59: {\n        _130 = _26 as *const ();\n        _131 = _130 as usize;\n        _132 = Eq(_131, 0_usize);\n        _133 = BitAnd(_132, true);\n        _134 = Not(_133);\n        assert(_134, \"null pointer dereference occurred\") -> [success: bb66, unwind unreachable];\n    }\n    bb60: {\n        _125 = _26 as *const ();\n        _126 = _125 as usize;\n        _127 = Eq(_126, 0_usize);\n        _128 = BitAnd(_127, true);\n        _129 = Not(_128);\n        assert(_129, \"null pointer dereference occurred\") -> [success: bb65, unwind unreachable];\n    }\n    bb61: {\n        _120 = _67 as *const ();\n        _121 = _120 as usize;\n        _122 = Eq(_121, 0_usize);\n        _123 = BitAnd(_122, true);\n        _124 = Not(_123);\n        assert(_124, \"null pointer dereference occurred\") -> [success: bb64, unwind unreachable];\n    }\n    bb62: {\n        _115 = _26 as *const ();\n        _116 = _115 as usize;\n        _117 = Eq(_116, 0_usize);\n        _118 = BitAnd(_117, true);\n        _119 = Not(_118);\n        assert(_119, \"null pointer dereference occurred\") -> [success: bb63, unwind unreachable];\n    }\n    bb63: {\n        _64 = &((*_26).1: [sync::mpmc::list::Slot<T>; 31]);\n        _63 = move _64 as &[sync::mpmc::list::Slot<T>];\n        StorageDead(_64);\n        _62 = core::slice::<impl [sync::mpmc::list::Slot<T>]>::get_unchecked::<usize>(move _63, _56) -> [return: bb37, unwind unreachable];\n    }\n    bb64: {\n        _66 = &mut (*_67);\n        StorageLive(_70);\n        _70 = core::mem::MaybeUninit::<T>::as_mut_ptr(_66) -> [return: bb40, unwind unreachable];\n    }\n    bb65: {\n        _72 = &(*_26);\n        _71 = sync::mpmc::list::Block::<T>::wait_next(move _72) -> [return: bb43, unwind unreachable];\n    }\n    bb66: {\n        _74 = &((*_26).0: core::sync::atomic::AtomicPtr<sync::mpmc::list::Block<T>>);\n        StorageLive(_75);\n        _75 = core::sync::atomic::Ordering::Acquire;\n        _73 = core::sync::atomic::AtomicPtr::<sync::mpmc::list::Block<T>>::load(move _74, move _75) -> [return: bb44, unwind unreachable];\n    }\n}\n",
  "doc": " Discards all messages.\n\n This method should only be called when all receivers are dropped.\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}