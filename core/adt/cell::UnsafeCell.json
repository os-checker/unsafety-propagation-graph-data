{
  "name": "cell::UnsafeCell",
  "constructors": [
    "cell::UnsafeCell::<T>::new",
    "<cell::UnsafeCell<T> as default::Default>::default",
    "<cell::UnsafeCell<T> as convert::From<T>>::from"
  ],
  "access_self_as_arg": {
    "read": [
      "cell::UnsafeCell::<T>::replace",
      "cell::UnsafeCell::<T>::get",
      "cell::UnsafeCell::<T>::as_ref_unchecked",
      "cell::UnsafeCell::<T>::as_mut_unchecked",
      "<cell::UnsafeCell<T> as fmt::Debug>::fmt"
    ],
    "write": [
      "cell::UnsafeCell::<T>::get_mut",
      "cell::UnsafeCell::<T>::get"
    ],
    "other": [
      "cell::UnsafeCell::<T>::into_inner",
      "cell::assert_coerce_unsized",
      "cell::UnsafeCell::<T>::into_inner"
    ]
  },
  "access_self_as_locals": {
    "read": [
      "cell::lazy::LazyCell::<T, F>::force",
      "cell::lazy::LazyCell::<T, F>::really_init",
      "cell::lazy::LazyCell::<T, F>::get",
      "cell::once::OnceCell::<T>::get",
      "cell::once::OnceCell::<T>::try_insert",
      "cell::Cell::<T>::swap",
      "cell::Cell::<T>::replace",
      "cell::Cell::<T>::get",
      "cell::Cell::<T>::as_ptr",
      "cell::RefCell::<T>::try_borrow",
      "cell::RefCell::<T>::try_borrow_mut",
      "cell::RefCell::<T>::as_ptr",
      "cell::RefCell::<T>::try_borrow_unguarded",
      "cell::SyncUnsafeCell::<T>::get",
      "pin::unsafe_pinned::UnsafePinned::<T>::get",
      "sync::atomic::AtomicBool::get_mut",
      "sync::atomic::AtomicBool::load",
      "sync::atomic::AtomicBool::store",
      "sync::atomic::AtomicBool::swap",
      "sync::atomic::AtomicBool::compare_exchange",
      "sync::atomic::AtomicBool::compare_exchange_weak",
      "sync::atomic::AtomicBool::fetch_and",
      "sync::atomic::AtomicBool::fetch_or",
      "sync::atomic::AtomicBool::fetch_xor",
      "sync::atomic::AtomicBool::as_ptr",
      "sync::atomic::AtomicPtr::<T>::load",
      "sync::atomic::AtomicPtr::<T>::store",
      "sync::atomic::AtomicPtr::<T>::swap",
      "sync::atomic::AtomicPtr::<T>::compare_exchange",
      "sync::atomic::AtomicPtr::<T>::compare_exchange_weak",
      "sync::atomic::AtomicPtr::<T>::fetch_byte_add",
      "sync::atomic::AtomicPtr::<T>::fetch_byte_sub",
      "sync::atomic::AtomicPtr::<T>::fetch_or",
      "sync::atomic::AtomicPtr::<T>::fetch_and",
      "sync::atomic::AtomicPtr::<T>::fetch_xor",
      "sync::atomic::AtomicPtr::<T>::as_ptr",
      "sync::atomic::AtomicI8::load",
      "sync::atomic::AtomicI8::store",
      "sync::atomic::AtomicI8::swap",
      "sync::atomic::AtomicI8::compare_exchange",
      "sync::atomic::AtomicI8::compare_exchange_weak",
      "sync::atomic::AtomicI8::fetch_add",
      "sync::atomic::AtomicI8::fetch_sub",
      "sync::atomic::AtomicI8::fetch_and",
      "sync::atomic::AtomicI8::fetch_nand",
      "sync::atomic::AtomicI8::fetch_or",
      "sync::atomic::AtomicI8::fetch_xor",
      "sync::atomic::AtomicI8::fetch_max",
      "sync::atomic::AtomicI8::fetch_min",
      "sync::atomic::AtomicI8::as_ptr",
      "sync::atomic::AtomicU8::load",
      "sync::atomic::AtomicU8::store",
      "sync::atomic::AtomicU8::swap",
      "sync::atomic::AtomicU8::compare_exchange",
      "sync::atomic::AtomicU8::compare_exchange_weak",
      "sync::atomic::AtomicU8::fetch_add",
      "sync::atomic::AtomicU8::fetch_sub",
      "sync::atomic::AtomicU8::fetch_and",
      "sync::atomic::AtomicU8::fetch_nand",
      "sync::atomic::AtomicU8::fetch_or",
      "sync::atomic::AtomicU8::fetch_xor",
      "sync::atomic::AtomicU8::fetch_max",
      "sync::atomic::AtomicU8::fetch_min",
      "sync::atomic::AtomicU8::as_ptr",
      "sync::atomic::AtomicI16::load",
      "sync::atomic::AtomicI16::store",
      "sync::atomic::AtomicI16::swap",
      "sync::atomic::AtomicI16::compare_exchange",
      "sync::atomic::AtomicI16::compare_exchange_weak",
      "sync::atomic::AtomicI16::fetch_add",
      "sync::atomic::AtomicI16::fetch_sub",
      "sync::atomic::AtomicI16::fetch_and",
      "sync::atomic::AtomicI16::fetch_nand",
      "sync::atomic::AtomicI16::fetch_or",
      "sync::atomic::AtomicI16::fetch_xor",
      "sync::atomic::AtomicI16::fetch_max",
      "sync::atomic::AtomicI16::fetch_min",
      "sync::atomic::AtomicI16::as_ptr",
      "sync::atomic::AtomicU16::load",
      "sync::atomic::AtomicU16::store",
      "sync::atomic::AtomicU16::swap",
      "sync::atomic::AtomicU16::compare_exchange",
      "sync::atomic::AtomicU16::compare_exchange_weak",
      "sync::atomic::AtomicU16::fetch_add",
      "sync::atomic::AtomicU16::fetch_sub",
      "sync::atomic::AtomicU16::fetch_and",
      "sync::atomic::AtomicU16::fetch_nand",
      "sync::atomic::AtomicU16::fetch_or",
      "sync::atomic::AtomicU16::fetch_xor",
      "sync::atomic::AtomicU16::fetch_max",
      "sync::atomic::AtomicU16::fetch_min",
      "sync::atomic::AtomicU16::as_ptr",
      "sync::atomic::AtomicI32::load",
      "sync::atomic::AtomicI32::store",
      "sync::atomic::AtomicI32::swap",
      "sync::atomic::AtomicI32::compare_exchange",
      "sync::atomic::AtomicI32::compare_exchange_weak",
      "sync::atomic::AtomicI32::fetch_add",
      "sync::atomic::AtomicI32::fetch_sub",
      "sync::atomic::AtomicI32::fetch_and",
      "sync::atomic::AtomicI32::fetch_nand",
      "sync::atomic::AtomicI32::fetch_or",
      "sync::atomic::AtomicI32::fetch_xor",
      "sync::atomic::AtomicI32::fetch_max",
      "sync::atomic::AtomicI32::fetch_min",
      "sync::atomic::AtomicI32::as_ptr",
      "sync::atomic::AtomicU32::load",
      "sync::atomic::AtomicU32::store",
      "sync::atomic::AtomicU32::swap",
      "sync::atomic::AtomicU32::compare_exchange",
      "sync::atomic::AtomicU32::compare_exchange_weak",
      "sync::atomic::AtomicU32::fetch_add",
      "sync::atomic::AtomicU32::fetch_sub",
      "sync::atomic::AtomicU32::fetch_and",
      "sync::atomic::AtomicU32::fetch_nand",
      "sync::atomic::AtomicU32::fetch_or",
      "sync::atomic::AtomicU32::fetch_xor",
      "sync::atomic::AtomicU32::fetch_max",
      "sync::atomic::AtomicU32::fetch_min",
      "sync::atomic::AtomicU32::as_ptr",
      "sync::atomic::AtomicI64::load",
      "sync::atomic::AtomicI64::store",
      "sync::atomic::AtomicI64::swap",
      "sync::atomic::AtomicI64::compare_exchange",
      "sync::atomic::AtomicI64::compare_exchange_weak",
      "sync::atomic::AtomicI64::fetch_add",
      "sync::atomic::AtomicI64::fetch_sub",
      "sync::atomic::AtomicI64::fetch_and",
      "sync::atomic::AtomicI64::fetch_nand",
      "sync::atomic::AtomicI64::fetch_or",
      "sync::atomic::AtomicI64::fetch_xor",
      "sync::atomic::AtomicI64::fetch_max",
      "sync::atomic::AtomicI64::fetch_min",
      "sync::atomic::AtomicI64::as_ptr",
      "sync::atomic::AtomicU64::load",
      "sync::atomic::AtomicU64::store",
      "sync::atomic::AtomicU64::swap",
      "sync::atomic::AtomicU64::compare_exchange",
      "sync::atomic::AtomicU64::compare_exchange_weak",
      "sync::atomic::AtomicU64::fetch_add",
      "sync::atomic::AtomicU64::fetch_sub",
      "sync::atomic::AtomicU64::fetch_and",
      "sync::atomic::AtomicU64::fetch_nand",
      "sync::atomic::AtomicU64::fetch_or",
      "sync::atomic::AtomicU64::fetch_xor",
      "sync::atomic::AtomicU64::fetch_max",
      "sync::atomic::AtomicU64::fetch_min",
      "sync::atomic::AtomicU64::as_ptr",
      "sync::atomic::AtomicIsize::load",
      "sync::atomic::AtomicIsize::store",
      "sync::atomic::AtomicIsize::swap",
      "sync::atomic::AtomicIsize::compare_exchange",
      "sync::atomic::AtomicIsize::compare_exchange_weak",
      "sync::atomic::AtomicIsize::fetch_add",
      "sync::atomic::AtomicIsize::fetch_sub",
      "sync::atomic::AtomicIsize::fetch_and",
      "sync::atomic::AtomicIsize::fetch_nand",
      "sync::atomic::AtomicIsize::fetch_or",
      "sync::atomic::AtomicIsize::fetch_xor",
      "sync::atomic::AtomicIsize::fetch_max",
      "sync::atomic::AtomicIsize::fetch_min",
      "sync::atomic::AtomicIsize::as_ptr",
      "sync::atomic::AtomicUsize::load",
      "sync::atomic::AtomicUsize::store",
      "sync::atomic::AtomicUsize::swap",
      "sync::atomic::AtomicUsize::compare_exchange",
      "sync::atomic::AtomicUsize::compare_exchange_weak",
      "sync::atomic::AtomicUsize::fetch_add",
      "sync::atomic::AtomicUsize::fetch_sub",
      "sync::atomic::AtomicUsize::fetch_and",
      "sync::atomic::AtomicUsize::fetch_nand",
      "sync::atomic::AtomicUsize::fetch_or",
      "sync::atomic::AtomicUsize::fetch_xor",
      "sync::atomic::AtomicUsize::fetch_max",
      "sync::atomic::AtomicUsize::fetch_min",
      "sync::atomic::AtomicUsize::as_ptr"
    ],
    "write": [
      "cell::lazy::LazyCell::<T, F>::force_mut",
      "cell::lazy::LazyCell::<T, F>::get_mut",
      "cell::once::OnceCell::<T>::get_mut",
      "cell::Cell::<T>::get_mut",
      "cell::RefCell::<T>::get_mut",
      "cell::UnsafeCell::<T>::from_mut",
      "cell::SyncUnsafeCell::<T>::get_mut",
      "sync::atomic::AtomicPtr::<T>::get_mut",
      "sync::atomic::AtomicI8::get_mut",
      "sync::atomic::AtomicU8::get_mut",
      "sync::atomic::AtomicI16::get_mut",
      "sync::atomic::AtomicU16::get_mut",
      "sync::atomic::AtomicI32::get_mut",
      "sync::atomic::AtomicU32::get_mut",
      "sync::atomic::AtomicI64::get_mut",
      "sync::atomic::AtomicU64::get_mut",
      "sync::atomic::AtomicIsize::get_mut",
      "sync::atomic::AtomicUsize::get_mut"
    ],
    "other": [
      "cell::lazy::LazyCell::<T, F>::new",
      "cell::lazy::LazyCell::<T, F>::into_inner",
      "cell::once::OnceCell::<T>::new",
      "cell::once::OnceCell::<T>::into_inner",
      "<cell::once::OnceCell<T> as convert::From<T>>::from",
      "cell::Cell::<T>::new",
      "cell::Cell::<T>::into_inner",
      "cell::RefCell::<T>::new",
      "cell::RefCell::<T>::into_inner",
      "cell::UnsafeCell::<T>::new",
      "<cell::UnsafeCell<T> as default::Default>::default",
      "<cell::UnsafeCell<T> as convert::From<T>>::from",
      "cell::SyncUnsafeCell::<T>::new",
      "cell::SyncUnsafeCell::<T>::into_inner",
      "pin::unsafe_pinned::UnsafePinned::<T>::new",
      "pin::unsafe_pinned::UnsafePinned::<T>::into_inner",
      "sync::atomic::AtomicBool::new",
      "sync::atomic::AtomicBool::into_inner",
      "sync::atomic::AtomicPtr::<T>::new",
      "sync::atomic::AtomicPtr::<T>::into_inner",
      "sync::atomic::AtomicI8::new",
      "sync::atomic::AtomicI8::into_inner",
      "sync::atomic::AtomicU8::new",
      "sync::atomic::AtomicU8::into_inner",
      "sync::atomic::AtomicI16::new",
      "sync::atomic::AtomicI16::into_inner",
      "sync::atomic::AtomicU16::new",
      "sync::atomic::AtomicU16::into_inner",
      "sync::atomic::AtomicI32::new",
      "sync::atomic::AtomicI32::into_inner",
      "sync::atomic::AtomicU32::new",
      "sync::atomic::AtomicU32::into_inner",
      "sync::atomic::AtomicI64::new",
      "sync::atomic::AtomicI64::into_inner",
      "sync::atomic::AtomicU64::new",
      "sync::atomic::AtomicU64::into_inner",
      "sync::atomic::AtomicIsize::new",
      "sync::atomic::AtomicIsize::into_inner",
      "sync::atomic::AtomicUsize::new",
      "sync::atomic::AtomicUsize::into_inner"
    ]
  },
  "access_field": [
    {
      "read": [],
      "write": [
        "cell::UnsafeCell::<T>::get_mut"
      ],
      "other": []
    }
  ],
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/cell.rs:2301:1: 2301:33",
  "src": "pub struct UnsafeCell<T: ?Sized>",
  "kind": "Struct",
  "doc_adt": " The core primitive for interior mutability in Rust.\n\n If you have a reference `&T`, then normally in Rust the compiler performs optimizations based on\n the knowledge that `&T` points to immutable data. Mutating that data, for example through an\n alias or by transmuting a `&T` into a `&mut T`, is considered undefined behavior.\n `UnsafeCell<T>` opts-out of the immutability guarantee for `&T`: a shared reference\n `&UnsafeCell<T>` may point to data that is being mutated. This is called \"interior mutability\".\n\n All other types that allow internal mutability, such as [`Cell<T>`] and [`RefCell<T>`], internally\n use `UnsafeCell` to wrap their data.\n\n Note that only the immutability guarantee for shared references is affected by `UnsafeCell`. The\n uniqueness guarantee for mutable references is unaffected. There is *no* legal way to obtain\n aliasing `&mut`, not even with `UnsafeCell<T>`.\n\n `UnsafeCell` does nothing to avoid data races; they are still undefined behavior. If multiple\n threads have access to the same `UnsafeCell`, they must follow the usual rules of the\n [concurrent memory model]: conflicting non-synchronized accesses must be done via the APIs in\n [`core::sync::atomic`].\n\n The `UnsafeCell` API itself is technically very simple: [`.get()`] gives you a raw pointer\n `*mut T` to its contents. It is up to _you_ as the abstraction designer to use that raw pointer\n correctly.\n\n [`.get()`]: `UnsafeCell::get`\n [concurrent memory model]: ../sync/atomic/index.html#memory-model-for-atomic-accesses\n\n # Aliasing rules\n\n The precise Rust aliasing rules are somewhat in flux, but the main points are not contentious:\n\n - If you create a safe reference with lifetime `'a` (either a `&T` or `&mut T` reference), then\n   you must not access the data in any way that contradicts that reference for the remainder of\n   `'a`. For example, this means that if you take the `*mut T` from an `UnsafeCell<T>` and cast it\n   to an `&T`, then the data in `T` must remain immutable (modulo any `UnsafeCell` data found\n   within `T`, of course) until that reference's lifetime expires. Similarly, if you create a\n   `&mut T` reference that is released to safe code, then you must not access the data within the\n   `UnsafeCell` until that reference expires.\n\n - For both `&T` without `UnsafeCell<_>` and `&mut T`, you must also not deallocate the data\n   until the reference expires. As a special exception, given an `&T`, any part of it that is\n   inside an `UnsafeCell<_>` may be deallocated during the lifetime of the reference, after the\n   last time the reference is used (dereferenced or reborrowed). Since you cannot deallocate a part\n   of what a reference points to, this means the memory an `&T` points to can be deallocated only if\n   *every part of it* (including padding) is inside an `UnsafeCell`.\n\n However, whenever a `&UnsafeCell<T>` is constructed or dereferenced, it must still point to\n live memory and the compiler is allowed to insert spurious reads if it can prove that this\n memory has not yet been deallocated.\n\n To assist with proper design, the following scenarios are explicitly declared legal\n for single-threaded code:\n\n 1. A `&T` reference can be released to safe code and there it can co-exist with other `&T`\n    references, but not with a `&mut T`\n\n 2. A `&mut T` reference may be released to safe code provided neither other `&mut T` nor `&T`\n    co-exist with it. A `&mut T` must always be unique.\n\n Note that whilst mutating the contents of an `&UnsafeCell<T>` (even while other\n `&UnsafeCell<T>` references alias the cell) is\n ok (provided you enforce the above invariants some other way), it is still undefined behavior\n to have multiple `&mut UnsafeCell<T>` aliases. That is, `UnsafeCell` is a wrapper\n designed to have a special interaction with _shared_ accesses (_i.e._, through an\n `&UnsafeCell<_>` reference); there is no magic whatsoever when dealing with _exclusive_\n accesses (_e.g._, through a `&mut UnsafeCell<_>`): neither the cell nor the wrapped value\n may be aliased for the duration of that `&mut` borrow.\n This is showcased by the [`.get_mut()`] accessor, which is a _safe_ getter that yields\n a `&mut T`.\n\n [`.get_mut()`]: `UnsafeCell::get_mut`\n\n # Memory layout\n\n `UnsafeCell<T>` has the same in-memory representation as its inner type `T`. A consequence\n of this guarantee is that it is possible to convert between `T` and `UnsafeCell<T>`.\n Special care has to be taken when converting a nested `T` inside of an `Outer<T>` type\n to an `Outer<UnsafeCell<T>>` type: this is not sound when the `Outer<T>` type enables [niche]\n optimizations. For example, the type `Option<NonNull<u8>>` is typically 8 bytes large on\n 64-bit platforms, but the type `Option<UnsafeCell<NonNull<u8>>>` takes up 16 bytes of space.\n Therefore this is not a valid conversion, despite `NonNull<u8>` and `UnsafeCell<NonNull<u8>>>`\n having the same memory layout. This is because `UnsafeCell` disables niche optimizations in\n order to avoid its interior mutability property from spreading from `T` into the `Outer` type,\n thus this can cause distortions in the type size in these cases.\n\n Note that the only valid way to obtain a `*mut T` pointer to the contents of a\n _shared_ `UnsafeCell<T>` is through [`.get()`]  or [`.raw_get()`]. A `&mut T` reference\n can be obtained by either dereferencing this pointer or by calling [`.get_mut()`]\n on an _exclusive_ `UnsafeCell<T>`. Even though `T` and `UnsafeCell<T>` have the\n same memory layout, the following is not allowed and undefined behavior:\n\n ```rust,compile_fail\n # use std::cell::UnsafeCell;\n unsafe fn not_allowed<T>(ptr: &UnsafeCell<T>) -> &mut T {\n   let t = ptr as *const UnsafeCell<T> as *mut T;\n   // This is undefined behavior, because the `*mut T` pointer\n   // was not obtained through `.get()` nor `.raw_get()`:\n   unsafe { &mut *t }\n }\n ```\n\n Instead, do this:\n\n ```rust\n # use std::cell::UnsafeCell;\n // Safety: the caller must ensure that there are no references that\n // point to the *contents* of the `UnsafeCell`.\n unsafe fn get_mut<T>(ptr: &UnsafeCell<T>) -> &mut T {\n   unsafe { &mut *ptr.get() }\n }\n ```\n\n Converting in the other direction from a `&mut T`\n to an `&UnsafeCell<T>` is allowed:\n\n ```rust\n # use std::cell::UnsafeCell;\n fn get_shared<T>(ptr: &mut T) -> &UnsafeCell<T> {\n   let t = ptr as *mut T as *const UnsafeCell<T>;\n   // SAFETY: `T` and `UnsafeCell<T>` have the same memory layout\n   unsafe { &*t }\n }\n ```\n\n [niche]: https://rust-lang.github.io/unsafe-code-guidelines/glossary.html#niche\n [`.raw_get()`]: `UnsafeCell::raw_get`\n\n # Examples\n\n Here is an example showcasing how to soundly mutate the contents of an `UnsafeCell<_>` despite\n there being multiple references aliasing the cell:\n\n ```\n use std::cell::UnsafeCell;\n\n let x: UnsafeCell<i32> = 42.into();\n // Get multiple / concurrent / shared references to the same `x`.\n let (p1, p2): (&UnsafeCell<i32>, &UnsafeCell<i32>) = (&x, &x);\n\n unsafe {\n     // SAFETY: within this scope there are no other references to `x`'s contents,\n     // so ours is effectively unique.\n     let p1_exclusive: &mut i32 = &mut *p1.get(); // -- borrow --+\n     *p1_exclusive += 27; //                                     |\n } // <---------- cannot go beyond this point -------------------+\n\n unsafe {\n     // SAFETY: within this scope nobody expects to have exclusive access to `x`'s contents,\n     // so we can have multiple shared accesses concurrently.\n     let p2_shared: &i32 = &*p2.get();\n     assert_eq!(*p2_shared, 42 + 27);\n     let p1_shared: &i32 = &*p1.get();\n     assert_eq!(*p1_shared, *p2_shared);\n }\n ```\n\n The following example showcases the fact that exclusive access to an `UnsafeCell<T>`\n implies exclusive access to its `T`:\n\n ```rust\n #![forbid(unsafe_code)]\n // with exclusive accesses, `UnsafeCell` is a transparent no-op wrapper, so no need for\n // `unsafe` here.\n use std::cell::UnsafeCell;\n\n let mut x: UnsafeCell<i32> = 42.into();\n\n // Get a compile-time-checked unique reference to `x`.\n let p_unique: &mut UnsafeCell<i32> = &mut x;\n // With an exclusive reference, we can mutate the contents for free.\n *p_unique.get_mut() = 0;\n // Or, equivalently:\n x = UnsafeCell::new(0);\n\n // When we own the value, we can extract the contents for free.\n let contents: i32 = x.into_inner();\n assert_eq!(contents, 0);\n ```\n",
  "variant_fields": {
    "VariantIdx(None)-FieldIdx(Some(0))": {
      "name": "value",
      "doc": ""
    }
  }
}