{
  "name": "core_arch::x86::sse::_mm_prefetch",
  "safe": false,
  "callees": {
    "core_arch::x86::sse::prefetch": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "",
      "adt": {}
    }
  },
  "adts": {},
  "path": {
    "type": "Local",
    "path": "core::core_arch::x86::sse::_mm_prefetch"
  },
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/../../stdarch/crates/core_arch/src/x86/sse.rs:1902:1: 1909:2",
  "src": "pub fn _mm_prefetch<const STRATEGY: i32>(p: *const i8) {\n    static_assert_uimm_bits!(STRATEGY, 3);\n    // We use the `llvm.prefetch` intrinsic with `cache type` = 1 (data cache).\n    // `locality` and `rw` are based on our `STRATEGY`.\n    unsafe {\n        prefetch(p, (STRATEGY >> 2) & 1, STRATEGY & 3, 1);\n    }\n}",
  "mir": "fn core_arch::x86::sse::_mm_prefetch(_1: *const i8) -> () {\n    let mut _0: ();\n    let  _2: ();\n    let mut _3: i32;\n    let mut _4: i32;\n    let mut _5: u32;\n    let mut _6: bool;\n    let mut _7: i32;\n    debug p => _1;\n    bb0: {\n        StorageLive(_3);\n        StorageLive(_4);\n        _5 = 2_i32 as u32;\n        _6 = Lt(move _5, 32_u32);\n        assert(move _6, \"attempt to shift right by `{}`, which would overflow\", 2_i32) -> [success: bb1, unwind unreachable];\n    }\n    bb1: {\n        _4 = Shr(STRATEGY, 2_i32);\n        _3 = BitAnd(move _4, 1_i32);\n        StorageDead(_4);\n        StorageLive(_7);\n        _7 = BitAnd(STRATEGY, 3_i32);\n        _2 = core_arch::x86::sse::prefetch(_1, move _3, move _7, 1_i32) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        StorageDead(_7);\n        StorageDead(_3);\n        return;\n    }\n}\n",
  "doc": " Fetch the cache line that contains address `p` using the given `STRATEGY`.\n\n The `STRATEGY` must be one of:\n\n * [`_MM_HINT_T0`](constant._MM_HINT_T0.html): Fetch into all levels of the\n   cache hierarchy.\n\n * [`_MM_HINT_T1`](constant._MM_HINT_T1.html): Fetch into L2 and higher.\n\n * [`_MM_HINT_T2`](constant._MM_HINT_T2.html): Fetch into L3 and higher or\n   an implementation-specific choice (e.g., L2 if there is no L3).\n\n * [`_MM_HINT_NTA`](constant._MM_HINT_NTA.html): Fetch data using the\n   non-temporal access (NTA) hint. It may be a place closer than main memory\n   but outside of the cache hierarchy. This is used to reduce access latency\n   without polluting the cache.\n\n * [`_MM_HINT_ET0`](constant._MM_HINT_ET0.html) and\n   [`_MM_HINT_ET1`](constant._MM_HINT_ET1.html) are similar to `_MM_HINT_T0`\n   and `_MM_HINT_T1` but indicate an anticipation to write to the address.\n\n The actual implementation depends on the particular CPU. This instruction\n is considered a hint, so the CPU is also free to simply ignore the request.\n\n The amount of prefetched data depends on the cache line size of the\n specific CPU, but it will be at least 32 bytes.\n\n Common caveats:\n\n * Most modern CPUs already automatically prefetch data based on predicted\n   access patterns.\n\n * Data is usually not fetched if this would cause a TLB miss or a page\n   fault.\n\n * Too much prefetching can cause unnecessary cache evictions.\n\n * Prefetching may also fail if there are not enough memory-subsystem\n   resources (e.g., request buffers).\n\n Note: this intrinsic is safe to use even though it takes a raw pointer argument. In general, this\n cannot change the behavior of the program, including not trapping on invalid pointers.\n\n [Intel's documentation](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_prefetch)\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}