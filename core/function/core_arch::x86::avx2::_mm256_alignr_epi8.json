{
  "name": "core_arch::x86::avx2::_mm256_alignr_epi8",
  "safe": false,
  "callees": {
    "core_arch::x86::avx::_mm256_setzero_si256": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns vector of type __m256i with all elements set to zero.\n\n [Intel's documentation](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm256_setzero_si256)\n",
      "adt": {
        "core_arch::x86::__m256i": "Constructor"
      }
    },
    "core_arch::x86::__m256i::as_i8x32": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "",
      "adt": {
        "core_arch::simd::i8x32": "Constructor"
      }
    },
    "intrinsics::simd::simd_shuffle": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Shuffles two vectors by const indices.\n\n `T` must be a vector.\n\n `U` must be a **const** vector of `u32`s. This means it must either refer to a named\n const or be given as an inline const expression (`const { ... }`).\n\n `V` must be a vector with the same element type as `T` and the same length as `U`.\n\n Returns a new vector such that element `i` is selected from `xy[idx[i]]`, where `xy`\n is the concatenation of `x` and `y`. It is a compile-time error if `idx[i]` is out-of-bounds\n of `xy`.\n",
      "adt": {}
    }
  },
  "adts": {
    "core_arch::x86::__m256i": [
      "Plain",
      "Unknown([Field(0, Ty { id: 17115, kind: RigidTy(Adt(AdtDef(DefId { id: 30027, name: \"core_arch::x86::__m256i\" }), GenericArgs([]))) })])",
      "Unknown([Field(1, Ty { id: 17115, kind: RigidTy(Adt(AdtDef(DefId { id: 30027, name: \"core_arch::x86::__m256i\" }), GenericArgs([]))) })])"
    ],
    "core_arch::simd::i8x32": [
      "Plain"
    ]
  },
  "path": 6045,
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/../../stdarch/crates/core_arch/src/x86/avx2.rs:169:1: 240:2",
  "src": "pub fn _mm256_alignr_epi8<const IMM8: i32>(a: __m256i, b: __m256i) -> __m256i {\n    static_assert_uimm_bits!(IMM8, 8);\n\n    // If palignr is shifting the pair of vectors more than the size of two\n    // lanes, emit zero.\n    if IMM8 >= 32 {\n        return _mm256_setzero_si256();\n    }\n    // If palignr is shifting the pair of input vectors more than one lane,\n    // but less than two lanes, convert to shifting in zeroes.\n    let (a, b) = if IMM8 > 16 {\n        (_mm256_setzero_si256(), a)\n    } else {\n        (a, b)\n    };\n    unsafe {\n        if IMM8 == 16 {\n            return transmute(a);\n        }\n    }\n    const fn mask(shift: u32, i: u32) -> u32 {\n        let shift = shift % 16;\n        let mod_i = i % 16;\n        if mod_i < (16 - shift) {\n            i + shift\n        } else {\n            i + 16 + shift\n        }\n    }\n\n    unsafe {\n        let r: i8x32 = simd_shuffle!(\n            b.as_i8x32(),\n            a.as_i8x32(),\n            [\n                mask(IMM8 as u32, 0),\n                mask(IMM8 as u32, 1),\n                mask(IMM8 as u32, 2),\n                mask(IMM8 as u32, 3),\n                mask(IMM8 as u32, 4),\n                mask(IMM8 as u32, 5),\n                mask(IMM8 as u32, 6),\n                mask(IMM8 as u32, 7),\n                mask(IMM8 as u32, 8),\n                mask(IMM8 as u32, 9),\n                mask(IMM8 as u32, 10),\n                mask(IMM8 as u32, 11),\n                mask(IMM8 as u32, 12),\n                mask(IMM8 as u32, 13),\n                mask(IMM8 as u32, 14),\n                mask(IMM8 as u32, 15),\n                mask(IMM8 as u32, 16),\n                mask(IMM8 as u32, 17),\n                mask(IMM8 as u32, 18),\n                mask(IMM8 as u32, 19),\n                mask(IMM8 as u32, 20),\n                mask(IMM8 as u32, 21),\n                mask(IMM8 as u32, 22),\n                mask(IMM8 as u32, 23),\n                mask(IMM8 as u32, 24),\n                mask(IMM8 as u32, 25),\n                mask(IMM8 as u32, 26),\n                mask(IMM8 as u32, 27),\n                mask(IMM8 as u32, 28),\n                mask(IMM8 as u32, 29),\n                mask(IMM8 as u32, 30),\n                mask(IMM8 as u32, 31),\n            ],\n        );\n        transmute(r)\n    }\n}",
  "mir": "fn core_arch::x86::avx2::_mm256_alignr_epi8(_1: core_arch::x86::__m256i, _2: core_arch::x86::__m256i) -> core_arch::x86::__m256i {\n    let mut _0: core_arch::x86::__m256i;\n    let mut _3: bool;\n    let  _4: core_arch::x86::__m256i;\n    let  _5: core_arch::x86::__m256i;\n    let mut _6: (core_arch::x86::__m256i, core_arch::x86::__m256i);\n    let mut _7: bool;\n    let mut _8: core_arch::x86::__m256i;\n    let mut _9: bool;\n    let  _10: core_arch::simd::i8x32;\n    let mut _11: core_arch::simd::i8x32;\n    let mut _12: core_arch::simd::i8x32;\n    debug a => _1;\n    debug b => _2;\n    debug a => _4;\n    debug b => _5;\n    debug r => _10;\n    bb0: {\n        StorageLive(_3);\n        _3 = Ge(IMM8, 32_i32);\n        switchInt(move _3) -> [0: bb2, otherwise: bb1];\n    }\n    bb1: {\n        _0 = core_arch::x86::avx::_mm256_setzero_si256() -> [return: bb12, unwind unreachable];\n    }\n    bb2: {\n        StorageDead(_3);\n        StorageLive(_6);\n        StorageLive(_7);\n        _7 = Gt(IMM8, 16_i32);\n        switchInt(move _7) -> [0: bb4, otherwise: bb3];\n    }\n    bb3: {\n        StorageLive(_8);\n        _8 = core_arch::x86::avx::_mm256_setzero_si256() -> [return: bb5, unwind unreachable];\n    }\n    bb4: {\n        _6 = (_1, _2);\n        goto -> bb6;\n    }\n    bb5: {\n        _6 = (move _8, _1);\n        StorageDead(_8);\n        goto -> bb6;\n    }\n    bb6: {\n        StorageDead(_7);\n        _4 = (_6.0: core_arch::x86::__m256i);\n        _5 = (_6.1: core_arch::x86::__m256i);\n        StorageDead(_6);\n        StorageLive(_9);\n        _9 = Eq(IMM8, 16_i32);\n        switchInt(move _9) -> [0: bb8, otherwise: bb7];\n    }\n    bb7: {\n        _0 = _4;\n        StorageDead(_9);\n        goto -> bb13;\n    }\n    bb8: {\n        StorageDead(_9);\n        StorageLive(_11);\n        _11 = core_arch::x86::__m256i::as_i8x32(_5) -> [return: bb9, unwind unreachable];\n    }\n    bb9: {\n        StorageLive(_12);\n        _12 = core_arch::x86::__m256i::as_i8x32(_4) -> [return: bb10, unwind unreachable];\n    }\n    bb10: {\n        _10 = intrinsics::simd::simd_shuffle::<core_arch::simd::i8x32, core_arch::macros::SimdShuffleIdx<32>, core_arch::simd::i8x32>(move _11, move _12, core_arch::x86::avx2::_mm256_alignr_epi8::<IMM8>::{constant#1}) -> [return: bb11, unwind unreachable];\n    }\n    bb11: {\n        StorageDead(_12);\n        StorageDead(_11);\n        _0 = _10 as core_arch::x86::__m256i;\n        goto -> bb13;\n    }\n    bb12: {\n        StorageDead(_3);\n        goto -> bb13;\n    }\n    bb13: {\n        return;\n    }\n}\n",
  "doc": " Concatenates pairs of 16-byte blocks in `a` and `b` into a 32-byte temporary\n result, shifts the result right by `n` bytes, and returns the low 16 bytes.\n\n [Intel's documentation](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm256_alignr_epi8)\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}