{
  "name": "slice::ascii::is_ascii",
  "safe": true,
  "callees": {
    "slice::ascii::is_ascii::runtime": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "",
      "adt": {}
    }
  },
  "adts": {},
  "path": 16305,
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/slice/ascii.rs:371:1: 464:2",
  "src": "const fn is_ascii(s: &[u8]) -> bool {\n    // The runtime version behaves the same as the compiletime version, it's\n    // just more optimized.\n    const_eval_select!(\n        @capture { s: &[u8] } -> bool:\n        if const {\n            is_ascii_simple(s)\n        } else {\n            /// Returns `true` if any byte in the word `v` is nonascii (>= 128). Snarfed\n            /// from `../str/mod.rs`, which does something similar for utf8 validation.\n            const fn contains_nonascii(v: usize) -> bool {\n                const NONASCII_MASK: usize = usize::repeat_u8(0x80);\n                (NONASCII_MASK & v) != 0\n            }\n\n            const USIZE_SIZE: usize = size_of::<usize>();\n\n            let len = s.len();\n            let align_offset = s.as_ptr().align_offset(USIZE_SIZE);\n\n            // If we wouldn't gain anything from the word-at-a-time implementation, fall\n            // back to a scalar loop.\n            //\n            // We also do this for architectures where `size_of::<usize>()` isn't\n            // sufficient alignment for `usize`, because it's a weird edge case.\n            if len < USIZE_SIZE || len < align_offset || USIZE_SIZE < align_of::<usize>() {\n                return is_ascii_simple(s);\n            }\n\n            // We always read the first word unaligned, which means `align_offset` is\n            // 0, we'd read the same value again for the aligned read.\n            let offset_to_aligned = if align_offset == 0 { USIZE_SIZE } else { align_offset };\n\n            let start = s.as_ptr();\n            // SAFETY: We verify `len < USIZE_SIZE` above.\n            let first_word = unsafe { (start as *const usize).read_unaligned() };\n\n            if contains_nonascii(first_word) {\n                return false;\n            }\n            // We checked this above, somewhat implicitly. Note that `offset_to_aligned`\n            // is either `align_offset` or `USIZE_SIZE`, both of are explicitly checked\n            // above.\n            debug_assert!(offset_to_aligned <= len);\n\n            // SAFETY: word_ptr is the (properly aligned) usize ptr we use to read the\n            // middle chunk of the slice.\n            let mut word_ptr = unsafe { start.add(offset_to_aligned) as *const usize };\n\n            // `byte_pos` is the byte index of `word_ptr`, used for loop end checks.\n            let mut byte_pos = offset_to_aligned;\n\n            // Paranoia check about alignment, since we're about to do a bunch of\n            // unaligned loads. In practice this should be impossible barring a bug in\n            // `align_offset` though.\n            // While this method is allowed to spuriously fail in CTFE, if it doesn't\n            // have alignment information it should have given a `usize::MAX` for\n            // `align_offset` earlier, sending things through the scalar path instead of\n            // this one, so this check should pass if it's reachable.\n            debug_assert!(word_ptr.is_aligned_to(align_of::<usize>()));\n\n            // Read subsequent words until the last aligned word, excluding the last\n            // aligned word by itself to be done in tail check later, to ensure that\n            // tail is always one `usize` at most to extra branch `byte_pos == len`.\n            while byte_pos < len - USIZE_SIZE {\n                // Sanity check that the read is in bounds\n                debug_assert!(byte_pos + USIZE_SIZE <= len);\n                // And that our assumptions about `byte_pos` hold.\n                debug_assert!(word_ptr.cast::<u8>() == start.wrapping_add(byte_pos));\n\n                // SAFETY: We know `word_ptr` is properly aligned (because of\n                // `align_offset`), and we know that we have enough bytes between `word_ptr` and the end\n                let word = unsafe { word_ptr.read() };\n                if contains_nonascii(word) {\n                    return false;\n                }\n\n                byte_pos += USIZE_SIZE;\n                // SAFETY: We know that `byte_pos <= len - USIZE_SIZE`, which means that\n                // after this `add`, `word_ptr` will be at most one-past-the-end.\n                word_ptr = unsafe { word_ptr.add(1) };\n            }\n\n            // Sanity check to ensure there really is only one `usize` left. This should\n            // be guaranteed by our loop condition.\n            debug_assert!(byte_pos <= len && len - byte_pos <= USIZE_SIZE);\n\n            // SAFETY: This relies on `len >= USIZE_SIZE`, which we check at the start.\n            let last_word = unsafe { (start.add(len - USIZE_SIZE) as *const usize).read_unaligned() };\n\n            !contains_nonascii(last_word)\n        }\n    )\n}",
  "mir": "fn slice::ascii::is_ascii(_1: &[u8]) -> bool {\n    let mut _0: bool;\n    let mut _2: (&[u8],);\n    debug s => _1;\n    bb0: {\n        StorageLive(_2);\n        _2 = (_1);\n        _0 = slice::ascii::is_ascii::runtime(move (_2.0: &[u8])) -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        StorageDead(_2);\n        return;\n    }\n}\n",
  "doc": " Optimized ASCII test that will use usize-at-a-time operations instead of\n byte-at-a-time operations (when possible).\n\n The algorithm we use here is pretty simple. If `s` is too short, we just\n check each byte and be done with it. Otherwise:\n\n - Read the first word with an unaligned load.\n - Align the pointer, read subsequent words until end with aligned loads.\n - Read the last `usize` from `s` with an unaligned load.\n\n If any of these loads produces something for which `contains_nonascii`\n (above) returns true, then we know the answer is false.\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}