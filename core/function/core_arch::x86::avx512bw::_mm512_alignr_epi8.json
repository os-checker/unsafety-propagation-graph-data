{
  "name": "core_arch::x86::avx512bw::_mm512_alignr_epi8",
  "safe": false,
  "callees": {
    "core_arch::x86::avx512f::_mm512_setzero_si512": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns vector of type `__m512i` with all elements set to zero.\n\n [Intel's documentation](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm512_setzero_si512&expand=5024)\n",
      "adt": {
        "core_arch::x86::__m512i": "Constructor"
      }
    },
    "core_arch::x86::__m512i::as_i8x64": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "",
      "adt": {
        "core_arch::simd::i8x64": "Constructor"
      }
    },
    "intrinsics::simd::simd_shuffle": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Shuffles two vectors by const indices.\n\n `T` must be a vector.\n\n `U` must be a **const** vector of `u32`s. This means it must either refer to a named\n const or be given as an inline const expression (`const { ... }`).\n\n `V` must be a vector with the same element type as `T` and the same length as `U`.\n\n Returns a new vector such that element `i` is selected from `xy[idx[i]]`, where `xy`\n is the concatenation of `x` and `y`. It is a compile-time error if `idx[i]` is out-of-bounds\n of `xy`.\n",
      "adt": {}
    }
  },
  "adts": {
    "core_arch::x86::__m512i": [
      "Plain",
      "Unknown([Field(0, Ty { id: 17883, kind: RigidTy(Adt(AdtDef(DefId { id: 30181, name: \"core_arch::x86::__m512i\" }), GenericArgs([]))) })])",
      "Unknown([Field(1, Ty { id: 17883, kind: RigidTy(Adt(AdtDef(DefId { id: 30181, name: \"core_arch::x86::__m512i\" }), GenericArgs([]))) })])"
    ],
    "core_arch::simd::i8x64": [
      "Plain"
    ]
  },
  "path": 6590,
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/../../stdarch/crates/core_arch/src/x86/avx512bw.rs:11373:1: 11473:2",
  "src": "pub fn _mm512_alignr_epi8<const IMM8: i32>(a: __m512i, b: __m512i) -> __m512i {\n    const fn mask(shift: u32, i: u32) -> u32 {\n        let shift = shift % 16;\n        let mod_i = i % 16;\n        if mod_i < (16 - shift) {\n            i + shift\n        } else {\n            i + 48 + shift\n        }\n    }\n\n    // If palignr is shifting the pair of vectors more than the size of two\n    // lanes, emit zero.\n    if IMM8 >= 32 {\n        return _mm512_setzero_si512();\n    }\n    // If palignr is shifting the pair of input vectors more than one lane,\n    // but less than two lanes, convert to shifting in zeroes.\n    let (a, b) = if IMM8 > 16 {\n        (_mm512_setzero_si512(), a)\n    } else {\n        (a, b)\n    };\n    unsafe {\n        if IMM8 == 16 {\n            return transmute(a);\n        }\n\n        let r: i8x64 = simd_shuffle!(\n            b.as_i8x64(),\n            a.as_i8x64(),\n            [\n                mask(IMM8 as u32, 0),\n                mask(IMM8 as u32, 1),\n                mask(IMM8 as u32, 2),\n                mask(IMM8 as u32, 3),\n                mask(IMM8 as u32, 4),\n                mask(IMM8 as u32, 5),\n                mask(IMM8 as u32, 6),\n                mask(IMM8 as u32, 7),\n                mask(IMM8 as u32, 8),\n                mask(IMM8 as u32, 9),\n                mask(IMM8 as u32, 10),\n                mask(IMM8 as u32, 11),\n                mask(IMM8 as u32, 12),\n                mask(IMM8 as u32, 13),\n                mask(IMM8 as u32, 14),\n                mask(IMM8 as u32, 15),\n                mask(IMM8 as u32, 16),\n                mask(IMM8 as u32, 17),\n                mask(IMM8 as u32, 18),\n                mask(IMM8 as u32, 19),\n                mask(IMM8 as u32, 20),\n                mask(IMM8 as u32, 21),\n                mask(IMM8 as u32, 22),\n                mask(IMM8 as u32, 23),\n                mask(IMM8 as u32, 24),\n                mask(IMM8 as u32, 25),\n                mask(IMM8 as u32, 26),\n                mask(IMM8 as u32, 27),\n                mask(IMM8 as u32, 28),\n                mask(IMM8 as u32, 29),\n                mask(IMM8 as u32, 30),\n                mask(IMM8 as u32, 31),\n                mask(IMM8 as u32, 32),\n                mask(IMM8 as u32, 33),\n                mask(IMM8 as u32, 34),\n                mask(IMM8 as u32, 35),\n                mask(IMM8 as u32, 36),\n                mask(IMM8 as u32, 37),\n                mask(IMM8 as u32, 38),\n                mask(IMM8 as u32, 39),\n                mask(IMM8 as u32, 40),\n                mask(IMM8 as u32, 41),\n                mask(IMM8 as u32, 42),\n                mask(IMM8 as u32, 43),\n                mask(IMM8 as u32, 44),\n                mask(IMM8 as u32, 45),\n                mask(IMM8 as u32, 46),\n                mask(IMM8 as u32, 47),\n                mask(IMM8 as u32, 48),\n                mask(IMM8 as u32, 49),\n                mask(IMM8 as u32, 50),\n                mask(IMM8 as u32, 51),\n                mask(IMM8 as u32, 52),\n                mask(IMM8 as u32, 53),\n                mask(IMM8 as u32, 54),\n                mask(IMM8 as u32, 55),\n                mask(IMM8 as u32, 56),\n                mask(IMM8 as u32, 57),\n                mask(IMM8 as u32, 58),\n                mask(IMM8 as u32, 59),\n                mask(IMM8 as u32, 60),\n                mask(IMM8 as u32, 61),\n                mask(IMM8 as u32, 62),\n                mask(IMM8 as u32, 63),\n            ],\n        );\n        transmute(r)\n    }\n}",
  "mir": "fn core_arch::x86::avx512bw::_mm512_alignr_epi8(_1: core_arch::x86::__m512i, _2: core_arch::x86::__m512i) -> core_arch::x86::__m512i {\n    let mut _0: core_arch::x86::__m512i;\n    let mut _3: bool;\n    let  _4: core_arch::x86::__m512i;\n    let  _5: core_arch::x86::__m512i;\n    let mut _6: (core_arch::x86::__m512i, core_arch::x86::__m512i);\n    let mut _7: bool;\n    let mut _8: core_arch::x86::__m512i;\n    let mut _9: bool;\n    let  _10: core_arch::simd::i8x64;\n    let mut _11: core_arch::simd::i8x64;\n    let mut _12: core_arch::simd::i8x64;\n    debug a => _1;\n    debug b => _2;\n    debug a => _4;\n    debug b => _5;\n    debug r => _10;\n    bb0: {\n        StorageLive(_3);\n        _3 = Ge(IMM8, 32_i32);\n        switchInt(move _3) -> [0: bb2, otherwise: bb1];\n    }\n    bb1: {\n        _0 = core_arch::x86::avx512f::_mm512_setzero_si512() -> [return: bb12, unwind unreachable];\n    }\n    bb2: {\n        StorageDead(_3);\n        StorageLive(_6);\n        StorageLive(_7);\n        _7 = Gt(IMM8, 16_i32);\n        switchInt(move _7) -> [0: bb4, otherwise: bb3];\n    }\n    bb3: {\n        StorageLive(_8);\n        _8 = core_arch::x86::avx512f::_mm512_setzero_si512() -> [return: bb5, unwind unreachable];\n    }\n    bb4: {\n        _6 = (_1, _2);\n        goto -> bb6;\n    }\n    bb5: {\n        _6 = (move _8, _1);\n        StorageDead(_8);\n        goto -> bb6;\n    }\n    bb6: {\n        StorageDead(_7);\n        _4 = (_6.0: core_arch::x86::__m512i);\n        _5 = (_6.1: core_arch::x86::__m512i);\n        StorageDead(_6);\n        StorageLive(_9);\n        _9 = Eq(IMM8, 16_i32);\n        switchInt(move _9) -> [0: bb8, otherwise: bb7];\n    }\n    bb7: {\n        _0 = _4;\n        StorageDead(_9);\n        goto -> bb13;\n    }\n    bb8: {\n        StorageDead(_9);\n        StorageLive(_11);\n        _11 = core_arch::x86::__m512i::as_i8x64(_5) -> [return: bb9, unwind unreachable];\n    }\n    bb9: {\n        StorageLive(_12);\n        _12 = core_arch::x86::__m512i::as_i8x64(_4) -> [return: bb10, unwind unreachable];\n    }\n    bb10: {\n        _10 = intrinsics::simd::simd_shuffle::<core_arch::simd::i8x64, core_arch::macros::SimdShuffleIdx<64>, core_arch::simd::i8x64>(move _11, move _12, core_arch::x86::avx512bw::_mm512_alignr_epi8::<IMM8>::{constant#0}) -> [return: bb11, unwind unreachable];\n    }\n    bb11: {\n        StorageDead(_12);\n        StorageDead(_11);\n        _0 = _10 as core_arch::x86::__m512i;\n        goto -> bb13;\n    }\n    bb12: {\n        StorageDead(_3);\n        goto -> bb13;\n    }\n    bb13: {\n        return;\n    }\n}\n",
  "doc": " Concatenate pairs of 16-byte blocks in a and b into a 32-byte temporary result, shift the result right by imm8 bytes, and store the low 16 bytes in dst.\n Unlike [`_mm_alignr_epi8`], [`_mm256_alignr_epi8`] functions, where the entire input vectors are concatenated to the temporary result,\n this concatenation happens in 4 steps, where each step builds 32-byte temporary result.\n\n [Intel's documentation](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm512_alignr_epi8&expand=263)\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}