{
  "name": "sync::atomic::AtomicPtr::<T>::fetch_byte_add",
  "safe": true,
  "callees": {
    "cell::UnsafeCell::<T>::get": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Gets a mutable pointer to the wrapped value.\n\n This can be cast to a pointer of any kind. When creating references, you must uphold the\n aliasing rules; see [the type-level docs][UnsafeCell#aliasing-rules] for more discussion and\n caveats.\n\n # Examples\n\n ```\n use std::cell::UnsafeCell;\n\n let uc = UnsafeCell::new(5);\n\n let five = uc.get();\n ```\n",
      "adt": {
        "cell::UnsafeCell": "MutableAsArgument"
      }
    },
    "sync::atomic::atomic_add": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns the previous value (like __sync_fetch_and_add).\n",
      "adt": {}
    },
    "ptr::mut_ptr::<impl *mut T>::cast": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Casts to a pointer of another type.\n",
      "adt": {}
    }
  },
  "adts": {
    "cell::UnsafeCell": [
      "Ref"
    ],
    "sync::atomic::AtomicPtr": [
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))",
      "Ref"
    ],
    "sync::atomic::Ordering": [
      "Plain"
    ]
  },
  "path": {
    "type": "Local",
    "path": "core::sync::atomic::AtomicPtr::<T>::fetch_byte_add"
  },
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/sync/atomic.rs:2322:5: 2325:6",
  "src": "pub fn fetch_byte_add(&self, val: usize, order: Ordering) -> *mut T {\n        // SAFETY: data races are prevented by atomic intrinsics.\n        unsafe { atomic_add(self.p.get(), val, order).cast() }\n    }",
  "mir": "fn sync::atomic::AtomicPtr::<T>::fetch_byte_add(_1: &sync::atomic::AtomicPtr<T>, _2: usize, _3: sync::atomic::Ordering) -> *mut T {\n    let mut _0: *mut T;\n    let mut _4: *mut T;\n    let mut _5: *mut *mut T;\n    let mut _6: &cell::UnsafeCell<*mut T>;\n    debug self => _1;\n    debug val => _2;\n    debug order => _3;\n    bb0: {\n        StorageLive(_4);\n        StorageLive(_5);\n        StorageLive(_6);\n        _6 = &((*_1).0: cell::UnsafeCell<*mut T>);\n        _5 = cell::UnsafeCell::<*mut T>::get(move _6) -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        StorageDead(_6);\n        _4 = sync::atomic::atomic_add::<*mut T, usize>(move _5, _2, _3) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        StorageDead(_5);\n        _0 = ptr::mut_ptr::<impl *mut T>::cast::<T>(move _4) -> [return: bb3, unwind unreachable];\n    }\n    bb3: {\n        StorageDead(_4);\n        return;\n    }\n}\n",
  "doc": " Offsets the pointer's address by adding `val` *bytes*, returning the\n previous pointer.\n\n This is equivalent to using [`wrapping_byte_add`] to atomically\n perform `ptr = ptr.wrapping_byte_add(val)`.\n\n `fetch_byte_add` takes an [`Ordering`] argument which describes the\n memory ordering of this operation. All ordering modes are possible. Note\n that using [`Acquire`] makes the store part of this operation\n [`Relaxed`], and using [`Release`] makes the load part [`Relaxed`].\n\n **Note**: This method is only available on platforms that support atomic\n operations on [`AtomicPtr`].\n\n [`wrapping_byte_add`]: pointer::wrapping_byte_add\n\n # Examples\n\n ```\n use core::sync::atomic::{AtomicPtr, Ordering};\n\n let atom = AtomicPtr::<i64>::new(core::ptr::null_mut());\n assert_eq!(atom.fetch_byte_add(1, Ordering::Relaxed).addr(), 0);\n // Note: in units of bytes, not `size_of::<i64>()`.\n assert_eq!(atom.load(Ordering::Relaxed).addr(), 1);\n ```\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}