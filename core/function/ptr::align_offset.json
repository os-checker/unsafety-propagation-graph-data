{
  "name": "ptr::align_offset",
  "safe": false,
  "callees": {
    "mem::size_of": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns the size of a type in bytes.\n\n More specifically, this is the offset in bytes between successive elements\n in an array with that item type including alignment padding. Thus, for any\n type `T` and length `n`, `[T; n]` has a size of `n * size_of::<T>()`.\n\n In general, the size of a type is not stable across compilations, but\n specific types such as primitives are.\n\n The following table gives the size for primitives.\n\n Type | `size_of::<Type>()`\n ---- | ---------------\n () | 0\n bool | 1\n u8 | 1\n u16 | 2\n u32 | 4\n u64 | 8\n u128 | 16\n i8 | 1\n i16 | 2\n i32 | 4\n i64 | 8\n i128 | 16\n f32 | 4\n f64 | 8\n char | 4\n\n Furthermore, `usize` and `isize` have the same size.\n\n The types [`*const T`], `&T`, [`Box<T>`], [`Option<&T>`], and `Option<Box<T>>` all have\n the same size. If `T` is `Sized`, all of those types have the same size as `usize`.\n\n The mutability of a pointer does not change its size. As such, `&T` and `&mut T`\n have the same size. Likewise for `*const T` and `*mut T`.\n\n # Size of `#[repr(C)]` items\n\n The `C` representation for items has a defined layout. With this layout,\n the size of items is also stable as long as all fields have a stable size.\n\n ## Size of Structs\n\n For `struct`s, the size is determined by the following algorithm.\n\n For each field in the struct ordered by declaration order:\n\n 1. Add the size of the field.\n 2. Round up the current size to the nearest multiple of the next field's [alignment].\n\n Finally, round the size of the struct to the nearest multiple of its [alignment].\n The alignment of the struct is usually the largest alignment of all its\n fields; this can be changed with the use of `repr(align(N))`.\n\n Unlike `C`, zero sized structs are not rounded up to one byte in size.\n\n ## Size of Enums\n\n Enums that carry no data other than the discriminant have the same size as C enums\n on the platform they are compiled for.\n\n ## Size of Unions\n\n The size of a union is the size of its largest field.\n\n Unlike `C`, zero sized unions are not rounded up to one byte in size.\n\n # Examples\n\n ```\n // Some primitives\n assert_eq!(4, size_of::<i32>());\n assert_eq!(8, size_of::<f64>());\n assert_eq!(0, size_of::<()>());\n\n // Some arrays\n assert_eq!(8, size_of::<[i32; 2]>());\n assert_eq!(12, size_of::<[i32; 3]>());\n assert_eq!(0, size_of::<[i32; 0]>());\n\n\n // Pointer size equality\n assert_eq!(size_of::<&i32>(), size_of::<*const i32>());\n assert_eq!(size_of::<&i32>(), size_of::<Box<i32>>());\n assert_eq!(size_of::<&i32>(), size_of::<Option<&i32>>());\n assert_eq!(size_of::<Box<i32>>(), size_of::<Option<Box<i32>>>());\n ```\n\n Using `#[repr(C)]`.\n\n ```\n #[repr(C)]\n struct FieldStruct {\n     first: u8,\n     second: u16,\n     third: u8\n }\n\n // The size of the first field is 1, so add 1 to the size. Size is 1.\n // The alignment of the second field is 2, so add 1 to the size for padding. Size is 2.\n // The size of the second field is 2, so add 2 to the size. Size is 4.\n // The alignment of the third field is 1, so add 0 to the size for padding. Size is 4.\n // The size of the third field is 1, so add 1 to the size. Size is 5.\n // Finally, the alignment of the struct is 2 (because the largest alignment amongst its\n // fields is 2), so add 1 to the size for padding. Size is 6.\n assert_eq!(6, size_of::<FieldStruct>());\n\n #[repr(C)]\n struct TupleStruct(u8, u16, u8);\n\n // Tuple structs follow the same rules.\n assert_eq!(6, size_of::<TupleStruct>());\n\n // Note that reordering the fields can lower the size. We can remove both padding bytes\n // by putting `third` before `second`.\n #[repr(C)]\n struct FieldStructOptimized {\n     first: u8,\n     third: u8,\n     second: u16\n }\n\n assert_eq!(4, size_of::<FieldStructOptimized>());\n\n // Union size is the size of the largest field.\n #[repr(C)]\n union ExampleUnion {\n     smaller: u8,\n     larger: u16\n }\n\n assert_eq!(2, size_of::<ExampleUnion>());\n ```\n\n [alignment]: align_of\n [`*const T`]: primitive@pointer\n [`Box<T>`]: ../../std/boxed/struct.Box.html\n [`Option<&T>`]: crate::option::Option\n\n",
      "adt": {}
    },
    "ptr::const_ptr::<impl *const T>::addr": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "",
      "adt": {}
    },
    "intrinsics::exact_div": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Performs an exact division, resulting in undefined behavior where\n `x % y != 0` or `y == 0` or `x == T::MIN && y == -1`\n\n This intrinsic does not have a stable counterpart.\n",
      "adt": {}
    },
    "intrinsics::cttz_nonzero": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Like `cttz`, but extra-unsafe as it returns `undef` when\n given an `x` with value `0`.\n\n This intrinsic does not have a stable counterpart.\n\n # Examples\n\n ```\n #![feature(core_intrinsics)]\n # #![allow(internal_features)]\n\n use std::intrinsics::cttz_nonzero;\n\n let x = 0b0011_1000_u8;\n let num_trailing = unsafe { cttz_nonzero(x) };\n assert_eq!(num_trailing, 3);\n ```\n",
      "adt": {}
    },
    "ptr::align_offset::mod_inv": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Calculate multiplicative modular inverse of `x` modulo `m`.\n\n This implementation is tailored for `align_offset` and has following preconditions:\n\n * `m` is a power-of-two;\n * `x < m`; (if `x ≥ m`, pass in `x % m` instead)\n\n Implementation of this function shall not panic. Ever.\n",
      "adt": {}
    }
  },
  "adts": {},
  "path": {
    "type": "Local",
    "path": "core::ptr::align_offset"
  },
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ptr/mod.rs:2191:1: 2361:2",
  "src": "pub(crate) unsafe fn align_offset<T: Sized>(p: *const T, a: usize) -> usize {\n    // FIXME(#75598): Direct use of these intrinsics improves codegen significantly at opt-level <=\n    // 1, where the method versions of these operations are not inlined.\n    use intrinsics::{\n        assume, cttz_nonzero, exact_div, mul_with_overflow, unchecked_rem, unchecked_shl,\n        unchecked_shr, unchecked_sub, wrapping_add, wrapping_mul, wrapping_sub,\n    };\n\n    /// Calculate multiplicative modular inverse of `x` modulo `m`.\n    ///\n    /// This implementation is tailored for `align_offset` and has following preconditions:\n    ///\n    /// * `m` is a power-of-two;\n    /// * `x < m`; (if `x ≥ m`, pass in `x % m` instead)\n    ///\n    /// Implementation of this function shall not panic. Ever.\n    #[inline]\n    const unsafe fn mod_inv(x: usize, m: usize) -> usize {\n        /// Multiplicative modular inverse table modulo 2⁴ = 16.\n        ///\n        /// Note, that this table does not contain values where inverse does not exist (i.e., for\n        /// `0⁻¹ mod 16`, `2⁻¹ mod 16`, etc.)\n        const INV_TABLE_MOD_16: [u8; 8] = [1, 11, 13, 7, 9, 3, 5, 15];\n        /// Modulo for which the `INV_TABLE_MOD_16` is intended.\n        const INV_TABLE_MOD: usize = 16;\n\n        // SAFETY: `m` is required to be a power-of-two, hence non-zero.\n        let m_minus_one = unsafe { unchecked_sub(m, 1) };\n        let mut inverse = INV_TABLE_MOD_16[(x & (INV_TABLE_MOD - 1)) >> 1] as usize;\n        let mut mod_gate = INV_TABLE_MOD;\n        // We iterate \"up\" using the following formula:\n        //\n        // $$ xy ≡ 1 (mod 2ⁿ) → xy (2 - xy) ≡ 1 (mod 2²ⁿ) $$\n        //\n        // This application needs to be applied at least until `2²ⁿ ≥ m`, at which point we can\n        // finally reduce the computation to our desired `m` by taking `inverse mod m`.\n        //\n        // This computation is `O(log log m)`, which is to say, that on 64-bit machines this loop\n        // will always finish in at most 4 iterations.\n        loop {\n            // y = y * (2 - xy) mod n\n            //\n            // Note, that we use wrapping operations here intentionally – the original formula\n            // uses e.g., subtraction `mod n`. It is entirely fine to do them `mod\n            // usize::MAX` instead, because we take the result `mod n` at the end\n            // anyway.\n            if mod_gate >= m {\n                break;\n            }\n            inverse = wrapping_mul(inverse, wrapping_sub(2usize, wrapping_mul(x, inverse)));\n            let (new_gate, overflow) = mul_with_overflow(mod_gate, mod_gate);\n            if overflow {\n                break;\n            }\n            mod_gate = new_gate;\n        }\n        inverse & m_minus_one\n    }\n\n    let stride = size_of::<T>();\n\n    let addr: usize = p.addr();\n\n    // SAFETY: `a` is a power-of-two, therefore non-zero.\n    let a_minus_one = unsafe { unchecked_sub(a, 1) };\n\n    if stride == 0 {\n        // SPECIAL_CASE: handle 0-sized types. No matter how many times we step, the address will\n        // stay the same, so no offset will be able to align the pointer unless it is already\n        // aligned. This branch _will_ be optimized out as `stride` is known at compile-time.\n        let p_mod_a = addr & a_minus_one;\n        return if p_mod_a == 0 { 0 } else { usize::MAX };\n    }\n\n    // SAFETY: `stride == 0` case has been handled by the special case above.\n    let a_mod_stride = unsafe { unchecked_rem(a, stride) };\n    if a_mod_stride == 0 {\n        // SPECIAL_CASE: In cases where the `a` is divisible by `stride`, byte offset to align a\n        // pointer can be computed more simply through `-p (mod a)`. In the off-chance the byte\n        // offset is not a multiple of `stride`, the input pointer was misaligned and no pointer\n        // offset will be able to produce a `p` aligned to the specified `a`.\n        //\n        // The naive `-p (mod a)` equation inhibits LLVM's ability to select instructions\n        // like `lea`. We compute `(round_up_to_next_alignment(p, a) - p)` instead. This\n        // redistributes operations around the load-bearing, but pessimizing `and` instruction\n        // sufficiently for LLVM to be able to utilize the various optimizations it knows about.\n        //\n        // LLVM handles the branch here particularly nicely. If this branch needs to be evaluated\n        // at runtime, it will produce a mask `if addr_mod_stride == 0 { 0 } else { usize::MAX }`\n        // in a branch-free way and then bitwise-OR it with whatever result the `-p mod a`\n        // computation produces.\n\n        let aligned_address = wrapping_add(addr, a_minus_one) & wrapping_sub(0, a);\n        let byte_offset = wrapping_sub(aligned_address, addr);\n        // FIXME: Remove the assume after <https://github.com/llvm/llvm-project/issues/62502>\n        // SAFETY: Masking by `-a` can only affect the low bits, and thus cannot have reduced\n        // the value by more than `a-1`, so even though the intermediate values might have\n        // wrapped, the byte_offset is always in `[0, a)`.\n        unsafe { assume(byte_offset < a) };\n\n        // SAFETY: `stride == 0` case has been handled by the special case above.\n        let addr_mod_stride = unsafe { unchecked_rem(addr, stride) };\n\n        return if addr_mod_stride == 0 {\n            // SAFETY: `stride` is non-zero. This is guaranteed to divide exactly as well, because\n            // addr has been verified to be aligned to the original type’s alignment requirements.\n            unsafe { exact_div(byte_offset, stride) }\n        } else {\n            usize::MAX\n        };\n    }\n\n    // GENERAL_CASE: From here on we’re handling the very general case where `addr` may be\n    // misaligned, there isn’t an obvious relationship between `stride` and `a` that we can take an\n    // advantage of, etc. This case produces machine code that isn’t particularly high quality,\n    // compared to the special cases above. The code produced here is still within the realm of\n    // miracles, given the situations this case has to deal with.\n\n    // SAFETY: a is power-of-two hence non-zero. stride == 0 case is handled above.\n    // FIXME(const-hack) replace with min\n    let gcdpow = unsafe {\n        let x = cttz_nonzero(stride);\n        let y = cttz_nonzero(a);\n        if x < y { x } else { y }\n    };\n    // SAFETY: gcdpow has an upper-bound that’s at most the number of bits in a `usize`.\n    let gcd = unsafe { unchecked_shl(1usize, gcdpow) };\n    // SAFETY: gcd is always greater or equal to 1.\n    if addr & unsafe { unchecked_sub(gcd, 1) } == 0 {\n        // This branch solves for the following linear congruence equation:\n        //\n        // ` p + so = 0 mod a `\n        //\n        // `p` here is the pointer value, `s` - stride of `T`, `o` offset in `T`s, and `a` - the\n        // requested alignment.\n        //\n        // With `g = gcd(a, s)`, and the above condition asserting that `p` is also divisible by\n        // `g`, we can denote `a' = a/g`, `s' = s/g`, `p' = p/g`, then this becomes equivalent to:\n        //\n        // ` p' + s'o = 0 mod a' `\n        // ` o = (a' - (p' mod a')) * (s'^-1 mod a') `\n        //\n        // The first term is \"the relative alignment of `p` to `a`\" (divided by the `g`), the\n        // second term is \"how does incrementing `p` by `s` bytes change the relative alignment of\n        // `p`\" (again divided by `g`). Division by `g` is necessary to make the inverse well\n        // formed if `a` and `s` are not co-prime.\n        //\n        // Furthermore, the result produced by this solution is not \"minimal\", so it is necessary\n        // to take the result `o mod lcm(s, a)`. This `lcm(s, a)` is the same as `a'`.\n\n        // SAFETY: `gcdpow` has an upper-bound not greater than the number of trailing 0-bits in\n        // `a`.\n        let a2 = unsafe { unchecked_shr(a, gcdpow) };\n        // SAFETY: `a2` is non-zero. Shifting `a` by `gcdpow` cannot shift out any of the set bits\n        // in `a` (of which it has exactly one).\n        let a2minus1 = unsafe { unchecked_sub(a2, 1) };\n        // SAFETY: `gcdpow` has an upper-bound not greater than the number of trailing 0-bits in\n        // `a`.\n        let s2 = unsafe { unchecked_shr(stride & a_minus_one, gcdpow) };\n        // SAFETY: `gcdpow` has an upper-bound not greater than the number of trailing 0-bits in\n        // `a`. Furthermore, the subtraction cannot overflow, because `a2 = a >> gcdpow` will\n        // always be strictly greater than `(p % a) >> gcdpow`.\n        let minusp2 = unsafe { unchecked_sub(a2, unchecked_shr(addr & a_minus_one, gcdpow)) };\n        // SAFETY: `a2` is a power-of-two, as proven above. `s2` is strictly less than `a2`\n        // because `(s % a) >> gcdpow` is strictly less than `a >> gcdpow`.\n        return wrapping_mul(minusp2, unsafe { mod_inv(s2, a2) }) & a2minus1;\n    }\n\n    // Cannot be aligned at all.\n    usize::MAX\n}",
  "mir": "fn ptr::align_offset(_1: *const T, _2: usize) -> usize {\n    let mut _0: usize;\n    let  _3: usize;\n    let  _4: usize;\n    let  _5: usize;\n    let  _6: usize;\n    let  _7: usize;\n    let  _8: usize;\n    let mut _9: usize;\n    let mut _10: usize;\n    let  _11: usize;\n    let mut _12: bool;\n    let  _13: usize;\n    let  _14: u32;\n    let  _15: u32;\n    let  _16: u32;\n    let mut _17: bool;\n    let  _18: usize;\n    let mut _19: u32;\n    let mut _20: usize;\n    let mut _21: usize;\n    let  _22: usize;\n    let mut _23: u32;\n    let  _24: usize;\n    let  _25: usize;\n    let mut _26: usize;\n    let mut _27: u32;\n    let  _28: usize;\n    let mut _29: usize;\n    let mut _30: usize;\n    let mut _31: u32;\n    let mut _32: usize;\n    let mut _33: usize;\n    debug p => _1;\n    debug a => _2;\n    debug stride => _3;\n    debug addr => _4;\n    debug a_minus_one => _5;\n    debug p_mod_a => _6;\n    debug a_mod_stride => _7;\n    debug aligned_address => _8;\n    debug byte_offset => _11;\n    debug addr_mod_stride => _13;\n    debug gcdpow => _14;\n    debug x => _15;\n    debug y => _16;\n    debug gcd => _18;\n    debug a2 => _22;\n    debug a2minus1 => _24;\n    debug s2 => _25;\n    debug minusp2 => _28;\n    bb0: {\n        _3 = mem::size_of::<T>() -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        _4 = ptr::const_ptr::<impl *const T>::addr(_1) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        _5 = SubUnchecked(_2, 1_usize);\n        switchInt(_3) -> [0: bb3, otherwise: bb7];\n    }\n    bb3: {\n        _6 = BitAnd(_4, _5);\n        switchInt(_6) -> [0: bb4, otherwise: bb5];\n    }\n    bb4: {\n        _0 = 0_usize;\n        goto -> bb6;\n    }\n    bb5: {\n        _0 = num::<impl usize>::MAX;\n        goto -> bb6;\n    }\n    bb6: {\n        goto -> bb23;\n    }\n    bb7: {\n        _7 = Rem(_2, _3);\n        switchInt(_7) -> [0: bb8, otherwise: bb13];\n    }\n    bb8: {\n        StorageLive(_9);\n        _9 = Add(_4, _5);\n        StorageLive(_10);\n        _10 = Sub(0_usize, _2);\n        _8 = BitAnd(move _9, move _10);\n        StorageDead(_10);\n        StorageDead(_9);\n        _11 = Sub(_8, _4);\n        StorageLive(_12);\n        _12 = Lt(_11, _2);\n        Intrinsic(Assume(Move(_12)));\n        StorageDead(_12);\n        _13 = Rem(_4, _3);\n        switchInt(_13) -> [0: bb9, otherwise: bb11];\n    }\n    bb9: {\n        _0 = intrinsics::exact_div::<usize>(_11, _3) -> [return: bb10, unwind unreachable];\n    }\n    bb10: {\n        goto -> bb12;\n    }\n    bb11: {\n        _0 = num::<impl usize>::MAX;\n        goto -> bb12;\n    }\n    bb12: {\n        goto -> bb22;\n    }\n    bb13: {\n        StorageLive(_14);\n        _15 = intrinsics::cttz_nonzero::<usize>(_3) -> [return: bb14, unwind unreachable];\n    }\n    bb14: {\n        _16 = intrinsics::cttz_nonzero::<usize>(_2) -> [return: bb15, unwind unreachable];\n    }\n    bb15: {\n        StorageLive(_17);\n        _17 = Lt(_15, _16);\n        switchInt(move _17) -> [0: bb17, otherwise: bb16];\n    }\n    bb16: {\n        _14 = _15;\n        goto -> bb18;\n    }\n    bb17: {\n        _14 = _16;\n        goto -> bb18;\n    }\n    bb18: {\n        StorageDead(_17);\n        StorageLive(_19);\n        _19 = _14;\n        _18 = ShlUnchecked(1_usize, move _19);\n        StorageDead(_19);\n        StorageLive(_20);\n        StorageLive(_21);\n        _21 = SubUnchecked(_18, 1_usize);\n        _20 = BitAnd(_4, move _21);\n        StorageDead(_21);\n        switchInt(move _20) -> [0: bb19, otherwise: bb21];\n    }\n    bb19: {\n        StorageDead(_20);\n        StorageLive(_23);\n        _23 = _14;\n        _22 = ShrUnchecked(_2, move _23);\n        StorageDead(_23);\n        _24 = SubUnchecked(_22, 1_usize);\n        StorageLive(_26);\n        _26 = BitAnd(_3, _5);\n        StorageLive(_27);\n        _27 = _14;\n        _25 = ShrUnchecked(move _26, move _27);\n        StorageDead(_27);\n        StorageDead(_26);\n        StorageLive(_29);\n        StorageLive(_30);\n        _30 = BitAnd(_4, _5);\n        StorageLive(_31);\n        _31 = _14;\n        _29 = ShrUnchecked(move _30, move _31);\n        StorageDead(_31);\n        StorageDead(_30);\n        _28 = SubUnchecked(_22, move _29);\n        StorageDead(_29);\n        StorageLive(_32);\n        StorageLive(_33);\n        _33 = ptr::align_offset::mod_inv(_25, _22) -> [return: bb20, unwind unreachable];\n    }\n    bb20: {\n        _32 = Mul(_28, move _33);\n        StorageDead(_33);\n        _0 = BitAnd(move _32, _24);\n        StorageDead(_32);\n        StorageDead(_14);\n        goto -> bb22;\n    }\n    bb21: {\n        StorageDead(_20);\n        _0 = num::<impl usize>::MAX;\n        StorageDead(_14);\n        goto -> bb24;\n    }\n    bb22: {\n        goto -> bb23;\n    }\n    bb23: {\n        goto -> bb24;\n    }\n    bb24: {\n        return;\n    }\n}\n",
  "doc": " Calculate an element-offset that increases a pointer's alignment.\n\n Calculate an element-offset (not byte-offset) that when added to a given pointer `p`, increases `p`'s alignment to at least the given alignment `a`.\n\n # Safety\n `a` must be a power of two.\n\n # Notes\n This implementation has been carefully tailored to not panic. It is UB for this to panic.\n The only real change that can be made here is change of `INV_TABLE_MOD_16` and associated\n constants.\n\n If we ever decide to make it possible to call the intrinsic with `a` that is not a\n power-of-two, it will probably be more prudent to just change to a naive implementation rather\n than trying to adapt this to accommodate that change.\n\n Any questions go to @nagisa.\n",
  "tags": {
    "tags": [
      {
        "tag": {
          "typ": null,
          "name": "ptr_to_integer_transmute_in_consts"
        },
        "args": []
      }
    ],
    "spec": {},
    "docs": [
      "* ptr_to_integer_transmute_in_consts\n"
    ]
  }
}