{
  "name": "core_arch::x86::avx512f::_mm256_mask_andnot_epi64",
  "safe": false,
  "callees": {
    "core_arch::x86::avx::_mm256_set1_epi64x": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Broadcasts 64-bit integer `a` to all elements of returned vector.\n This intrinsic may generate the `vpbroadcastq`.\n\n [Intel's documentation](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm256_set1_epi64x)\n",
      "adt": {
        "core_arch::x86::__m256i": "Constructor"
      }
    },
    "core_arch::x86::avx512f::_mm256_xor_epi64": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Compute the bitwise XOR of packed 64-bit integers in a and b, and store the results in dst.\n\n [Intel's documentation](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm256_xor_epi64&expand=6148)\n",
      "adt": {
        "core_arch::x86::__m256i": "Constructor"
      }
    },
    "core_arch::x86::__m256i::as_i64x4": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "",
      "adt": {
        "core_arch::simd::i64x4": "Constructor"
      }
    },
    "intrinsics::simd::simd_and": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " \"And\"s vectors elementwise.\n\n `T` must be a vector of integers.\n",
      "adt": {}
    },
    "intrinsics::simd::simd_select_bitmask": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Selects elements from a bitmask.\n\n `M` must be an unsigned integer or array of `u8`, matching `simd_bitmask`.\n\n `T` must be a vector.\n\n For each element, if the bit in `mask` is `1`, select the element from\n `if_true`.  If the corresponding bit in `mask` is `0`, select the element from\n `if_false`.\n The remaining bits of the mask are ignored.\n\n The bitmask bit order matches `simd_bitmask`.\n",
      "adt": {}
    }
  },
  "adts": {
    "core_arch::x86::__m256i": [
      "Plain"
    ],
    "core_arch::simd::i64x4": [
      "Plain"
    ]
  },
  "path": 7682,
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/../../stdarch/crates/core_arch/src/x86/avx512f.rs:28948:1: 28954:2",
  "src": "pub fn _mm256_mask_andnot_epi64(src: __m256i, k: __mmask8, a: __m256i, b: __m256i) -> __m256i {\n    unsafe {\n        let not = _mm256_xor_epi64(a, _mm256_set1_epi64x(u64::MAX as i64));\n        let andnot = simd_and(not.as_i64x4(), b.as_i64x4());\n        transmute(simd_select_bitmask(k, andnot, src.as_i64x4()))\n    }\n}",
  "mir": "fn core_arch::x86::avx512f::_mm256_mask_andnot_epi64(_1: core_arch::x86::__m256i, _2: u8, _3: core_arch::x86::__m256i, _4: core_arch::x86::__m256i) -> core_arch::x86::__m256i {\n    let mut _0: core_arch::x86::__m256i;\n    let  _5: core_arch::x86::__m256i;\n    let mut _6: core_arch::x86::__m256i;\n    let mut _7: i64;\n    let  _8: core_arch::simd::i64x4;\n    let mut _9: core_arch::simd::i64x4;\n    let mut _10: core_arch::simd::i64x4;\n    let mut _11: core_arch::simd::i64x4;\n    let mut _12: core_arch::simd::i64x4;\n    debug src => _1;\n    debug k => _2;\n    debug a => _3;\n    debug b => _4;\n    debug not => _5;\n    debug andnot => _8;\n    bb0: {\n        StorageLive(_6);\n        StorageLive(_7);\n        _7 = num::<impl u64>::MAX as i64;\n        _6 = core_arch::x86::avx::_mm256_set1_epi64x(move _7) -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        StorageDead(_7);\n        _5 = core_arch::x86::avx512f::_mm256_xor_epi64(_3, move _6) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        StorageDead(_6);\n        StorageLive(_9);\n        _9 = core_arch::x86::__m256i::as_i64x4(_5) -> [return: bb3, unwind unreachable];\n    }\n    bb3: {\n        StorageLive(_10);\n        _10 = core_arch::x86::__m256i::as_i64x4(_4) -> [return: bb4, unwind unreachable];\n    }\n    bb4: {\n        _8 = intrinsics::simd::simd_and::<core_arch::simd::i64x4>(move _9, move _10) -> [return: bb5, unwind unreachable];\n    }\n    bb5: {\n        StorageDead(_10);\n        StorageDead(_9);\n        StorageLive(_11);\n        StorageLive(_12);\n        _12 = core_arch::x86::__m256i::as_i64x4(_1) -> [return: bb6, unwind unreachable];\n    }\n    bb6: {\n        _11 = intrinsics::simd::simd_select_bitmask::<u8, core_arch::simd::i64x4>(_2, _8, move _12) -> [return: bb7, unwind unreachable];\n    }\n    bb7: {\n        StorageDead(_12);\n        _0 = move _11 as core_arch::x86::__m256i;\n        StorageDead(_11);\n        return;\n    }\n}\n",
  "doc": " Compute the bitwise NOT of packed 64-bit integers in a and then AND with b, and store the results in dst using writemask k (elements are copied from src when the corresponding mask bit is not set).\n\n [Intel's documentation](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm256_mask_andnot_epi64&expand=315)\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}