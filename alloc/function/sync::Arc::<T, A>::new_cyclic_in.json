{
  "name": "sync::Arc::<T, A>::new_cyclic_in",
  "safe": true,
  "callees": {
    "core::sync::atomic::AtomicUsize::new": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a new atomic integer.\n\n # Examples\n\n ```\n\n ```\n",
      "adt": {}
    },
    "core::mem::MaybeUninit::<T>::uninit": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Creates a new `MaybeUninit<T>` in an uninitialized state.\n\n Note that dropping a `MaybeUninit<T>` will never call `T`'s drop code.\n It is your responsibility to make sure `T` gets dropped if it got initialized.\n\n See the [type-level documentation][MaybeUninit] for some examples.\n\n # Example\n\n ```\n use std::mem::MaybeUninit;\n\n let v: MaybeUninit<String> = MaybeUninit::uninit();\n ```\n",
      "adt": {}
    },
    "boxed::Box::<T, A>::new_in": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Allocates memory in the given allocator then places `x` into it.\n\n This doesn't actually allocate if `T` is zero-sized.\n\n # Examples\n\n ```\n #![feature(allocator_api)]\n\n use std::alloc::System;\n\n let five = Box::new_in(5, System);\n ```\n",
      "adt": {
        "boxed::Box": "Constructor"
      }
    },
    "boxed::Box::<T, A>::into_raw_with_allocator": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Consumes the `Box`, returning a wrapped raw pointer and the allocator.\n\n The pointer will be properly aligned and non-null.\n\n After calling this function, the caller is responsible for the\n memory previously managed by the `Box`. In particular, the\n caller should properly destroy `T` and release the memory, taking\n into account the [memory layout] used by `Box`. The easiest way to\n do this is to convert the raw pointer back into a `Box` with the\n [`Box::from_raw_in`] function, allowing the `Box` destructor to perform\n the cleanup.\n\n Note: this is an associated function, which means that you have\n to call it as `Box::into_raw_with_allocator(b)` instead of `b.into_raw_with_allocator()`. This\n is so that there is no conflict with a method on the inner type.\n\n # Examples\n Converting the raw pointer back into a `Box` with [`Box::from_raw_in`]\n for automatic cleanup:\n ```\n #![feature(allocator_api)]\n\n use std::alloc::System;\n\n let x = Box::new_in(String::from(\"Hello\"), System);\n let (ptr, alloc) = Box::into_raw_with_allocator(x);\n let x = unsafe { Box::from_raw_in(ptr, alloc) };\n ```\n Manual cleanup by explicitly running the destructor and deallocating\n the memory:\n ```\n #![feature(allocator_api)]\n\n use std::alloc::{Allocator, Layout, System};\n use std::ptr::{self, NonNull};\n\n let x = Box::new_in(String::from(\"Hello\"), System);\n let (ptr, alloc) = Box::into_raw_with_allocator(x);\n unsafe {\n     ptr::drop_in_place(ptr);\n     let non_null = NonNull::new_unchecked(ptr);\n     alloc.deallocate(non_null.cast(), Layout::new::<String>());\n }\n ```\n\n [memory layout]: self#memory-layout\n",
      "adt": {
        "boxed::Box": "MutableAsArgument"
      }
    },
    "core::convert::Into::into": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Converts this type into the (usually inferred) input type.\n",
      "adt": {}
    },
    "core::ptr::NonNull::<T>::cast": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Casts to a pointer of another type.\n\n # Examples\n\n ```\n use std::ptr::NonNull;\n\n let mut x = 0u32;\n let ptr = NonNull::new(&mut x as *mut _).expect(\"null pointer\");\n\n let casted_ptr = ptr.cast::<i8>();\n let raw_ptr: *mut i8 = casted_ptr.as_ptr();\n ```\n",
      "adt": {}
    },
    "core::ops::FnOnce::call_once": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Performs the call operation.\n",
      "adt": {}
    },
    "core::ptr::NonNull::<T>::as_ptr": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Acquires the underlying `*mut` pointer.\n\n # Examples\n\n ```\n use std::ptr::NonNull;\n\n let mut x = 0u32;\n let ptr = NonNull::new(&mut x).expect(\"ptr is null!\");\n\n let x_value = unsafe { *ptr.as_ptr() };\n assert_eq!(x_value, 0);\n\n unsafe { *ptr.as_ptr() += 2; }\n let x_value = unsafe { *ptr.as_ptr() };\n assert_eq!(x_value, 2);\n ```\n",
      "adt": {}
    },
    "core::ptr::write": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Overwrites a memory location with the given value without reading or\n dropping the old value.\n\n `write` does not drop the contents of `dst`. This is safe, but it could leak\n allocations or resources, so care should be taken not to overwrite an object\n that should be dropped.\n\n Additionally, it does not drop `src`. Semantically, `src` is moved into the\n location pointed to by `dst`.\n\n This is appropriate for initializing uninitialized memory, or overwriting\n memory that has previously been [`read`] from.\n\n # Safety\n\n Behavior is undefined if any of the following conditions are violated:\n\n * `dst` must be [valid] for writes.\n\n * `dst` must be properly aligned. Use [`write_unaligned`] if this is not the\n   case.\n\n Note that even if `T` has size `0`, the pointer must be properly aligned.\n\n [valid]: self#safety\n\n # Examples\n\n Basic usage:\n\n ```\n let mut x = 0;\n let y = &mut x as *mut i32;\n let z = 12;\n\n unsafe {\n     std::ptr::write(y, z);\n     assert_eq!(std::ptr::read(y), 12);\n }\n ```\n\n Manually implement [`mem::swap`]:\n\n ```\n use std::ptr;\n\n fn swap<T>(a: &mut T, b: &mut T) {\n     unsafe {\n         // Create a bitwise copy of the value at `a` in `tmp`.\n         let tmp = ptr::read(a);\n\n         // Exiting at this point (either by explicitly returning or by\n         // calling a function which panics) would cause the value in `tmp` to\n         // be dropped while the same value is still referenced by `a`. This\n         // could trigger undefined behavior if `T` is not `Copy`.\n\n         // Create a bitwise copy of the value at `b` in `a`.\n         // This is safe because mutable references cannot alias.\n         ptr::copy_nonoverlapping(b, a, 1);\n\n         // As above, exiting here could trigger undefined behavior because\n         // the same value is referenced by `a` and `b`.\n\n         // Move `tmp` into `b`.\n         ptr::write(b, tmp);\n\n         // `tmp` has been moved (`write` takes ownership of its second argument),\n         // so nothing is dropped implicitly here.\n     }\n }\n\n let mut foo = \"foo\".to_owned();\n let mut bar = \"bar\".to_owned();\n\n swap(&mut foo, &mut bar);\n\n assert_eq!(foo, \"bar\");\n assert_eq!(bar, \"foo\");\n ```\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicUsize::fetch_add": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Adds to the current value, returning the previous value.\n\n This operation wraps around on overflow.\n\n `fetch_add` takes an [`Ordering`] argument which describes the memory ordering\n of this operation. All ordering modes are possible. Note that using\n [`Acquire`] makes the store part of this operation [`Relaxed`], and\n using [`Release`] makes the load part [`Relaxed`].\n\n **Note**: This method is only available on platforms that support atomic operations on\n\n # Examples\n\n ```\n\n assert_eq!(foo.fetch_add(10, Ordering::SeqCst), 0);\n assert_eq!(foo.load(Ordering::SeqCst), 10);\n ```\n",
      "adt": {}
    },
    "sync::Weak::<T, A>::into_raw_with_allocator": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Consumes the `Weak<T>`, returning the wrapped pointer and allocator.\n\n This converts the weak pointer into a raw pointer, while still preserving the ownership of\n one weak reference (the weak count is not modified by this operation). It can be turned\n back into the `Weak<T>` with [`from_raw_in`].\n\n The same restrictions of accessing the target of the pointer as with\n [`as_ptr`] apply.\n\n # Examples\n\n ```\n #![feature(allocator_api)]\n use std::sync::{Arc, Weak};\n use std::alloc::System;\n\n let strong = Arc::new_in(\"hello\".to_owned(), System);\n let weak = Arc::downgrade(&strong);\n let (raw, alloc) = weak.into_raw_with_allocator();\n\n assert_eq!(1, Arc::weak_count(&strong));\n assert_eq!(\"hello\", unsafe { &*raw });\n\n drop(unsafe { Weak::from_raw_in(raw, alloc) });\n assert_eq!(0, Arc::weak_count(&strong));\n ```\n\n [`from_raw_in`]: Weak::from_raw_in\n [`as_ptr`]: Weak::as_ptr\n",
      "adt": {
        "sync::Weak": "ImmutableAsArgument"
      }
    },
    "core::fmt::Arguments::<'a>::from_str": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Create a `fmt::Arguments` object for a single static string.\n\n Formatting this `fmt::Arguments` will just produce the string as-is.\n",
      "adt": {}
    },
    "core::panicking::assert_failed": {
      "safe": true,
      "tags": {
        "tags": [
          {
            "tag": {
              "typ": null,
              "name": "hidden"
            },
            "args": []
          }
        ],
        "spec": {},
        "docs": [
          "* hidden\n"
        ]
      },
      "doc": " Internal function for `assert_eq!` and `assert_ne!` macros\n",
      "adt": {}
    },
    "sync::Arc::<T, A>::from_inner_in": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "",
      "adt": {
        "sync::Arc": "Constructor"
      }
    }
  },
  "adts": {
    "core::sync::atomic::AtomicUsize": [
      "Plain",
      "Ref"
    ],
    "core::mem::MaybeUninit": [
      "Plain"
    ],
    "sync::ArcInner": [
      "Plain",
      "MutRef"
    ],
    "boxed::Box": [
      "Plain"
    ],
    "core::ptr::NonNull": [
      "Plain"
    ],
    "sync::Weak": [
      "Plain",
      "Ref"
    ],
    "core::sync::atomic::Ordering": [
      "Plain"
    ],
    "core::panicking::AssertKind": [
      "Plain"
    ],
    "core::fmt::Arguments": [
      "Plain"
    ],
    "core::option::Option": [
      "Plain"
    ],
    "sync::Arc": [
      "Plain"
    ]
  },
  "path": 2085,
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/sync.rs:882:5: 940:6",
  "src": "pub fn new_cyclic_in<F>(data_fn: F, alloc: A) -> Arc<T, A>\n    where\n        F: FnOnce(&Weak<T, A>) -> T,\n    {\n        // Construct the inner in the \"uninitialized\" state with a single\n        // weak reference.\n        let (uninit_raw_ptr, alloc) = Box::into_raw_with_allocator(Box::new_in(\n            ArcInner {\n                strong: atomic::AtomicUsize::new(0),\n                weak: atomic::AtomicUsize::new(1),\n                data: mem::MaybeUninit::<T>::uninit(),\n            },\n            alloc,\n        ));\n        let uninit_ptr: NonNull<_> = (unsafe { &mut *uninit_raw_ptr }).into();\n        let init_ptr: NonNull<ArcInner<T>> = uninit_ptr.cast();\n\n        let weak = Weak { ptr: init_ptr, alloc };\n\n        // It's important we don't give up ownership of the weak pointer, or\n        // else the memory might be freed by the time `data_fn` returns. If\n        // we really wanted to pass ownership, we could create an additional\n        // weak pointer for ourselves, but this would result in additional\n        // updates to the weak reference count which might not be necessary\n        // otherwise.\n        let data = data_fn(&weak);\n\n        // Now we can properly initialize the inner value and turn our weak\n        // reference into a strong reference.\n        let strong = unsafe {\n            let inner = init_ptr.as_ptr();\n            ptr::write(&raw mut (*inner).data, data);\n\n            // The above write to the data field must be visible to any threads which\n            // observe a non-zero strong count. Therefore we need at least \"Release\" ordering\n            // in order to synchronize with the `compare_exchange_weak` in `Weak::upgrade`.\n            //\n            // \"Acquire\" ordering is not required. When considering the possible behaviors\n            // of `data_fn` we only need to look at what it could do with a reference to a\n            // non-upgradeable `Weak`:\n            // - It can *clone* the `Weak`, increasing the weak reference count.\n            // - It can drop those clones, decreasing the weak reference count (but never to zero).\n            //\n            // These side effects do not impact us in any way, and no other side effects are\n            // possible with safe code alone.\n            let prev_value = (*inner).strong.fetch_add(1, Release);\n            debug_assert_eq!(prev_value, 0, \"No prior strong references should exist\");\n\n            // Strong references should collectively own a shared weak reference,\n            // so don't run the destructor for our old weak reference.\n            // Calling into_raw_with_allocator has the double effect of giving us back the allocator,\n            // and forgetting the weak reference.\n            let alloc = weak.into_raw_with_allocator().1;\n\n            Arc::from_inner_in(init_ptr, alloc)\n        };\n\n        strong\n    }",
  "mir": "fn sync::Arc::<T, A>::new_cyclic_in(_1: F, _2: A) -> sync::Arc<T, A> {\n    let mut _0: sync::Arc<T, A>;\n    let  _3: *mut sync::ArcInner<core::mem::MaybeUninit<T>>;\n    let  _4: A;\n    let mut _5: (*mut sync::ArcInner<core::mem::MaybeUninit<T>>, A);\n    let mut _6: boxed::Box<sync::ArcInner<core::mem::MaybeUninit<T>>, A>;\n    let mut _7: sync::ArcInner<core::mem::MaybeUninit<T>>;\n    let mut _8: core::sync::atomic::AtomicUsize;\n    let mut _9: core::sync::atomic::AtomicUsize;\n    let mut _10: core::mem::MaybeUninit<T>;\n    let  _11: core::ptr::NonNull<sync::ArcInner<core::mem::MaybeUninit<T>>>;\n    let mut _12: &mut sync::ArcInner<core::mem::MaybeUninit<T>>;\n    let  _13: core::ptr::NonNull<sync::ArcInner<T>>;\n    let  _14: sync::Weak<T, A>;\n    let  _15: T;\n    let mut _16: (&sync::Weak<T, A>,);\n    let  _17: &sync::Weak<T, A>;\n    let  _18: *mut sync::ArcInner<T>;\n    let  _19: ();\n    let mut _20: *mut T;\n    let  _21: usize;\n    let mut _22: &core::sync::atomic::AtomicUsize;\n    let mut _23: core::sync::atomic::Ordering;\n    let mut _24: (&usize, &usize);\n    let mut _25: &usize;\n    let mut _26: &usize;\n    let  _27: &usize;\n    let  _28: &usize;\n    let mut _29: bool;\n    let mut _30: usize;\n    let mut _31: usize;\n    let  _32: core::panicking::AssertKind;\n    let  _33: !;\n    let mut _34: core::option::Option<core::fmt::Arguments<'_>>;\n    let mut _35: core::fmt::Arguments<'_>;\n    let  _36: A;\n    let mut _37: (*const T, A);\n    let mut _38: sync::Weak<T, A>;\n    debug data_fn => _1;\n    debug alloc => _2;\n    debug uninit_raw_ptr => _3;\n    debug alloc => _4;\n    debug uninit_ptr => _11;\n    debug init_ptr => _13;\n    debug weak => _14;\n    debug data => _15;\n    debug strong => _0;\n    debug inner => _18;\n    debug prev_value => _21;\n    debug left_val => _27;\n    debug right_val => _28;\n    debug kind => _32;\n    debug alloc => _36;\n    bb0: {\n        StorageLive(_5);\n        StorageLive(_6);\n        StorageLive(_7);\n        StorageLive(_8);\n        _8 = core::sync::atomic::AtomicUsize::new(0_usize) -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        StorageLive(_9);\n        _9 = core::sync::atomic::AtomicUsize::new(1_usize) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        StorageLive(_10);\n        _10 = core::mem::MaybeUninit::<T>::uninit() -> [return: bb3, unwind unreachable];\n    }\n    bb3: {\n        _7 = ArcInner(move _8, move _9, move _10);\n        StorageDead(_10);\n        StorageDead(_9);\n        StorageDead(_8);\n        _6 = boxed::Box::<sync::ArcInner<core::mem::MaybeUninit<T>>, A>::new_in(move _7, _2) -> [return: bb4, unwind unreachable];\n    }\n    bb4: {\n        StorageDead(_7);\n        _5 = boxed::Box::<sync::ArcInner<core::mem::MaybeUninit<T>>, A>::into_raw_with_allocator(move _6) -> [return: bb5, unwind unreachable];\n    }\n    bb5: {\n        StorageDead(_6);\n        StorageLive(_3);\n        _3 = (_5.0: *mut sync::ArcInner<core::mem::MaybeUninit<T>>);\n        _4 = move (_5.1: A);\n        StorageDead(_5);\n        _12 = &mut (*_3);\n        _11 = <&mut sync::ArcInner<core::mem::MaybeUninit<T>> as core::convert::Into<core::ptr::NonNull<sync::ArcInner<core::mem::MaybeUninit<T>>>>>::into(_12) -> [return: bb6, unwind unreachable];\n    }\n    bb6: {\n        _13 = core::ptr::NonNull::<sync::ArcInner<core::mem::MaybeUninit<T>>>::cast::<sync::ArcInner<T>>(_11) -> [return: bb7, unwind unreachable];\n    }\n    bb7: {\n        StorageLive(_14);\n        _14 = Weak(_13, _4);\n        StorageLive(_16);\n        _17 = &_14;\n        _16 = (_17);\n        _15 = <F as core::ops::FnOnce<(&sync::Weak<T, A>,)>>::call_once(_1, move _16) -> [return: bb8, unwind unreachable];\n    }\n    bb8: {\n        StorageDead(_16);\n        StorageLive(_18);\n        _18 = core::ptr::NonNull::<sync::ArcInner<T>>::as_ptr(_13) -> [return: bb9, unwind unreachable];\n    }\n    bb9: {\n        StorageLive(_20);\n        _20 = &raw mut ((*_18).2: T);\n        _19 = core::ptr::write::<T>(move _20, _15) -> [return: bb10, unwind unreachable];\n    }\n    bb10: {\n        StorageDead(_20);\n        StorageLive(_21);\n        StorageLive(_22);\n        _22 = &((*_18).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_23);\n        _23 = core::sync::atomic::Ordering::Release;\n        _21 = core::sync::atomic::AtomicUsize::fetch_add(move _22, 1_usize, move _23) -> [return: bb11, unwind unreachable];\n    }\n    bb11: {\n        StorageDead(_23);\n        StorageDead(_22);\n        StorageLive(_24);\n        StorageLive(_25);\n        _25 = &_21;\n        StorageLive(_26);\n        _26 = sync::Arc::<T, A>::new_cyclic_in::<F>::promoted[0];\n        _24 = (move _25, move _26);\n        StorageDead(_26);\n        StorageDead(_25);\n        _27 = (_24.0: &usize);\n        _28 = (_24.1: &usize);\n        StorageLive(_29);\n        StorageLive(_30);\n        _30 = (*_27);\n        StorageLive(_31);\n        _31 = (*_28);\n        _29 = Eq(move _30, move _31);\n        switchInt(move _29) -> [0: bb13, otherwise: bb12];\n    }\n    bb12: {\n        StorageDead(_31);\n        StorageDead(_30);\n        StorageDead(_29);\n        StorageDead(_24);\n        StorageLive(_37);\n        StorageLive(_38);\n        _38 = move _14;\n        _37 = sync::Weak::<T, A>::into_raw_with_allocator(move _38) -> [return: bb15, unwind unreachable];\n    }\n    bb13: {\n        StorageDead(_31);\n        StorageDead(_30);\n        _32 = core::panicking::AssertKind::Eq;\n        StorageLive(_34);\n        StorageLive(_35);\n        _35 = core::fmt::Arguments::<'_>::from_str(\"No prior strong references should exist\") -> [return: bb14, unwind unreachable];\n    }\n    bb14: {\n        _34 = core::option::Option::Some(move _35);\n        StorageDead(_35);\n        _33 = core::panicking::assert_failed::<usize, usize>(_32, _27, _28, move _34) -> unwind unreachable;\n    }\n    bb15: {\n        StorageDead(_38);\n        _36 = move (_37.1: A);\n        StorageDead(_37);\n        _0 = sync::Arc::<T, A>::from_inner_in(_13, _36) -> [return: bb16, unwind unreachable];\n    }\n    bb16: {\n        StorageDead(_21);\n        StorageDead(_18);\n        StorageDead(_14);\n        StorageDead(_3);\n        return;\n    }\n}\n",
  "doc": " Constructs a new `Arc<T, A>` in the given allocator while giving you a `Weak<T, A>` to the allocation,\n to allow you to construct a `T` which holds a weak pointer to itself.\n\n Generally, a structure circularly referencing itself, either directly or\n indirectly, should not hold a strong reference to itself to prevent a memory leak.\n Using this function, you get access to the weak pointer during the\n initialization of `T`, before the `Arc<T, A>` is created, such that you can\n clone and store it inside the `T`.\n\n `new_cyclic_in` first allocates the managed allocation for the `Arc<T, A>`,\n then calls your closure, giving it a `Weak<T, A>` to this allocation,\n and only afterwards completes the construction of the `Arc<T, A>` by placing\n the `T` returned from your closure into the allocation.\n\n Since the new `Arc<T, A>` is not fully-constructed until `Arc<T, A>::new_cyclic_in`\n returns, calling [`upgrade`] on the weak reference inside your closure will\n fail and result in a `None` value.\n\n # Panics\n\n If `data_fn` panics, the panic is propagated to the caller, and the\n temporary [`Weak<T>`] is dropped normally.\n\n # Example\n\n See [`new_cyclic`]\n\n [`new_cyclic`]: Arc::new_cyclic\n [`upgrade`]: Weak::upgrade\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}