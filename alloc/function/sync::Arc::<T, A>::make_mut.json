{
  "name": "sync::Arc::<T, A>::make_mut",
  "safe": true,
  "callees": {
    "core::ops::Deref::deref": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Dereferences the value.\n",
      "adt": {}
    },
    "core::mem::size_of_val": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns the size of the pointed-to value in bytes.\n\n This is usually the same as [`size_of::<T>()`]. However, when `T` *has* no\n statically-known size, e.g., a slice [`[T]`][slice] or a [trait object],\n then `size_of_val` can be used to get the dynamically-known size.\n\n [trait object]: ../../book/ch17-02-trait-objects.html\n\n # Examples\n\n ```\n assert_eq!(4, size_of_val(&5i32));\n\n let x: [u8; 13] = [0; 13];\n let y: &[u8] = &x;\n assert_eq!(13, size_of_val(y));\n ```\n\n [`size_of::<T>()`]: size_of\n",
      "adt": {}
    },
    "sync::Arc::<T, A>::inner": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "",
      "adt": {
        "sync::Arc": "ImmutableAsArgument"
      }
    },
    "core::sync::atomic::AtomicUsize::compare_exchange": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Stores a value into the atomic integer if the current value is the same as\n the `current` value.\n\n The return value is a result indicating whether the new value was written and\n containing the previous value. On success this value is guaranteed to be equal to\n `current`.\n\n `compare_exchange` takes two [`Ordering`] arguments to describe the memory\n ordering of this operation. `success` describes the required ordering for the\n read-modify-write operation that takes place if the comparison with `current` succeeds.\n `failure` describes the required ordering for the load operation that takes place when\n the comparison fails. Using [`Acquire`] as success ordering makes the store part\n of this operation [`Relaxed`], and using [`Release`] makes the successful load\n [`Relaxed`]. The failure ordering can only be [`SeqCst`], [`Acquire`] or [`Relaxed`].\n\n **Note**: This method is only available on platforms that support atomic operations on\n\n # Examples\n\n ```\n\n\n assert_eq!(some_var.compare_exchange(5, 10,\n                                      Ordering::Acquire,\n                                      Ordering::Relaxed),\n            Ok(5));\n assert_eq!(some_var.load(Ordering::Relaxed), 10);\n\n assert_eq!(some_var.compare_exchange(6, 12,\n                                      Ordering::SeqCst,\n                                      Ordering::Acquire),\n            Err(10));\n assert_eq!(some_var.load(Ordering::Relaxed), 10);\n ```\n\n # Considerations\n\n `compare_exchange` is a [compare-and-swap operation] and thus exhibits the usual downsides\n of CAS operations. In particular, a load of the value followed by a successful\n `compare_exchange` with the previous load *does not ensure* that other threads have not\n changed the value in the interim! This is usually important when the *equality* check in\n the `compare_exchange` is being used to check the *identity* of a value, but equality\n does not necessarily imply identity. This is a particularly common case for pointers, as\n a pointer holding the same address does not imply that the same object exists at that\n address! In this case, `compare_exchange` can lead to the [ABA problem].\n\n [ABA Problem]: https://en.wikipedia.org/wiki/ABA_problem\n [compare-and-swap operation]: https://en.wikipedia.org/wiki/Compare-and-swap\n",
      "adt": {}
    },
    "core::result::Result::<T, E>::is_err": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns `true` if the result is [`Err`].\n\n # Examples\n\n ```\n let x: Result<i32, &str> = Ok(-3);\n assert_eq!(x.is_err(), false);\n\n let x: Result<i32, &str> = Err(\"Some error message\");\n assert_eq!(x.is_err(), true);\n ```\n",
      "adt": {}
    },
    "core::clone::Clone::clone": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns a duplicate of the value.\n\n Note that what \"duplicate\" means varies by type:\n - For most types, this creates a deep, independent copy\n - For reference types like `&T`, this creates another reference to the same value\n - For smart pointers like [`Arc`] or [`Rc`], this increments the reference count\n   but still points to the same underlying data\n\n [`Arc`]: ../../std/sync/struct.Arc.html\n [`Rc`]: ../../std/rc/struct.Rc.html\n\n # Examples\n\n ```\n # #![allow(noop_method_call)]\n let hello = \"Hello\"; // &str implements Clone\n\n assert_eq!(\"Hello\", hello.clone());\n ```\n\n Example with a reference-counted type:\n\n ```\n use std::sync::{Arc, Mutex};\n\n let data = Arc::new(Mutex::new(vec![1, 2, 3]));\n let data_clone = data.clone(); // Creates another Arc pointing to the same Mutex\n\n {\n     let mut lock = data.lock().unwrap();\n     lock.push(4);\n }\n\n // Changes are visible through the clone because they share the same underlying data\n assert_eq!(*data_clone.lock().unwrap(), vec![1, 2, 3, 4]);\n ```\n",
      "adt": {}
    },
    "sync::Arc::<T, A>::clone_from_ref_in": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Constructs a new `Arc<T>` with a clone of `value` in the provided allocator.\n\n # Examples\n\n ```\n #![feature(clone_from_ref)]\n #![feature(allocator_api)]\n use std::sync::Arc;\n use std::alloc::System;\n\n let hello: Arc<str, System> = Arc::clone_from_ref_in(\"hello\", System);\n ```\n",
      "adt": {
        "sync::Arc": "Constructor"
      }
    },
    "core::sync::atomic::AtomicUsize::load": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Loads a value from the atomic integer.\n\n `load` takes an [`Ordering`] argument which describes the memory ordering of this operation.\n Possible values are [`SeqCst`], [`Acquire`] and [`Relaxed`].\n\n # Panics\n\n Panics if `order` is [`Release`] or [`AcqRel`].\n\n # Examples\n\n ```\n\n\n assert_eq!(some_var.load(Ordering::Relaxed), 5);\n ```\n",
      "adt": {}
    },
    "sync::UniqueArcUninit::<T, A>::new": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Allocates an ArcInner with layout suitable to contain `for_value` or a clone of it.\n",
      "adt": {
        "sync::UniqueArcUninit": "Constructor"
      }
    },
    "core::ptr::from_ref": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Converts a reference to a raw pointer.\n\n For `r: &T`, `from_ref(r)` is equivalent to `r as *const T` (except for the caveat noted below),\n but is a bit safer since it will never silently change type or mutability, in particular if the\n code is refactored.\n\n The caller must ensure that the pointee outlives the pointer this function returns, or else it\n will end up dangling.\n\n The caller must also ensure that the memory the pointer (non-transitively) points to is never\n written to (except inside an `UnsafeCell`) using this pointer or any pointer derived from it. If\n you need to mutate the pointee, use [`from_mut`]. Specifically, to turn a mutable reference `m:\n &mut T` into `*const T`, prefer `from_mut(m).cast_const()` to obtain a pointer that can later be\n used for mutation.\n\n ## Interaction with lifetime extension\n\n Note that this has subtle interactions with the rules for lifetime extension of temporaries in\n tail expressions. This code is valid, albeit in a non-obvious way:\n ```rust\n # type T = i32;\n # fn foo() -> T { 42 }\n // The temporary holding the return value of `foo` has its lifetime extended,\n // because the surrounding expression involves no function call.\n let p = &foo() as *const T;\n unsafe { p.read() };\n ```\n Naively replacing the cast with `from_ref` is not valid:\n ```rust,no_run\n # use std::ptr;\n # type T = i32;\n # fn foo() -> T { 42 }\n // The temporary holding the return value of `foo` does *not* have its lifetime extended,\n // because the surrounding expression involves a function call.\n let p = ptr::from_ref(&foo());\n unsafe { p.read() }; // UB! Reading from a dangling pointer ⚠️\n ```\n The recommended way to write this code is to avoid relying on lifetime extension\n when raw pointers are involved:\n ```rust\n # use std::ptr;\n # type T = i32;\n # fn foo() -> T { 42 }\n let x = foo();\n let p = ptr::from_ref(&x);\n unsafe { p.read() };\n ```\n",
      "adt": {}
    },
    "core::ptr::const_ptr::<impl *const T>::cast": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Casts to a pointer of another type.\n",
      "adt": {}
    },
    "sync::UniqueArcUninit::<T, A>::data_ptr": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns the pointer to be written into to initialize the [`Arc`].\n",
      "adt": {
        "sync::UniqueArcUninit": "MutableAsArgument"
      }
    },
    "core::ptr::mut_ptr::<impl *mut T>::cast": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Casts to a pointer of another type.\n",
      "adt": {}
    },
    "core::ptr::copy_nonoverlapping": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Copies `count * size_of::<T>()` bytes from `src` to `dst`. The source\n and destination must *not* overlap.\n\n For regions of memory which might overlap, use [`copy`] instead.\n\n `copy_nonoverlapping` is semantically equivalent to C's [`memcpy`], but\n with the source and destination arguments swapped,\n and `count` counting the number of `T`s instead of bytes.\n\n The copy is \"untyped\" in the sense that data may be uninitialized or otherwise violate the\n requirements of `T`. The initialization state is preserved exactly.\n\n [`memcpy`]: https://en.cppreference.com/w/c/string/byte/memcpy\n\n # Safety\n\n Behavior is undefined if any of the following conditions are violated:\n\n * `src` must be [valid] for reads of `count * size_of::<T>()` bytes.\n\n * `dst` must be [valid] for writes of `count * size_of::<T>()` bytes.\n\n * Both `src` and `dst` must be properly aligned.\n\n * The region of memory beginning at `src` with a size of `count *\n   size_of::<T>()` bytes must *not* overlap with the region of memory\n   beginning at `dst` with the same size.\n\n Like [`read`], `copy_nonoverlapping` creates a bitwise copy of `T`, regardless of\n whether `T` is [`Copy`]. If `T` is not [`Copy`], using *both* the values\n in the region beginning at `*src` and the region beginning at `*dst` can\n [violate memory safety][read-ownership].\n\n Note that even if the effectively copied size (`count * size_of::<T>()`) is\n `0`, the pointers must be properly aligned.\n\n [`read`]: crate::ptr::read\n [read-ownership]: crate::ptr::read#ownership-of-the-returned-value\n [valid]: crate::ptr#safety\n\n # Examples\n\n Manually implement [`Vec::append`]:\n\n ```\n use std::ptr;\n\n /// Moves all the elements of `src` into `dst`, leaving `src` empty.\n fn append<T>(dst: &mut Vec<T>, src: &mut Vec<T>) {\n     let src_len = src.len();\n     let dst_len = dst.len();\n\n     // Ensure that `dst` has enough capacity to hold all of `src`.\n     dst.reserve(src_len);\n\n     unsafe {\n         // The call to add is always safe because `Vec` will never\n         // allocate more than `isize::MAX` bytes.\n         let dst_ptr = dst.as_mut_ptr().add(dst_len);\n         let src_ptr = src.as_ptr();\n\n         // Truncate `src` without dropping its contents. We do this first,\n         // to avoid problems in case something further down panics.\n         src.set_len(0);\n\n         // The two regions cannot overlap because mutable references do\n         // not alias, and two different vectors cannot own the same\n         // memory.\n         ptr::copy_nonoverlapping(src_ptr, dst_ptr, src_len);\n\n         // Notify `dst` that it now holds the contents of `src`.\n         dst.set_len(dst_len + src_len);\n     }\n }\n\n let mut a = vec!['r'];\n let mut b = vec!['u', 's', 't'];\n\n append(&mut a, &mut b);\n\n assert_eq!(a, &['r', 'u', 's', 't']);\n assert!(b.is_empty());\n ```\n\n [`Vec::append`]: ../../std/vec/struct.Vec.html#method.append\n",
      "adt": {}
    },
    "sync::UniqueArcUninit::<T, A>::into_arc": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Upgrade this into a normal [`Arc`].\n\n # Safety\n\n The data must have been initialized (by writing to [`Self::data_ptr()`]).\n",
      "adt": {
        "sync::Arc": "Constructor",
        "sync::UniqueArcUninit": "MutableAsArgument"
      }
    },
    "core::ptr::write": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Overwrites a memory location with the given value without reading or\n dropping the old value.\n\n `write` does not drop the contents of `dst`. This is safe, but it could leak\n allocations or resources, so care should be taken not to overwrite an object\n that should be dropped.\n\n Additionally, it does not drop `src`. Semantically, `src` is moved into the\n location pointed to by `dst`.\n\n This is appropriate for initializing uninitialized memory, or overwriting\n memory that has previously been [`read`] from.\n\n # Safety\n\n Behavior is undefined if any of the following conditions are violated:\n\n * `dst` must be [valid] for writes.\n\n * `dst` must be properly aligned. Use [`write_unaligned`] if this is not the\n   case.\n\n Note that even if `T` has size `0`, the pointer must be properly aligned.\n\n [valid]: self#safety\n\n # Examples\n\n Basic usage:\n\n ```\n let mut x = 0;\n let y = &mut x as *mut i32;\n let z = 12;\n\n unsafe {\n     std::ptr::write(y, z);\n     assert_eq!(std::ptr::read(y), 12);\n }\n ```\n\n Manually implement [`mem::swap`]:\n\n ```\n use std::ptr;\n\n fn swap<T>(a: &mut T, b: &mut T) {\n     unsafe {\n         // Create a bitwise copy of the value at `a` in `tmp`.\n         let tmp = ptr::read(a);\n\n         // Exiting at this point (either by explicitly returning or by\n         // calling a function which panics) would cause the value in `tmp` to\n         // be dropped while the same value is still referenced by `a`. This\n         // could trigger undefined behavior if `T` is not `Copy`.\n\n         // Create a bitwise copy of the value at `b` in `a`.\n         // This is safe because mutable references cannot alias.\n         ptr::copy_nonoverlapping(b, a, 1);\n\n         // As above, exiting here could trigger undefined behavior because\n         // the same value is referenced by `a` and `b`.\n\n         // Move `tmp` into `b`.\n         ptr::write(b, tmp);\n\n         // `tmp` has been moved (`write` takes ownership of its second argument),\n         // so nothing is dropped implicitly here.\n     }\n }\n\n let mut foo = \"foo\".to_owned();\n let mut bar = \"bar\".to_owned();\n\n swap(&mut foo, &mut bar);\n\n assert_eq!(foo, \"bar\");\n assert_eq!(bar, \"foo\");\n ```\n",
      "adt": {}
    },
    "core::sync::atomic::AtomicUsize::store": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Stores a value into the atomic integer.\n\n `store` takes an [`Ordering`] argument which describes the memory ordering of this operation.\n  Possible values are [`SeqCst`], [`Release`] and [`Relaxed`].\n\n # Panics\n\n Panics if `order` is [`Acquire`] or [`AcqRel`].\n\n # Examples\n\n ```\n\n\n some_var.store(10, Ordering::Relaxed);\n assert_eq!(some_var.load(Ordering::Relaxed), 10);\n ```\n",
      "adt": {}
    },
    "sync::Arc::<T, A>::get_mut_unchecked": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns a mutable reference into the given `Arc`,\n without any check.\n\n See also [`get_mut`], which is safe and does appropriate checks.\n\n [`get_mut`]: Arc::get_mut\n\n # Safety\n\n If any other `Arc` or [`Weak`] pointers to the same allocation exist, then\n they must not be dereferenced or have active borrows for the duration\n of the returned borrow, and their inner type must be exactly the same as the\n inner type of this Arc (including lifetimes). This is trivially the case if no\n such pointers exist, for example immediately after `Arc::new`.\n\n # Examples\n\n ```\n #![feature(get_mut_unchecked)]\n\n use std::sync::Arc;\n\n let mut x = Arc::new(String::new());\n unsafe {\n     Arc::get_mut_unchecked(&mut x).push_str(\"foo\")\n }\n assert_eq!(*x, \"foo\");\n ```\n Other `Arc` pointers to the same allocation must be to the same type.\n ```no_run\n #![feature(get_mut_unchecked)]\n\n use std::sync::Arc;\n\n let x: Arc<str> = Arc::from(\"Hello, world!\");\n let mut y: Arc<[u8]> = x.clone().into();\n unsafe {\n     // this is Undefined Behavior, because x's inner type is str, not [u8]\n     Arc::get_mut_unchecked(&mut y).fill(0xff); // 0xff is invalid in UTF-8\n }\n println!(\"{}\", &*x); // Invalid UTF-8 in a str\n ```\n Other `Arc` pointers to the same allocation must be to the exact same type, including lifetimes.\n ```no_run\n #![feature(get_mut_unchecked)]\n\n use std::sync::Arc;\n\n let x: Arc<&str> = Arc::new(\"Hello, world!\");\n {\n     let s = String::from(\"Oh, no!\");\n     let mut y: Arc<&str> = x.clone();\n     unsafe {\n         // this is Undefined Behavior, because x's inner type\n         // is &'long str, not &'short str\n         *Arc::get_mut_unchecked(&mut y) = &s;\n     }\n }\n println!(\"{}\", &*x); // Use-after-free\n ```\n",
      "adt": {
        "sync::Arc": "MutableAsArgument"
      }
    }
  },
  "adts": {
    "sync::Arc": [
      "Ref",
      "Deref",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(2)))",
      "Plain",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))",
      "MutRef"
    ],
    "sync::ArcInner": [
      "Ref",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(1)))"
    ],
    "core::sync::atomic::AtomicUsize": [
      "Ref"
    ],
    "core::sync::atomic::Ordering": [
      "Plain"
    ],
    "core::result::Result": [
      "Plain",
      "Ref"
    ],
    "core::ptr::NonNull": [
      "Plain"
    ],
    "sync::Weak": [
      "Plain"
    ],
    "sync::UniqueArcUninit": [
      "Plain",
      "MutRef"
    ]
  },
  "path": {
    "type": "Local",
    "path": "alloc::sync::Arc::<T, A>::make_mut"
  },
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/sync.rs:2492:5: 2550:6",
  "src": "pub fn make_mut(this: &mut Self) -> &mut T {\n        let size_of_val = size_of_val::<T>(&**this);\n\n        // Note that we hold both a strong reference and a weak reference.\n        // Thus, releasing our strong reference only will not, by itself, cause\n        // the memory to be deallocated.\n        //\n        // Use Acquire to ensure that we see any writes to `weak` that happen\n        // before release writes (i.e., decrements) to `strong`. Since we hold a\n        // weak count, there's no chance the ArcInner itself could be\n        // deallocated.\n        if this.inner().strong.compare_exchange(1, 0, Acquire, Relaxed).is_err() {\n            // Another strong pointer exists, so we must clone.\n            *this = Arc::clone_from_ref_in(&**this, this.alloc.clone());\n        } else if this.inner().weak.load(Relaxed) != 1 {\n            // Relaxed suffices in the above because this is fundamentally an\n            // optimization: we are always racing with weak pointers being\n            // dropped. Worst case, we end up allocated a new Arc unnecessarily.\n\n            // We removed the last strong ref, but there are additional weak\n            // refs remaining. We'll move the contents to a new Arc, and\n            // invalidate the other weak refs.\n\n            // Note that it is not possible for the read of `weak` to yield\n            // usize::MAX (i.e., locked), since the weak count can only be\n            // locked by a thread with a strong reference.\n\n            // Materialize our own implicit weak pointer, so that it can clean\n            // up the ArcInner as needed.\n            let _weak = Weak { ptr: this.ptr, alloc: this.alloc.clone() };\n\n            // Can just steal the data, all that's left is Weaks\n            //\n            // We don't need panic-protection like the above branch does, but we might as well\n            // use the same mechanism.\n            let mut in_progress: UniqueArcUninit<T, A> =\n                UniqueArcUninit::new(&**this, this.alloc.clone());\n            unsafe {\n                // Initialize `in_progress` with move of **this.\n                // We have to express this in terms of bytes because `T: ?Sized`; there is no\n                // operation that just copies a value based on its `size_of_val()`.\n                ptr::copy_nonoverlapping(\n                    ptr::from_ref(&**this).cast::<u8>(),\n                    in_progress.data_ptr().cast::<u8>(),\n                    size_of_val,\n                );\n\n                ptr::write(this, in_progress.into_arc());\n            }\n        } else {\n            // We were the sole reference of either kind; bump back up the\n            // strong ref count.\n            this.inner().strong.store(1, Release);\n        }\n\n        // As with `get_mut()`, the unsafety is ok because our reference was\n        // either unique to begin with, or became one upon cloning the contents.\n        unsafe { Self::get_mut_unchecked(this) }\n    }",
  "mir": "fn sync::Arc::<T, A>::make_mut(_1: &mut sync::Arc<T, A>) -> &mut T {\n    let mut _0: &mut T;\n    let  _2: usize;\n    let  _3: &T;\n    let mut _4: &sync::Arc<T, A>;\n    let mut _5: bool;\n    let mut _6: &core::result::Result<usize, usize>;\n    let  _7: core::result::Result<usize, usize>;\n    let mut _8: &core::sync::atomic::AtomicUsize;\n    let  _9: &sync::ArcInner<T>;\n    let mut _10: &sync::Arc<T, A>;\n    let mut _11: core::sync::atomic::Ordering;\n    let mut _12: core::sync::atomic::Ordering;\n    let mut _13: sync::Arc<T, A>;\n    let  _14: &T;\n    let mut _15: &sync::Arc<T, A>;\n    let mut _16: A;\n    let mut _17: &A;\n    let mut _18: usize;\n    let mut _19: &core::sync::atomic::AtomicUsize;\n    let  _20: &sync::ArcInner<T>;\n    let mut _21: &sync::Arc<T, A>;\n    let mut _22: core::sync::atomic::Ordering;\n    let  _23: sync::Weak<T, A>;\n    let mut _24: core::ptr::NonNull<sync::ArcInner<T>>;\n    let mut _25: A;\n    let mut _26: &A;\n    let mut _27: sync::UniqueArcUninit<T, A>;\n    let  _28: &T;\n    let mut _29: &sync::Arc<T, A>;\n    let mut _30: A;\n    let mut _31: &A;\n    let  _32: ();\n    let mut _33: *const u8;\n    let mut _34: *const T;\n    let  _35: &T;\n    let mut _36: &sync::Arc<T, A>;\n    let mut _37: *mut u8;\n    let mut _38: *mut T;\n    let mut _39: &mut sync::UniqueArcUninit<T, A>;\n    let  _40: ();\n    let mut _41: *mut sync::Arc<T, A>;\n    let mut _42: sync::Arc<T, A>;\n    let mut _43: sync::UniqueArcUninit<T, A>;\n    let  _44: ();\n    let mut _45: &core::sync::atomic::AtomicUsize;\n    let  _46: &sync::ArcInner<T>;\n    let mut _47: &sync::Arc<T, A>;\n    let mut _48: core::sync::atomic::Ordering;\n    debug this => _1;\n    debug size_of_val => _2;\n    debug _weak => _23;\n    debug in_progress => _27;\n    bb0: {\n        StorageLive(_4);\n        _4 = &(*_1);\n        _3 = <sync::Arc<T, A> as core::ops::Deref>::deref(move _4) -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        StorageDead(_4);\n        _2 = core::mem::size_of_val::<T>(_3) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        StorageLive(_5);\n        StorageLive(_6);\n        StorageLive(_7);\n        StorageLive(_8);\n        StorageLive(_9);\n        StorageLive(_10);\n        _10 = &(*_1);\n        _9 = sync::Arc::<T, A>::inner(move _10) -> [return: bb3, unwind unreachable];\n    }\n    bb3: {\n        StorageDead(_10);\n        _8 = &((*_9).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_11);\n        _11 = core::sync::atomic::Ordering::Acquire;\n        StorageLive(_12);\n        _12 = core::sync::atomic::Ordering::Relaxed;\n        _7 = core::sync::atomic::AtomicUsize::compare_exchange(move _8, 1_usize, 0_usize, move _11, move _12) -> [return: bb4, unwind unreachable];\n    }\n    bb4: {\n        _6 = &_7;\n        StorageDead(_12);\n        StorageDead(_11);\n        StorageDead(_8);\n        _5 = core::result::Result::<usize, usize>::is_err(move _6) -> [return: bb5, unwind unreachable];\n    }\n    bb5: {\n        switchInt(move _5) -> [0: bb11, otherwise: bb6];\n    }\n    bb6: {\n        StorageDead(_9);\n        StorageDead(_7);\n        StorageDead(_6);\n        StorageLive(_13);\n        StorageLive(_15);\n        _15 = &(*_1);\n        _14 = <sync::Arc<T, A> as core::ops::Deref>::deref(move _15) -> [return: bb7, unwind unreachable];\n    }\n    bb7: {\n        StorageDead(_15);\n        StorageLive(_16);\n        StorageLive(_17);\n        _17 = &((*_1).2: A);\n        _16 = <A as core::clone::Clone>::clone(move _17) -> [return: bb8, unwind unreachable];\n    }\n    bb8: {\n        StorageDead(_17);\n        _13 = sync::Arc::<T, A>::clone_from_ref_in(_14, move _16) -> [return: bb9, unwind unreachable];\n    }\n    bb9: {\n        StorageDead(_16);\n        drop((*_1)) -> [return: bb10, unwind unreachable];\n    }\n    bb10: {\n        (*_1) = move _13;\n        StorageDead(_13);\n        goto -> bb32;\n    }\n    bb11: {\n        StorageDead(_9);\n        StorageDead(_7);\n        StorageDead(_6);\n        StorageLive(_18);\n        StorageLive(_19);\n        StorageLive(_20);\n        StorageLive(_21);\n        _21 = &(*_1);\n        _20 = sync::Arc::<T, A>::inner(move _21) -> [return: bb12, unwind unreachable];\n    }\n    bb12: {\n        StorageDead(_21);\n        _19 = &((*_20).1: core::sync::atomic::AtomicUsize);\n        StorageLive(_22);\n        _22 = core::sync::atomic::Ordering::Relaxed;\n        _18 = core::sync::atomic::AtomicUsize::load(move _19, move _22) -> [return: bb13, unwind unreachable];\n    }\n    bb13: {\n        StorageDead(_22);\n        StorageDead(_19);\n        switchInt(move _18) -> [1: bb28, otherwise: bb14];\n    }\n    bb14: {\n        StorageDead(_20);\n        StorageDead(_18);\n        StorageLive(_23);\n        StorageLive(_24);\n        _24 = ((*_1).0: core::ptr::NonNull<sync::ArcInner<T>>);\n        StorageLive(_25);\n        StorageLive(_26);\n        _26 = &((*_1).2: A);\n        _25 = <A as core::clone::Clone>::clone(move _26) -> [return: bb15, unwind unreachable];\n    }\n    bb15: {\n        StorageDead(_26);\n        _23 = Weak(move _24, move _25);\n        StorageDead(_25);\n        StorageDead(_24);\n        StorageLive(_27);\n        StorageLive(_29);\n        _29 = &(*_1);\n        _28 = <sync::Arc<T, A> as core::ops::Deref>::deref(move _29) -> [return: bb16, unwind unreachable];\n    }\n    bb16: {\n        StorageDead(_29);\n        StorageLive(_30);\n        StorageLive(_31);\n        _31 = &((*_1).2: A);\n        _30 = <A as core::clone::Clone>::clone(move _31) -> [return: bb17, unwind unreachable];\n    }\n    bb17: {\n        StorageDead(_31);\n        _27 = sync::UniqueArcUninit::<T, A>::new(_28, move _30) -> [return: bb18, unwind unreachable];\n    }\n    bb18: {\n        StorageDead(_30);\n        StorageLive(_33);\n        StorageLive(_34);\n        StorageLive(_36);\n        _36 = &(*_1);\n        _35 = <sync::Arc<T, A> as core::ops::Deref>::deref(move _36) -> [return: bb19, unwind unreachable];\n    }\n    bb19: {\n        StorageDead(_36);\n        _34 = core::ptr::from_ref::<T>(_35) -> [return: bb20, unwind unreachable];\n    }\n    bb20: {\n        _33 = core::ptr::const_ptr::<impl *const T>::cast::<u8>(move _34) -> [return: bb21, unwind unreachable];\n    }\n    bb21: {\n        StorageDead(_34);\n        StorageLive(_37);\n        StorageLive(_38);\n        StorageLive(_39);\n        _39 = &mut _27;\n        _38 = sync::UniqueArcUninit::<T, A>::data_ptr(move _39) -> [return: bb22, unwind unreachable];\n    }\n    bb22: {\n        StorageDead(_39);\n        _37 = core::ptr::mut_ptr::<impl *mut T>::cast::<u8>(move _38) -> [return: bb23, unwind unreachable];\n    }\n    bb23: {\n        StorageDead(_38);\n        _32 = core::ptr::copy_nonoverlapping::<u8>(move _33, move _37, _2) -> [return: bb24, unwind unreachable];\n    }\n    bb24: {\n        StorageDead(_37);\n        StorageDead(_33);\n        StorageLive(_41);\n        _41 = &raw mut (*_1);\n        StorageLive(_42);\n        StorageLive(_43);\n        _43 = move _27;\n        _42 = sync::UniqueArcUninit::<T, A>::into_arc(move _43) -> [return: bb25, unwind unreachable];\n    }\n    bb25: {\n        StorageDead(_43);\n        _40 = core::ptr::write::<sync::Arc<T, A>>(move _41, move _42) -> [return: bb26, unwind unreachable];\n    }\n    bb26: {\n        StorageDead(_42);\n        StorageDead(_41);\n        StorageDead(_27);\n        drop(_23) -> [return: bb27, unwind unreachable];\n    }\n    bb27: {\n        StorageDead(_23);\n        goto -> bb31;\n    }\n    bb28: {\n        StorageDead(_20);\n        StorageDead(_18);\n        StorageLive(_45);\n        StorageLive(_46);\n        StorageLive(_47);\n        _47 = &(*_1);\n        _46 = sync::Arc::<T, A>::inner(move _47) -> [return: bb29, unwind unreachable];\n    }\n    bb29: {\n        StorageDead(_47);\n        _45 = &((*_46).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_48);\n        _48 = core::sync::atomic::Ordering::Release;\n        _44 = core::sync::atomic::AtomicUsize::store(move _45, 1_usize, move _48) -> [return: bb30, unwind unreachable];\n    }\n    bb30: {\n        StorageDead(_48);\n        StorageDead(_45);\n        StorageDead(_46);\n        goto -> bb31;\n    }\n    bb31: {\n        goto -> bb32;\n    }\n    bb32: {\n        StorageDead(_5);\n        _0 = sync::Arc::<T, A>::get_mut_unchecked(_1) -> [return: bb33, unwind unreachable];\n    }\n    bb33: {\n        return;\n    }\n}\n",
  "doc": " Makes a mutable reference into the given `Arc`.\n\n If there are other `Arc` pointers to the same allocation, then `make_mut` will\n [`clone`] the inner value to a new allocation to ensure unique ownership.  This is also\n referred to as clone-on-write.\n\n However, if there are no other `Arc` pointers to this allocation, but some [`Weak`]\n pointers, then the [`Weak`] pointers will be dissociated and the inner value will not\n be cloned.\n\n See also [`get_mut`], which will fail rather than cloning the inner value\n or dissociating [`Weak`] pointers.\n\n [`clone`]: Clone::clone\n [`get_mut`]: Arc::get_mut\n\n # Examples\n\n ```\n use std::sync::Arc;\n\n let mut data = Arc::new(5);\n\n *Arc::make_mut(&mut data) += 1;         // Won't clone anything\n let mut other_data = Arc::clone(&data); // Won't clone inner data\n *Arc::make_mut(&mut data) += 1;         // Clones inner data\n *Arc::make_mut(&mut data) += 1;         // Won't clone anything\n *Arc::make_mut(&mut other_data) *= 2;   // Won't clone anything\n\n // Now `data` and `other_data` point to different allocations.\n assert_eq!(*data, 8);\n assert_eq!(*other_data, 12);\n ```\n\n [`Weak`] pointers will be dissociated:\n\n ```\n use std::sync::Arc;\n\n let mut data = Arc::new(75);\n let weak = Arc::downgrade(&data);\n\n assert!(75 == *data);\n assert!(75 == *weak.upgrade().unwrap());\n\n *Arc::make_mut(&mut data) += 1;\n\n assert!(76 == *data);\n assert!(weak.upgrade().is_none());\n ```\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}