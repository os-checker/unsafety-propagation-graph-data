{
  "name": "sync::Arc::<T, A>::into_inner",
  "safe": true,
  "callees": {
    "core::mem::ManuallyDrop::<T>::new": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Wrap a value to be manually dropped.\n\n # Examples\n\n ```rust\n use std::mem::ManuallyDrop;\n let mut x = ManuallyDrop::new(String::from(\"Hello World!\"));\n x.truncate(5); // You can still safely operate on the value\n assert_eq!(*x, \"Hello\");\n // But `Drop` will not be run here\n # // FIXME(https://github.com/rust-lang/miri/issues/3670):\n # // use -Zmiri-disable-leak-check instead of unleaking in tests meant to leak.\n # let _ = ManuallyDrop::into_inner(x);\n ```\n",
      "adt": {}
    },
    "core::ops::Deref::deref": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Dereferences the value.\n",
      "adt": {}
    },
    "sync::Arc::<T, A>::inner": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": "",
      "adt": {
        "sync::Arc": "ImmutableAsArgument"
      }
    },
    "core::sync::atomic::AtomicUsize::fetch_sub": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Subtracts from the current value, returning the previous value.\n\n This operation wraps around on overflow.\n\n `fetch_sub` takes an [`Ordering`] argument which describes the memory ordering\n of this operation. All ordering modes are possible. Note that using\n [`Acquire`] makes the store part of this operation [`Relaxed`], and\n using [`Release`] makes the load part [`Relaxed`].\n\n **Note**: This method is only available on platforms that support atomic operations on\n\n # Examples\n\n ```\n\n assert_eq!(foo.fetch_sub(10, Ordering::SeqCst), 20);\n assert_eq!(foo.load(Ordering::SeqCst), 10);\n ```\n",
      "adt": {}
    },
    "core::sync::atomic::fence": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " An atomic fence.\n\n Fences create synchronization between themselves and atomic operations or fences in other\n threads. To achieve this, a fence prevents the compiler and CPU from reordering certain types of\n memory operations around it.\n\n There are 3 different ways to use an atomic fence:\n\n - atomic - fence synchronization: an atomic operation with (at least) [`Release`] ordering\n   semantics synchronizes with a fence with (at least) [`Acquire`] ordering semantics.\n - fence - atomic synchronization: a fence with (at least) [`Release`] ordering semantics\n   synchronizes with an atomic operation with (at least) [`Acquire`] ordering semantics.\n - fence - fence synchronization: a fence with (at least) [`Release`] ordering semantics\n   synchronizes with a fence with (at least) [`Acquire`] ordering semantics.\n\n These 3 ways complement the regular, fence-less, atomic - atomic synchronization.\n\n ## Atomic - Fence\n\n An atomic operation on one thread will synchronize with a fence on another thread when:\n\n -   on thread 1:\n     -   an atomic operation 'X' with (at least) [`Release`] ordering semantics on some atomic\n         object 'm',\n\n -   is paired on thread 2 with:\n     -   an atomic read 'Y' with any order on 'm',\n     -   followed by a fence 'B' with (at least) [`Acquire`] ordering semantics.\n\n This provides a happens-before dependence between X and B.\n\n ```text\n     Thread 1                                          Thread 2\n\n m.store(3, Release); X ---------\n                                |\n                                |\n                                -------------> Y  if m.load(Relaxed) == 3 {\n                                               B      fence(Acquire);\n                                                      ...\n                                                  }\n ```\n\n ## Fence - Atomic\n\n A fence on one thread will synchronize with an atomic operation on another thread when:\n\n -   on thread:\n     -   a fence 'A' with (at least) [`Release`] ordering semantics,\n     -   followed by an atomic write 'X' with any ordering on some atomic object 'm',\n\n -   is paired on thread 2 with:\n     -   an atomic operation 'Y' with (at least) [`Acquire`] ordering semantics.\n\n This provides a happens-before dependence between A and Y.\n\n ```text\n     Thread 1                                          Thread 2\n\n fence(Release);      A\n m.store(3, Relaxed); X ---------\n                                |\n                                |\n                                -------------> Y  if m.load(Acquire) == 3 {\n                                                      ...\n                                                  }\n ```\n\n ## Fence - Fence\n\n A fence on one thread will synchronize with a fence on another thread when:\n\n -   on thread 1:\n     -   a fence 'A' which has (at least) [`Release`] ordering semantics,\n     -   followed by an atomic write 'X' with any ordering on some atomic object 'm',\n\n -   is paired on thread 2 with:\n     -   an atomic read 'Y' with any ordering on 'm',\n     -   followed by a fence 'B' with (at least) [`Acquire`] ordering semantics.\n\n This provides a happens-before dependence between A and B.\n\n ```text\n     Thread 1                                          Thread 2\n\n fence(Release);      A --------------\n m.store(3, Relaxed); X ---------    |\n                                |    |\n                                |    |\n                                -------------> Y  if m.load(Relaxed) == 3 {\n                                     |-------> B      fence(Acquire);\n                                                      ...\n                                                  }\n ```\n\n ## Mandatory Atomic\n\n Note that in the examples above, it is crucial that the access to `m` are atomic. Fences cannot\n be used to establish synchronization between non-atomic accesses in different threads. However,\n thanks to the happens-before relationship, any non-atomic access that happen-before the atomic\n operation or fence with (at least) [`Release`] ordering semantics are now also properly\n synchronized with any non-atomic accesses that happen-after the atomic operation or fence with\n (at least) [`Acquire`] ordering semantics.\n\n ## Memory Ordering\n\n A fence which has [`SeqCst`] ordering, in addition to having both [`Acquire`] and [`Release`]\n semantics, participates in the global program order of the other [`SeqCst`] operations and/or\n fences.\n\n Accepts [`Acquire`], [`Release`], [`AcqRel`] and [`SeqCst`] orderings.\n\n # Panics\n\n Panics if `order` is [`Relaxed`].\n\n # Examples\n\n ```\n use std::sync::atomic::AtomicBool;\n use std::sync::atomic::fence;\n use std::sync::atomic::Ordering;\n\n // A mutual exclusion primitive based on spinlock.\n pub struct Mutex {\n     flag: AtomicBool,\n }\n\n impl Mutex {\n     pub fn new() -> Mutex {\n         Mutex {\n             flag: AtomicBool::new(false),\n         }\n     }\n\n     pub fn lock(&self) {\n         // Wait until the old value is `false`.\n         while self\n             .flag\n             .compare_exchange_weak(false, true, Ordering::Relaxed, Ordering::Relaxed)\n             .is_err()\n         {}\n         // This fence synchronizes-with store in `unlock`.\n         fence(Ordering::Acquire);\n     }\n\n     pub fn unlock(&self) {\n         self.flag.store(false, Ordering::Release);\n     }\n }\n ```\n",
      "adt": {}
    },
    "core::ops::DerefMut::deref_mut": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Mutably dereferences the value.\n",
      "adt": {}
    },
    "sync::Arc::<T, A>::get_mut_unchecked": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns a mutable reference into the given `Arc`,\n without any check.\n\n See also [`get_mut`], which is safe and does appropriate checks.\n\n [`get_mut`]: Arc::get_mut\n\n # Safety\n\n If any other `Arc` or [`Weak`] pointers to the same allocation exist, then\n they must not be dereferenced or have active borrows for the duration\n of the returned borrow, and their inner type must be exactly the same as the\n inner type of this Arc (including lifetimes). This is trivially the case if no\n such pointers exist, for example immediately after `Arc::new`.\n\n # Examples\n\n ```\n #![feature(get_mut_unchecked)]\n\n use std::sync::Arc;\n\n let mut x = Arc::new(String::new());\n unsafe {\n     Arc::get_mut_unchecked(&mut x).push_str(\"foo\")\n }\n assert_eq!(*x, \"foo\");\n ```\n Other `Arc` pointers to the same allocation must be to the same type.\n ```no_run\n #![feature(get_mut_unchecked)]\n\n use std::sync::Arc;\n\n let x: Arc<str> = Arc::from(\"Hello, world!\");\n let mut y: Arc<[u8]> = x.clone().into();\n unsafe {\n     // this is Undefined Behavior, because x's inner type is str, not [u8]\n     Arc::get_mut_unchecked(&mut y).fill(0xff); // 0xff is invalid in UTF-8\n }\n println!(\"{}\", &*x); // Invalid UTF-8 in a str\n ```\n Other `Arc` pointers to the same allocation must be to the exact same type, including lifetimes.\n ```no_run\n #![feature(get_mut_unchecked)]\n\n use std::sync::Arc;\n\n let x: Arc<&str> = Arc::new(\"Hello, world!\");\n {\n     let s = String::from(\"Oh, no!\");\n     let mut y: Arc<&str> = x.clone();\n     unsafe {\n         // this is Undefined Behavior, because x's inner type\n         // is &'long str, not &'short str\n         *Arc::get_mut_unchecked(&mut y) = &s;\n     }\n }\n println!(\"{}\", &*x); // Use-after-free\n ```\n",
      "adt": {
        "sync::Arc": "MutableAsArgument"
      }
    },
    "core::ptr::read": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Reads the value from `src` without moving it. This leaves the\n memory in `src` unchanged.\n\n # Safety\n\n Behavior is undefined if any of the following conditions are violated:\n\n * `src` must be [valid] for reads.\n\n * `src` must be properly aligned. Use [`read_unaligned`] if this is not the\n   case.\n\n * `src` must point to a properly initialized value of type `T`.\n\n Note that even if `T` has size `0`, the pointer must be properly aligned.\n\n # Examples\n\n Basic usage:\n\n ```\n let x = 12;\n let y = &x as *const i32;\n\n unsafe {\n     assert_eq!(std::ptr::read(y), 12);\n }\n ```\n\n Manually implement [`mem::swap`]:\n\n ```\n use std::ptr;\n\n fn swap<T>(a: &mut T, b: &mut T) {\n     unsafe {\n         // Create a bitwise copy of the value at `a` in `tmp`.\n         let tmp = ptr::read(a);\n\n         // Exiting at this point (either by explicitly returning or by\n         // calling a function which panics) would cause the value in `tmp` to\n         // be dropped while the same value is still referenced by `a`. This\n         // could trigger undefined behavior if `T` is not `Copy`.\n\n         // Create a bitwise copy of the value at `b` in `a`.\n         // This is safe because mutable references cannot alias.\n         ptr::copy_nonoverlapping(b, a, 1);\n\n         // As above, exiting here could trigger undefined behavior because\n         // the same value is referenced by `a` and `b`.\n\n         // Move `tmp` into `b`.\n         ptr::write(b, tmp);\n\n         // `tmp` has been moved (`write` takes ownership of its second argument),\n         // so nothing is dropped implicitly here.\n     }\n }\n\n let mut foo = \"foo\".to_owned();\n let mut bar = \"bar\".to_owned();\n\n swap(&mut foo, &mut bar);\n\n assert_eq!(foo, \"bar\");\n assert_eq!(bar, \"foo\");\n ```\n\n ## Ownership of the Returned Value\n\n `read` creates a bitwise copy of `T`, regardless of whether `T` is [`Copy`].\n If `T` is not [`Copy`], using both the returned value and the value at\n `*src` can violate memory safety. Note that assigning to `*src` counts as a\n use because it will attempt to drop the value at `*src`.\n\n [`write()`] can be used to overwrite data without causing it to be dropped.\n\n ```\n use std::ptr;\n\n let mut s = String::from(\"foo\");\n unsafe {\n     // `s2` now points to the same underlying memory as `s`.\n     let mut s2: String = ptr::read(&s);\n\n     assert_eq!(s2, \"foo\");\n\n     // Assigning to `s2` causes its original value to be dropped. Beyond\n     // this point, `s` must no longer be used, as the underlying memory has\n     // been freed.\n     s2 = String::default();\n     assert_eq!(s2, \"\");\n\n     // Assigning to `s` would cause the old value to be dropped again,\n     // resulting in undefined behavior.\n     // s = String::from(\"bar\"); // ERROR\n\n     // `ptr::write` can be used to overwrite a value without dropping it.\n     ptr::write(&mut s, String::from(\"bar\"));\n }\n\n assert_eq!(s, \"bar\");\n ```\n\n [valid]: self#safety\n",
      "adt": {}
    },
    "core::mem::drop": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Disposes of a value.\n\n This effectively does nothing for types which implement `Copy`, e.g.\n integers. Such values are copied and _then_ moved into the function, so the\n value persists after this function call.\n\n This function is not magic; it is literally defined as\n\n ```\n pub fn drop<T>(_x: T) {}\n ```\n\n Because `_x` is moved into the function, it is automatically [dropped][drop] before\n the function returns.\n\n [drop]: Drop\n\n # Examples\n\n Basic usage:\n\n ```\n let v = vec![1, 2, 3];\n\n drop(v); // explicitly drop the vector\n ```\n\n Since [`RefCell`] enforces the borrow rules at runtime, `drop` can\n release a [`RefCell`] borrow:\n\n ```\n use std::cell::RefCell;\n\n let x = RefCell::new(1);\n\n let mut mutable_borrow = x.borrow_mut();\n *mutable_borrow = 1;\n\n drop(mutable_borrow); // relinquish the mutable borrow on this slot\n\n let borrow = x.borrow();\n println!(\"{}\", *borrow);\n ```\n\n Integers and other types implementing [`Copy`] are unaffected by `drop`.\n\n ```\n # #![allow(dropping_copy_types)]\n #[derive(Copy, Clone)]\n struct Foo(u8);\n\n let x = 1;\n let y = Foo(2);\n drop(x); // a copy of `x` is moved and dropped\n drop(y); // a copy of `y` is moved and dropped\n\n println!(\"x: {}, y: {}\", x, y.0); // still available\n ```\n\n [`RefCell`]: crate::cell::RefCell\n",
      "adt": {}
    }
  },
  "adts": {
    "sync::Arc": [
      "Plain",
      "Ref",
      "MutRef",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(2)))",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))"
    ],
    "core::mem::ManuallyDrop": [
      "Plain",
      "Ref",
      "MutRef"
    ],
    "sync::ArcInner": [
      "Ref",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))"
    ],
    "core::sync::atomic::AtomicUsize": [
      "Ref"
    ],
    "core::sync::atomic::Ordering": [
      "Plain"
    ],
    "core::option::Option": [
      "Plain"
    ],
    "core::ptr::NonNull": [
      "Plain"
    ],
    "sync::Weak": [
      "Plain"
    ]
  },
  "path": 2076,
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/sync.rs:1221:5: 1246:6",
  "src": "pub fn into_inner(this: Self) -> Option<T> {\n        // Make sure that the ordinary `Drop` implementation isnâ€™t called as well\n        let mut this = mem::ManuallyDrop::new(this);\n\n        // Following the implementation of `drop` and `drop_slow`\n        if this.inner().strong.fetch_sub(1, Release) != 1 {\n            return None;\n        }\n\n        acquire!(this.inner().strong);\n\n        // SAFETY: This mirrors the line\n        //\n        //     unsafe { ptr::drop_in_place(Self::get_mut_unchecked(self)) };\n        //\n        // in `drop_slow`. Instead of dropping the value behind the pointer,\n        // it is read and eventually returned; `ptr::read` has the same\n        // safety conditions as `ptr::drop_in_place`.\n\n        let inner = unsafe { ptr::read(Self::get_mut_unchecked(&mut this)) };\n        let alloc = unsafe { ptr::read(&this.alloc) };\n\n        drop(Weak { ptr: this.ptr, alloc });\n\n        Some(inner)\n    }",
  "mir": "fn sync::Arc::<T, A>::into_inner(_1: sync::Arc<T, A>) -> core::option::Option<T> {\n    let mut _0: core::option::Option<T>;\n    let mut _2: core::mem::ManuallyDrop<sync::Arc<T, A>>;\n    let mut _3: usize;\n    let mut _4: &core::sync::atomic::AtomicUsize;\n    let  _5: &sync::ArcInner<T>;\n    let  _6: &sync::Arc<T, A>;\n    let mut _7: &core::mem::ManuallyDrop<sync::Arc<T, A>>;\n    let mut _8: core::sync::atomic::Ordering;\n    let  _9: ();\n    let mut _10: core::sync::atomic::Ordering;\n    let  _11: T;\n    let mut _12: *const T;\n    let  _13: &mut T;\n    let mut _14: &mut sync::Arc<T, A>;\n    let mut _15: &mut core::mem::ManuallyDrop<sync::Arc<T, A>>;\n    let  _16: A;\n    let mut _17: *const A;\n    let  _18: &A;\n    let  _19: &sync::Arc<T, A>;\n    let mut _20: &core::mem::ManuallyDrop<sync::Arc<T, A>>;\n    let  _21: ();\n    let mut _22: sync::Weak<T, A>;\n    let mut _23: core::ptr::NonNull<sync::ArcInner<T>>;\n    let mut _24: &sync::Arc<T, A>;\n    let mut _25: &core::mem::ManuallyDrop<sync::Arc<T, A>>;\n    debug this => _1;\n    debug this => _2;\n    debug inner => _11;\n    debug alloc => _16;\n    bb0: {\n        StorageLive(_2);\n        _2 = core::mem::ManuallyDrop::<sync::Arc<T, A>>::new(_1) -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        StorageLive(_3);\n        StorageLive(_4);\n        StorageLive(_5);\n        StorageLive(_7);\n        _7 = &_2;\n        _6 = <core::mem::ManuallyDrop<sync::Arc<T, A>> as core::ops::Deref>::deref(move _7) -> [return: bb2, unwind unreachable];\n    }\n    bb2: {\n        StorageDead(_7);\n        _5 = sync::Arc::<T, A>::inner(_6) -> [return: bb3, unwind unreachable];\n    }\n    bb3: {\n        _4 = &((*_5).0: core::sync::atomic::AtomicUsize);\n        StorageLive(_8);\n        _8 = core::sync::atomic::Ordering::Release;\n        _3 = core::sync::atomic::AtomicUsize::fetch_sub(move _4, 1_usize, move _8) -> [return: bb4, unwind unreachable];\n    }\n    bb4: {\n        StorageDead(_8);\n        StorageDead(_4);\n        switchInt(move _3) -> [1: bb6, otherwise: bb5];\n    }\n    bb5: {\n        StorageDead(_5);\n        StorageDead(_3);\n        _0 = core::option::Option::None;\n        StorageDead(_2);\n        goto -> bb15;\n    }\n    bb6: {\n        StorageDead(_5);\n        StorageDead(_3);\n        StorageLive(_10);\n        _10 = core::sync::atomic::Ordering::Acquire;\n        _9 = core::sync::atomic::fence(move _10) -> [return: bb7, unwind unreachable];\n    }\n    bb7: {\n        StorageDead(_10);\n        StorageLive(_12);\n        StorageLive(_13);\n        _15 = &mut _2;\n        _14 = <core::mem::ManuallyDrop<sync::Arc<T, A>> as core::ops::DerefMut>::deref_mut(_15) -> [return: bb8, unwind unreachable];\n    }\n    bb8: {\n        _13 = sync::Arc::<T, A>::get_mut_unchecked(_14) -> [return: bb9, unwind unreachable];\n    }\n    bb9: {\n        _12 = &raw const (*_13);\n        _11 = core::ptr::read::<T>(move _12) -> [return: bb10, unwind unreachable];\n    }\n    bb10: {\n        StorageDead(_13);\n        StorageDead(_12);\n        StorageLive(_17);\n        StorageLive(_18);\n        StorageLive(_19);\n        StorageLive(_20);\n        _20 = &_2;\n        _19 = <core::mem::ManuallyDrop<sync::Arc<T, A>> as core::ops::Deref>::deref(move _20) -> [return: bb11, unwind unreachable];\n    }\n    bb11: {\n        StorageDead(_20);\n        _18 = &((*_19).2: A);\n        _17 = &raw const (*_18);\n        _16 = core::ptr::read::<A>(move _17) -> [return: bb12, unwind unreachable];\n    }\n    bb12: {\n        StorageDead(_19);\n        StorageDead(_18);\n        StorageDead(_17);\n        StorageLive(_22);\n        StorageLive(_23);\n        StorageLive(_24);\n        StorageLive(_25);\n        _25 = &_2;\n        _24 = <core::mem::ManuallyDrop<sync::Arc<T, A>> as core::ops::Deref>::deref(move _25) -> [return: bb13, unwind unreachable];\n    }\n    bb13: {\n        StorageDead(_25);\n        _23 = ((*_24).0: core::ptr::NonNull<sync::ArcInner<T>>);\n        _22 = Weak(move _23, _16);\n        StorageDead(_23);\n        _21 = core::mem::drop::<sync::Weak<T, A>>(move _22) -> [return: bb14, unwind unreachable];\n    }\n    bb14: {\n        StorageDead(_22);\n        StorageDead(_24);\n        _0 = core::option::Option::Some(_11);\n        StorageDead(_2);\n        goto -> bb15;\n    }\n    bb15: {\n        return;\n    }\n}\n",
  "doc": " Returns the inner value, if the `Arc` has exactly one strong reference.\n\n Otherwise, [`None`] is returned and the `Arc` is dropped.\n\n This will succeed even if there are outstanding weak references.\n\n If `Arc::into_inner` is called on every clone of this `Arc`,\n it is guaranteed that exactly one of the calls returns the inner value.\n This means in particular that the inner value is not dropped.\n\n [`Arc::try_unwrap`] is conceptually similar to `Arc::into_inner`, but it\n is meant for different use-cases. If used as a direct replacement\n for `Arc::into_inner` anyway, such as with the expression\n <code>[Arc::try_unwrap]\\(this).[ok][Result::ok]()</code>, then it does\n **not** give the same guarantee as described in the previous paragraph.\n For more information, see the examples below and read the documentation\n of [`Arc::try_unwrap`].\n\n # Examples\n\n Minimal example demonstrating the guarantee that `Arc::into_inner` gives.\n ```\n use std::sync::Arc;\n\n let x = Arc::new(3);\n let y = Arc::clone(&x);\n\n // Two threads calling `Arc::into_inner` on both clones of an `Arc`:\n let x_thread = std::thread::spawn(|| Arc::into_inner(x));\n let y_thread = std::thread::spawn(|| Arc::into_inner(y));\n\n let x_inner_value = x_thread.join().unwrap();\n let y_inner_value = y_thread.join().unwrap();\n\n // One of the threads is guaranteed to receive the inner value:\n assert!(matches!(\n     (x_inner_value, y_inner_value),\n     (None, Some(3)) | (Some(3), None)\n ));\n // The result could also be `(None, None)` if the threads called\n // `Arc::try_unwrap(x).ok()` and `Arc::try_unwrap(y).ok()` instead.\n ```\n\n A more practical example demonstrating the need for `Arc::into_inner`:\n ```\n use std::sync::Arc;\n\n // Definition of a simple singly linked list using `Arc`:\n #[derive(Clone)]\n struct LinkedList<T>(Option<Arc<Node<T>>>);\n struct Node<T>(T, Option<Arc<Node<T>>>);\n\n // Dropping a long `LinkedList<T>` relying on the destructor of `Arc`\n // can cause a stack overflow. To prevent this, we can provide a\n // manual `Drop` implementation that does the destruction in a loop:\n impl<T> Drop for LinkedList<T> {\n     fn drop(&mut self) {\n         let mut link = self.0.take();\n         while let Some(arc_node) = link.take() {\n             if let Some(Node(_value, next)) = Arc::into_inner(arc_node) {\n                 link = next;\n             }\n         }\n     }\n }\n\n // Implementation of `new` and `push` omitted\n impl<T> LinkedList<T> {\n     /* ... */\n #   fn new() -> Self {\n #       LinkedList(None)\n #   }\n #   fn push(&mut self, x: T) {\n #       self.0 = Some(Arc::new(Node(x, self.0.take())));\n #   }\n }\n\n // The following code could have still caused a stack overflow\n // despite the manual `Drop` impl if that `Drop` impl had used\n // `Arc::try_unwrap(arc).ok()` instead of `Arc::into_inner(arc)`.\n\n // Create a long list and clone it\n let mut x = LinkedList::new();\n let size = 100000;\n # let size = if cfg!(miri) { 100 } else { size };\n for i in 0..size {\n     x.push(i); // Adds i to the front of x\n }\n let y = x.clone();\n\n // Drop the clones in parallel\n let x_thread = std::thread::spawn(|| drop(x));\n let y_thread = std::thread::spawn(|| drop(y));\n x_thread.join().unwrap();\n y_thread.join().unwrap();\n ```\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}