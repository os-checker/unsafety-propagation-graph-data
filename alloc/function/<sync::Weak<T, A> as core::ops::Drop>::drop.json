{
  "name": "<sync::Weak<T, A> as core::ops::Drop>::drop",
  "safe": true,
  "callees": {
    "sync::Weak::<T, A>::inner": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Returns `None` when the pointer is dangling and there is no allocated `ArcInner`,\n (i.e., when this `Weak` was created by `Weak::new`).\n",
      "adt": {
        "sync::Weak": "ImmutableAsArgument",
        "core::option::Option": "Constructor",
        "sync::WeakInner": "Constructor"
      }
    },
    "core::sync::atomic::AtomicUsize::fetch_sub": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Subtracts from the current value, returning the previous value.\n\n This operation wraps around on overflow.\n\n `fetch_sub` takes an [`Ordering`] argument which describes the memory ordering\n of this operation. All ordering modes are possible. Note that using\n [`Acquire`] makes the store part of this operation [`Relaxed`], and\n using [`Release`] makes the load part [`Relaxed`].\n\n **Note**: This method is only available on platforms that support atomic operations on\n\n # Examples\n\n ```\n\n assert_eq!(foo.fetch_sub(10, Ordering::SeqCst), 20);\n assert_eq!(foo.load(Ordering::SeqCst), 10);\n ```\n",
      "adt": {}
    },
    "core::sync::atomic::fence": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " An atomic fence.\n\n Fences create synchronization between themselves and atomic operations or fences in other\n threads. To achieve this, a fence prevents the compiler and CPU from reordering certain types of\n memory operations around it.\n\n There are 3 different ways to use an atomic fence:\n\n - atomic - fence synchronization: an atomic operation with (at least) [`Release`] ordering\n   semantics synchronizes with a fence with (at least) [`Acquire`] ordering semantics.\n - fence - atomic synchronization: a fence with (at least) [`Release`] ordering semantics\n   synchronizes with an atomic operation with (at least) [`Acquire`] ordering semantics.\n - fence - fence synchronization: a fence with (at least) [`Release`] ordering semantics\n   synchronizes with a fence with (at least) [`Acquire`] ordering semantics.\n\n These 3 ways complement the regular, fence-less, atomic - atomic synchronization.\n\n ## Atomic - Fence\n\n An atomic operation on one thread will synchronize with a fence on another thread when:\n\n -   on thread 1:\n     -   an atomic operation 'X' with (at least) [`Release`] ordering semantics on some atomic\n         object 'm',\n\n -   is paired on thread 2 with:\n     -   an atomic read 'Y' with any order on 'm',\n     -   followed by a fence 'B' with (at least) [`Acquire`] ordering semantics.\n\n This provides a happens-before dependence between X and B.\n\n ```text\n     Thread 1                                          Thread 2\n\n m.store(3, Release); X ---------\n                                |\n                                |\n                                -------------> Y  if m.load(Relaxed) == 3 {\n                                               B      fence(Acquire);\n                                                      ...\n                                                  }\n ```\n\n ## Fence - Atomic\n\n A fence on one thread will synchronize with an atomic operation on another thread when:\n\n -   on thread:\n     -   a fence 'A' with (at least) [`Release`] ordering semantics,\n     -   followed by an atomic write 'X' with any ordering on some atomic object 'm',\n\n -   is paired on thread 2 with:\n     -   an atomic operation 'Y' with (at least) [`Acquire`] ordering semantics.\n\n This provides a happens-before dependence between A and Y.\n\n ```text\n     Thread 1                                          Thread 2\n\n fence(Release);      A\n m.store(3, Relaxed); X ---------\n                                |\n                                |\n                                -------------> Y  if m.load(Acquire) == 3 {\n                                                      ...\n                                                  }\n ```\n\n ## Fence - Fence\n\n A fence on one thread will synchronize with a fence on another thread when:\n\n -   on thread 1:\n     -   a fence 'A' which has (at least) [`Release`] ordering semantics,\n     -   followed by an atomic write 'X' with any ordering on some atomic object 'm',\n\n -   is paired on thread 2 with:\n     -   an atomic read 'Y' with any ordering on 'm',\n     -   followed by a fence 'B' with (at least) [`Acquire`] ordering semantics.\n\n This provides a happens-before dependence between A and B.\n\n ```text\n     Thread 1                                          Thread 2\n\n fence(Release);      A --------------\n m.store(3, Relaxed); X ---------    |\n                                |    |\n                                |    |\n                                -------------> Y  if m.load(Relaxed) == 3 {\n                                     |-------> B      fence(Acquire);\n                                                      ...\n                                                  }\n ```\n\n ## Mandatory Atomic\n\n Note that in the examples above, it is crucial that the access to `m` are atomic. Fences cannot\n be used to establish synchronization between non-atomic accesses in different threads. However,\n thanks to the happens-before relationship, any non-atomic access that happen-before the atomic\n operation or fence with (at least) [`Release`] ordering semantics are now also properly\n synchronized with any non-atomic accesses that happen-after the atomic operation or fence with\n (at least) [`Acquire`] ordering semantics.\n\n ## Memory Ordering\n\n A fence which has [`SeqCst`] ordering, in addition to having both [`Acquire`] and [`Release`]\n semantics, participates in the global program order of the other [`SeqCst`] operations and/or\n fences.\n\n Accepts [`Acquire`], [`Release`], [`AcqRel`] and [`SeqCst`] orderings.\n\n # Panics\n\n Panics if `order` is [`Relaxed`].\n\n # Examples\n\n ```\n use std::sync::atomic::AtomicBool;\n use std::sync::atomic::fence;\n use std::sync::atomic::Ordering;\n\n // A mutual exclusion primitive based on spinlock.\n pub struct Mutex {\n     flag: AtomicBool,\n }\n\n impl Mutex {\n     pub fn new() -> Mutex {\n         Mutex {\n             flag: AtomicBool::new(false),\n         }\n     }\n\n     pub fn lock(&self) {\n         // Wait until the old value is `false`.\n         while self\n             .flag\n             .compare_exchange_weak(false, true, Ordering::Relaxed, Ordering::Relaxed)\n             .is_err()\n         {}\n         // This fence synchronizes-with store in `unlock`.\n         fence(Ordering::Acquire);\n     }\n\n     pub fn unlock(&self) {\n         self.flag.store(false, Ordering::Release);\n     }\n }\n ```\n",
      "adt": {}
    },
    "core::ptr::NonNull::<T>::as_ptr": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Acquires the underlying `*mut` pointer.\n\n # Examples\n\n ```\n use std::ptr::NonNull;\n\n let mut x = 0u32;\n let ptr = NonNull::new(&mut x).expect(\"ptr is null!\");\n\n let x_value = unsafe { *ptr.as_ptr() };\n assert_eq!(x_value, 0);\n\n unsafe { *ptr.as_ptr() += 2; }\n let x_value = unsafe { *ptr.as_ptr() };\n assert_eq!(x_value, 2);\n ```\n",
      "adt": {}
    },
    "core::ptr::addr_eq": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Compares the *addresses* of the two pointers for equality,\n ignoring any metadata in fat pointers.\n\n If the arguments are thin pointers of the same type,\n then this is the same as [`eq`].\n\n # Examples\n\n ```\n use std::ptr;\n\n let whole: &[i32; 3] = &[1, 2, 3];\n let first: &i32 = &whole[0];\n\n assert!(ptr::addr_eq(whole, first));\n assert!(!ptr::eq::<dyn std::fmt::Debug>(whole, first));\n ```\n",
      "adt": {}
    },
    "core::fmt::Arguments::<'a>::from_str": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Create a `fmt::Arguments` object for a single static string.\n\n Formatting this `fmt::Arguments` will just produce the string as-is.\n",
      "adt": {}
    },
    "core::ptr::NonNull::<T>::cast": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Casts to a pointer of another type.\n\n # Examples\n\n ```\n use std::ptr::NonNull;\n\n let mut x = 0u32;\n let ptr = NonNull::new(&mut x as *mut _).expect(\"null pointer\");\n\n let casted_ptr = ptr.cast::<i8>();\n let raw_ptr: *mut i8 = casted_ptr.as_ptr();\n ```\n",
      "adt": {}
    },
    "core::panicking::panic_fmt": {
      "safe": true,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " The entry point for panicking with a formatted message.\n\n This is designed to reduce the amount of code required at the call\n site as much as possible (so that `panic!()` has as low an impact\n on (e.g.) the inlining of other functions as possible), by moving\n the actual formatting into this shared place.\n",
      "adt": {}
    },
    "core::alloc::Layout::for_value_raw": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Produces layout describing a record that could be used to\n allocate backing structure for `T` (which could be a trait\n or other unsized type like a slice).\n\n # Safety\n\n This function is only safe to call if the following conditions hold:\n\n - If `T` is `Sized`, this function is always safe to call.\n - If the unsized tail of `T` is:\n     - a [slice], then the length of the slice tail must be an initialized\n       integer, and the size of the *entire value*\n       (dynamic tail length + statically sized prefix) must fit in `isize`.\n       For the special case where the dynamic tail length is 0, this function\n       is safe to call.\n     - a [trait object], then the vtable part of the pointer must point\n       to a valid vtable for the type `T` acquired by an unsizing coercion,\n       and the size of the *entire value*\n       (dynamic tail length + statically sized prefix) must fit in `isize`.\n     - an (unstable) [extern type], then this function is always safe to\n       call, but may panic or otherwise return the wrong value, as the\n       extern type's layout is not known. This is the same behavior as\n       [`Layout::for_value`] on a reference to an extern type tail.\n     - otherwise, it is conservatively not allowed to call this function.\n\n [trait object]: ../../book/ch17-02-trait-objects.html\n [extern type]: ../../unstable-book/language-features/extern-types.html\n",
      "adt": {}
    },
    "core::alloc::Allocator::deallocate": {
      "safe": false,
      "tags": {
        "tags": [],
        "spec": {},
        "docs": []
      },
      "doc": " Deallocates the memory referenced by `ptr`.\n\n # Safety\n\n * `ptr` must denote a block of memory [*currently allocated*] via this allocator, and\n * `layout` must [*fit*] that block of memory.\n\n [*currently allocated*]: #currently-allocated-memory\n [*fit*]: #memory-fitting\n",
      "adt": {}
    }
  },
  "adts": {
    "sync::Weak": [
      "Ref",
      "Deref",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(1)))",
      "MutRef"
    ],
    "core::option::Option": [
      "Plain",
      "Unknown([Downcast(VariantIdx(1, ThreadLocalIndex)), Field(0, Ty { id: 9740, kind: RigidTy(Adt(AdtDef(DefId { id: 5759, name: \"sync::WeakInner\" }), GenericArgs([Lifetime(Region { kind: ReErased })]))) })])"
    ],
    "sync::WeakInner": [
      "Plain",
      "Unknown([Field(0, Ty { id: 9469, kind: RigidTy(Ref(Region { kind: ReErased }, Ty { id: 9365, kind: RigidTy(Adt(AdtDef(DefId { id: 5675, name: \"core::sync::atomic::AtomicUsize\" }), GenericArgs([]))) }, Not)) })])"
    ],
    "core::sync::atomic::AtomicUsize": [
      "Ref"
    ],
    "core::sync::atomic::Ordering": [
      "Plain"
    ],
    "core::ptr::NonNull": [
      "Plain"
    ],
    "sync::SliceArcInnerForStatic": [
      "Ref",
      "DerefVariantField(VariantIdx(None)-FieldIdx(Some(0)))"
    ],
    "sync::ArcInner": [
      "Ref",
      "Deref"
    ],
    "core::fmt::Arguments": [
      "Plain"
    ],
    "core::alloc::Layout": [
      "Plain"
    ]
  },
  "path": 2161,
  "span": "/home/gh-zjp-CN/.rustup/toolchains/nightly-2025-12-06-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/sync.rs:3468:5: 3494:6",
  "src": "fn drop(&mut self) {\n        // If we find out that we were the last weak pointer, then its time to\n        // deallocate the data entirely. See the discussion in Arc::drop() about\n        // the memory orderings\n        //\n        // It's not necessary to check for the locked state here, because the\n        // weak count can only be locked if there was precisely one weak ref,\n        // meaning that drop could only subsequently run ON that remaining weak\n        // ref, which can only happen after the lock is released.\n        let inner = if let Some(inner) = self.inner() { inner } else { return };\n\n        if inner.weak.fetch_sub(1, Release) == 1 {\n            acquire!(inner.weak);\n\n            // Make sure we aren't trying to \"deallocate\" the shared static for empty slices\n            // used by Default::default.\n            debug_assert!(\n                !ptr::addr_eq(self.ptr.as_ptr(), &STATIC_INNER_SLICE.inner),\n                \"Arc/Weaks backed by a static should never be deallocated. \\\n                Likely decrement_strong_count or from_raw were called too many times.\",\n            );\n\n            unsafe {\n                self.alloc.deallocate(self.ptr.cast(), Layout::for_value_raw(self.ptr.as_ptr()))\n            }\n        }\n    }",
  "mir": "fn <sync::Weak<T, A> as core::ops::Drop>::drop(_1: &mut sync::Weak<T, A>) -> () {\n    let mut _0: ();\n    let mut _2: core::option::Option<sync::WeakInner<'_>>;\n    let mut _3: &sync::Weak<T, A>;\n    let mut _4: isize;\n    let  _5: sync::WeakInner<'_>;\n    let mut _6: usize;\n    let mut _7: core::sync::atomic::Ordering;\n    let  _8: ();\n    let mut _9: core::sync::atomic::Ordering;\n    let mut _10: bool;\n    let mut _11: *const sync::ArcInner<T>;\n    let mut _12: *mut sync::ArcInner<T>;\n    let mut _13: core::ptr::NonNull<sync::ArcInner<T>>;\n    let mut _14: *const sync::ArcInner<[u8; 1]>;\n    let  _15: &sync::ArcInner<[u8; 1]>;\n    let  _16: &sync::SliceArcInnerForStatic;\n    let  _17: !;\n    let mut _18: core::fmt::Arguments<'_>;\n    let mut _19: &A;\n    let mut _20: core::ptr::NonNull<u8>;\n    let mut _21: core::ptr::NonNull<sync::ArcInner<T>>;\n    let mut _22: core::alloc::Layout;\n    let mut _23: *const sync::ArcInner<T>;\n    let mut _24: *mut sync::ArcInner<T>;\n    let mut _25: core::ptr::NonNull<sync::ArcInner<T>>;\n    let mut _26: &core::sync::atomic::AtomicUsize;\n    debug self => _1;\n    debug inner => _5;\n    debug inner => _5;\n    bb0: {\n        StorageLive(_2);\n        StorageLive(_3);\n        _3 = &(*_1);\n        _2 = sync::Weak::<T, A>::inner(move _3) -> [return: bb1, unwind unreachable];\n    }\n    bb1: {\n        StorageDead(_3);\n        _4 = discriminant(_2);\n        switchInt(move _4) -> [1: bb2, 0: bb3, otherwise: bb19];\n    }\n    bb2: {\n        _5 = move ((_2 as variant#1).0: sync::WeakInner<'_>);\n        StorageDead(_2);\n        StorageLive(_6);\n        _26 = (_5.0: &core::sync::atomic::AtomicUsize);\n        StorageLive(_7);\n        _7 = core::sync::atomic::Ordering::Release;\n        _6 = core::sync::atomic::AtomicUsize::fetch_sub(_26, 1_usize, move _7) -> [return: bb4, unwind unreachable];\n    }\n    bb3: {\n        StorageDead(_2);\n        goto -> bb18;\n    }\n    bb4: {\n        StorageDead(_7);\n        switchInt(move _6) -> [1: bb5, otherwise: bb16];\n    }\n    bb5: {\n        StorageDead(_6);\n        StorageLive(_9);\n        _9 = core::sync::atomic::Ordering::Acquire;\n        _8 = core::sync::atomic::fence(move _9) -> [return: bb6, unwind unreachable];\n    }\n    bb6: {\n        StorageDead(_9);\n        StorageLive(_10);\n        StorageLive(_11);\n        StorageLive(_12);\n        StorageLive(_13);\n        _13 = ((*_1).0: core::ptr::NonNull<sync::ArcInner<T>>);\n        _12 = core::ptr::NonNull::<sync::ArcInner<T>>::as_ptr(move _13) -> [return: bb7, unwind unreachable];\n    }\n    bb7: {\n        _11 = move _12 as *const sync::ArcInner<T>;\n        StorageDead(_13);\n        StorageDead(_12);\n        StorageLive(_14);\n        StorageLive(_15);\n        StorageLive(_16);\n        _16 = {alloc107: &sync::SliceArcInnerForStatic};\n        _15 = &((*_16).0: sync::ArcInner<[u8; 1]>);\n        _14 = &raw const (*_15);\n        _10 = core::ptr::addr_eq::<sync::ArcInner<T>, sync::ArcInner<[u8; 1]>>(move _11, move _14) -> [return: bb8, unwind unreachable];\n    }\n    bb8: {\n        switchInt(move _10) -> [0: bb10, otherwise: bb9];\n    }\n    bb9: {\n        StorageDead(_14);\n        StorageDead(_11);\n        StorageDead(_16);\n        StorageDead(_15);\n        StorageLive(_18);\n        _18 = core::fmt::Arguments::<'_>::from_str(\"Arc/Weaks backed by a static should never be deallocated. Likely decrement_strong_count or from_raw were called too many times.\") -> [return: bb11, unwind unreachable];\n    }\n    bb10: {\n        StorageDead(_14);\n        StorageDead(_11);\n        StorageDead(_16);\n        StorageDead(_15);\n        StorageDead(_10);\n        StorageLive(_19);\n        _19 = &((*_1).1: A);\n        StorageLive(_20);\n        StorageLive(_21);\n        _21 = ((*_1).0: core::ptr::NonNull<sync::ArcInner<T>>);\n        _20 = core::ptr::NonNull::<sync::ArcInner<T>>::cast::<u8>(move _21) -> [return: bb12, unwind unreachable];\n    }\n    bb11: {\n        _17 = core::panicking::panic_fmt(move _18) -> unwind unreachable;\n    }\n    bb12: {\n        StorageDead(_21);\n        StorageLive(_22);\n        StorageLive(_23);\n        StorageLive(_24);\n        StorageLive(_25);\n        _25 = ((*_1).0: core::ptr::NonNull<sync::ArcInner<T>>);\n        _24 = core::ptr::NonNull::<sync::ArcInner<T>>::as_ptr(move _25) -> [return: bb13, unwind unreachable];\n    }\n    bb13: {\n        _23 = move _24 as *const sync::ArcInner<T>;\n        StorageDead(_25);\n        StorageDead(_24);\n        _22 = core::alloc::Layout::for_value_raw::<sync::ArcInner<T>>(move _23) -> [return: bb14, unwind unreachable];\n    }\n    bb14: {\n        StorageDead(_23);\n        _0 = <A as core::alloc::Allocator>::deallocate(move _19, move _20, move _22) -> [return: bb15, unwind unreachable];\n    }\n    bb15: {\n        StorageDead(_22);\n        StorageDead(_20);\n        StorageDead(_19);\n        goto -> bb17;\n    }\n    bb16: {\n        StorageDead(_6);\n        goto -> bb17;\n    }\n    bb17: {\n        goto -> bb18;\n    }\n    bb18: {\n        return;\n    }\n    bb19: {\n        unreachable;\n    }\n}\n",
  "doc": " Drops the `Weak` pointer.\n\n # Examples\n\n ```\n use std::sync::{Arc, Weak};\n\n struct Foo;\n\n impl Drop for Foo {\n     fn drop(&mut self) {\n         println!(\"dropped!\");\n     }\n }\n\n let foo = Arc::new(Foo);\n let weak_foo = Arc::downgrade(&foo);\n let other_weak_foo = Weak::clone(&weak_foo);\n\n drop(weak_foo);   // Doesn't print anything\n drop(foo);        // Prints \"dropped!\"\n\n assert!(other_weak_foo.upgrade().is_none());\n ```\n",
  "tags": {
    "tags": [],
    "spec": {},
    "docs": []
  }
}