{
  "name": "alloc::sync::Arc::<T, A>::make_mut",
  "span": "$library/alloc/src/sync.rs:2492:5: 2492:47",
  "src": "pub fn make_mut(this: &mut Self) -> &mut T {\n        let size_of_val = size_of_val::<T>(&**this);\n\n        // Note that we hold both a strong reference and a weak reference.\n        // Thus, releasing our strong reference only will not, by itself, cause\n        // the memory to be deallocated.\n        //\n        // Use Acquire to ensure that we see any writes to `weak` that happen\n        // before release writes (i.e., decrements) to `strong`. Since we hold a\n        // weak count, there's no chance the ArcInner itself could be\n        // deallocated.\n        if this.inner().strong.compare_exchange(1, 0, Acquire, Relaxed).is_err() {\n            // Another strong pointer exists, so we must clone.\n            *this = Arc::clone_from_ref_in(&**this, this.alloc.clone());\n        } else if this.inner().weak.load(Relaxed) != 1 {\n            // Relaxed suffices in the above because this is fundamentally an\n            // optimization: we are always racing with weak pointers being\n            // dropped. Worst case, we end up allocated a new Arc unnecessarily.\n\n            // We removed the last strong ref, but there are additional weak\n            // refs remaining. We'll move the contents to a new Arc, and\n            // invalidate the other weak refs.\n\n            // Note that it is not possible for the read of `weak` to yield\n            // usize::MAX (i.e., locked), since the weak count can only be\n            // locked by a thread with a strong reference.\n\n            // Materialize our own implicit weak pointer, so that it can clean\n            // up the ArcInner as needed.\n            let _weak = Weak { ptr: this.ptr, alloc: this.alloc.clone() };\n\n            // Can just steal the data, all that's left is Weaks\n            //\n            // We don't need panic-protection like the above branch does, but we might as well\n            // use the same mechanism.\n            let mut in_progress: UniqueArcUninit<T, A> =\n                UniqueArcUninit::new(&**this, this.alloc.clone());\n            unsafe {\n                // Initialize `in_progress` with move of **this.\n                // We have to express this in terms of bytes because `T: ?Sized`; there is no\n                // operation that just copies a value based on its `size_of_val()`.\n                ptr::copy_nonoverlapping(\n                    ptr::from_ref(&**this).cast::<u8>(),\n                    in_progress.data_ptr().cast::<u8>(),\n                    size_of_val,\n                );\n\n                ptr::write(this, in_progress.into_arc());\n            }\n        } else {\n            // We were the sole reference of either kind; bump back up the\n            // strong ref count.\n            this.inner().strong.store(1, Release);\n        }\n\n        // As with `get_mut()`, the unsafety is ok because our reference was\n        // either unique to begin with, or became one upon cloning the contents.\n        unsafe { Self::get_mut_unchecked(this) }\n    }"
}